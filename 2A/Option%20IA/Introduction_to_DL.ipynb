{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbHVxTiHw2ZCrNVZTKFJl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/2A/Option%2520IA/Introduction_to_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFaa7vpd7PRa"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 12PZB5hWpwAyRuSWAgkaFdZdZluGf8_6Z\n",
        "!gdown --id 1g1CEUs8BvSJXSDroHgjUbTa3izGp9WMO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNNs with TensorFlow\n",
        "\n",
        "Welcome to the Deep Learning Lab sessions. As you had a slight introduction to Deep Learning through the Course, you will now apply it through this izi pizi labs.\n",
        "This lab will be done using TensorFlow from Google. \n",
        "You will learn to do the following in TensorFlow: \n",
        "\n",
        "- Initialize variables\n",
        "- Start your own session\n",
        "- Train algorithms \n",
        "- Implement a Neural Network\n",
        "\n",
        "This part of the lab is splitted in two parts :\n",
        "- Using Tensorflow\n",
        "- Creating and Performing Classification using TF.\n",
        "\n",
        "<img src=\"https://www.memesmonkey.com/images/memesmonkey/3f/3fd47d627866ac3f67bc4a38b0b2941c.jpeg\">\n"
      ],
      "metadata": {
        "id": "dcOlbCv-7P0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Time Hun ?\n",
        "\n",
        "So this might be your first time using Deep Learning for a certain task. So in this lab we will perform a certain Task : Classification. We will classify hands. Hun.\n",
        "\n",
        "When training a DL Algorithm you need few things : \n",
        "* A Dataset\n",
        "* A Model\n",
        "* A Learning Algorithm\n",
        "* Something more ?\n",
        "\n",
        "So we will navigate through each of these points to train a Model to perform Hand Signs Classification.\n",
        "\n",
        "<img src = \"https://i.kym-cdn.com/photos/images/original/000/123/620/Oh-boy-here-we-go.jpg\">\n",
        "\n"
      ],
      "metadata": {
        "id": "B6t5QBmc7RIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Dataset : SIGNS"
      ],
      "metadata": {
        "id": "RVTDKEE9-Lgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We wil use SIGNS Dataset. Let's first have some insight look on our Dataset\n",
        "Load the train and test datasets and answer the following questions:\n",
        "* What is the length of each datasets ?\n",
        "* What is the size of one data ?\n"
      ],
      "metadata": {
        "id": "-jEdisCc94vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "# TODO : load the h5 files in the correct variables\n",
        "train_dataset = h5py.File('/content/train_signs.h5', \"r\")\n",
        "test_dataset = h5py.File('/content/test_signs.h5', \"r\")\n",
        "\n",
        "# TODO : print the keys of each variables. \n",
        "# Questions: Are the keys different ? \n",
        "print(train_dataset.keys())\n",
        "print(test_dataset.keys())\n",
        "\n",
        "# TODO : create x_train, y_train, x_test, y_test and convert them to array using numpy\n",
        "# Questions : What is the length of each Dataset.\n",
        "# Questions : What are the labels, what are our images ?\n",
        "\n",
        "print(len(train_dataset))\n",
        "x_train = np.array(train_dataset['train_set_x'][:])\n",
        "y_train = np.array(train_dataset['train_set_y'][:])\n",
        "\n",
        "x_test = np.array(test_dataset['test_set_x'][:])\n",
        "y_test = np.array(test_dataset['test_set_y'][:])\n",
        "print(x_train.shape, y_train.shape, type(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zv5idtSAyaF",
        "outputId": "bc01d91f-ef73-4569-8b76-fefcf8b7e13a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<KeysViewHDF5 ['list_classes', 'train_set_x', 'train_set_y']>\n",
            "<KeysViewHDF5 ['list_classes', 'test_set_x', 'test_set_y']>\n",
            "3\n",
            "(1080, 64, 64, 3) (1080,) <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "metadata": {
        "id": "G5oiFBJdAfRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "II- The Model : Your Choice\n",
        "\n"
      ],
      "metadata": {
        "id": "nCynSEY9H7BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "l8f_9xapH42z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "III- The Loss \n",
        "\n",
        "Now we need something to tell our model if it does well or no. Let's say your model predicts something. We need to compute how far the prediction is compared to the real label. \n",
        "We will use the cross entropy."
      ],
      "metadata": {
        "id": "DfsGRDejIMMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mRGTfxuSJ35g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IV - The Optimizer\n",
        "\n",
        "So at the end of the Model, we can compute a loss. We need to back-propagate the loss to all the layers so that it sees and understands the end result of a prediction. \n",
        "\n",
        "You have plenty of optimizers available, the simplest is Gradient Descent."
      ],
      "metadata": {
        "id": "cZ0j9m8bISOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rZt9Lv11J4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V - The Compilation\n",
        "\n",
        "Now that we have all we need, we have to compile the model with all the preceding stuff. \n",
        "\n",
        "That's where TensorFlow is cool. It has a compile method that compiles everything for you."
      ],
      "metadata": {
        "id": "q7DxtlUdIUfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VI- The Training \n",
        "\n",
        "When you train, you become fit. In Deep Learning, we fit a distribution of Data to the model so that it learns a specific task.\n",
        "\n",
        "Again, TensorFlow has a great method that fits data for you."
      ],
      "metadata": {
        "id": "7tjgUzaTIWU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "o6hQFAbXKRzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We decide to teach our computers to decipher sign language. Here are the datasets we will use:\n",
        "\n",
        "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
        "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number)."
      ],
      "metadata": {
        "id": "mk4qWRGsAROH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VII- The Evalutation"
      ],
      "metadata": {
        "id": "fYvRK1IOIdkU"
      }
    }
  ]
}