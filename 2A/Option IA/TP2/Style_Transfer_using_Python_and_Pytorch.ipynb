{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer using Python and Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO93+M/vFiXbq2w31CzQCEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/2A/Option%20IA/Style_Transfer_using_Python_and_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Lab Questions\n",
        "\n",
        "* What is a Gram Matrix ?\n",
        "\n",
        "Watch this : https://www.coursera.org/lecture/convolutional-neural-networks/what-is-neural-style-transfer-SA5H8\n",
        "* What is Neural Style Transfer ?\n",
        "* What is the difference between a Shallow and a Deep Network ?\n",
        "* What is a Conv Layer's Filter ?\n",
        "* What is a feature ? Explain it with an example. \n",
        "* What type of features detected in the earlier layers of a neural net ? later layers ?\n"
      ],
      "metadata": {
        "id": "gmdBYJD2JNJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style Transfer "
      ],
      "metadata": {
        "id": "gTOrt8R9mURa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning models often seems like black boxes that no one understands... In fact, we can actually go inside these model to understand what the model learned. Think of it as if you were looking at some coffee grounds and were trying to construe an object from it.\n",
        "\n",
        "We are going to do some Style Transfer. You know that thing on Snapchat where you can add a Comic Filter on your face... We are basically redoing that kind of style transfer in order to understand what DL is.\n",
        "\n",
        "Goal on this lab :\n",
        "* See how a model works\n",
        "* Import a Pre Trained Model\n",
        "* Code in class using Python\n",
        "\n",
        "\n",
        "Alright, let's get started.\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/16/b2/96/16b296afb78ec57d12c931bc72b42eec.gif\">"
      ],
      "metadata": {
        "id": "wzuQhL9ql1Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Run the following Cell to import some frameworks\n"
      ],
      "metadata": {
        "id": "2yuLSlSHmcrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import skimage\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "metadata": {
        "id": "vtu5adzUvtQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Some Practice on Python and Pytorch"
      ],
      "metadata": {
        "id": "57lJlFpkm8DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's have a quick run on Python. You might have used it before but let's do a quick refreshment"
      ],
      "metadata": {
        "id": "PDJh15dpnoa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a - Defining a function\n",
        "\n",
        "We are going to define few functions. Let's define the following functions:\n",
        "\n",
        "* add\n",
        "* substract\n",
        "* divide\n",
        "* multiply\n",
        "* power\n",
        "\n",
        "**All your functions must take two input arguments a and b **\n",
        "\n",
        "To define a function, it's like matlab :\n",
        "\n",
        "```\n",
        "def function(**kwargs):\n",
        "      # do stuff\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "p5t-OYVRn2Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a,b):\n",
        "\n",
        "def substract(a,b):\n",
        "\n",
        "def divide(a,b):\n",
        "\n",
        "def multiply(a,b):\n",
        "\n",
        "def power(a, b):\n"
      ],
      "metadata": {
        "id": "C0yp6kXInHlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test the functions"
      ],
      "metadata": {
        "id": "f6MQWc6VqsIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \n",
        "b =\n"
      ],
      "metadata": {
        "id": "VrHHUa4eqrn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b - Defining a class\n",
        "\n",
        "So we basically have defined all the functions, but they are 'independant'. However, as you can see, they all have the same goal : Do some calculations...\n",
        "\n",
        "So let's define a basic object : a Calculator. In fact, we are going to create a **Class** which **Attributes** are a and b, to which calculation **Methods** will be applied. Remember your Java Class :')\n",
        "\n",
        "To define a class\n",
        "```\n",
        "class YourClass():\n",
        "\n",
        "    def __init__(self,*kwargs):\n",
        "        # Here you define the attributes of your model. \n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def method1(self,..):\n",
        "        #do stuff\n",
        "    \n",
        "    ...\n",
        "    \n",
        "```\n",
        "\n",
        "If you still don't understand, look at the following skeleton"
      ],
      "metadata": {
        "id": "d9PyVLSmqbAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We consider a and b two numbers that we want to apply calculation on.\n",
        "# Define a class that takes as attributes a and b and gather all the previous functions as method of this class \n",
        "# We use self to refer to something that is inside the class, an attribute or a method for example. Self represents an instance of the Class\n",
        "class Calculator():\n",
        "    \n",
        "    def __init__(self,a,b):\n",
        "        # TODO : Fill the attributes initialisation\n",
        "        self.a = ...\n",
        "\n",
        "    \n",
        "    def add(self):\n",
        "        # Call and return the sum of attributes\n",
        "        sum = self.a...\n",
        "\n",
        "    def substract(self):\n",
        "      ...\n",
        "\n",
        "    def divide(self):\n",
        "      ...\n",
        "\n",
        "    def multiply(self):\n",
        "      ...\n",
        "\n",
        "    def power(self):\n",
        "      ..."
      ],
      "metadata": {
        "id": "8m23-Secq1m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a calculator object and do some calculation"
      ],
      "metadata": {
        "id": "a4cqYJRvto4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \n",
        "b =\n",
        "calculator = Calculator(a,b)\n",
        "# Call the different methods of your class. To call a method use object.method()"
      ],
      "metadata": {
        "id": "1zlXSiy4t044"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About PyTorch \n",
        "\n",
        "PyTorch is Facebook's Deep Learning Framwork. We will be using it throughout this lab. It is not the only DL framework, you also have Tensorflow, JAX, Caffee...\n",
        "\n",
        "We will use PyTorch as we can have lots of control on everything."
      ],
      "metadata": {
        "id": "Cv6lbpbru8_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Some explanation on Style Transfer\n",
        "\n",
        "Style Transfer is doing this : \n",
        "\n",
        "<img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-example.jpg\">\n",
        "\n",
        "So we have 3 things :\n",
        "* A content Image (C)\n",
        "* A style Image (S)\n",
        "* A generated Image (G)\n",
        "\n",
        "\n",
        "The principle is the following :    \n",
        "* We define a Content Distance and a Style Distance. \n",
        "  * The CD measures how different the content between two images is. \n",
        "  * The SD measures how differents the style between the two images is.\n",
        "\n"
      ],
      "metadata": {
        "id": "DjakeZuAnCLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be defining methods that we will add as things progresses in a Class. ***Here is the skeleton of the class***. Don't be afraid we will be progressing step by step."
      ],
      "metadata": {
        "id": "swjydNnDoCbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Style_Transfer():\n",
        "\n",
        "    def __init__(self,model, content_image, style_image,num_steps =500):\n",
        "        # Some Useful Attributes \n",
        "        self.model_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device) \n",
        "        self.model_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        self.content_layers = ['conv_4']\n",
        "        self.style_layers= ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "        # Some useful Constant attributes\n",
        "        self.steps = 0 \n",
        "        self.num_steps =num_steps\n",
        "        self.style_weight=1000000\n",
        "        self.content_weight=1\n",
        "\n",
        "    def copy_model(self,model,layer,i):\n",
        "        # First Method \n",
        "        return model, name ,i\n",
        "\n",
        "    def add_module(self,model, name,content_losses, style_losses,i):\n",
        "        # Second Method \n",
        "        return model , content_losses, style_losses\n",
        "\n",
        "    def trim(self,model):\n",
        "        # Third Method \n",
        "        return model\n",
        "    \n",
        "    def get_model_and_losses(self):\n",
        "      # Fourth Method \n",
        "      return self.new_model, content_loss, style_loss\n",
        "\n",
        "    def optimizer_img(self,input_img):\n",
        "        # Fifth Method \n",
        "        return optimizer\n",
        "      \n",
        "    def run(self,input_image):\n",
        "        # Final Boss\n",
        "        return input_image\n"
      ],
      "metadata": {
        "id": "7jycYIfQoUj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Okaayyyyy Let's Goooo"
      ],
      "metadata": {
        "id": "8HRyxPxUM7uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KbDFCn29BGE\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MIlKqB6-MaZb",
        "outputId": "eba7d534-ac80-4922-c1b1-3efe52acf1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KbDFCn29BGE\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Losses\n",
        "\n",
        "So we need to define two losses \n",
        "* StyleLoss\n",
        "* ContentLoss\n",
        "\n"
      ],
      "metadata": {
        "id": "Dclf_n_h1v3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some explanation about these things \n",
        "* When you learn to differentiate Pikachu from Jigglypuff, you build a representation where features (forms, texture, colors...) are extracted from the objects.\n",
        "* For a neural net, when classification is performed, images (in other words bunch of pixels) are used to construct that type of representation by extracting features from the input image. From image, the model creates a representation.\n",
        "\n",
        "When training a model using ConvNets, they develop an understanding of the objects features. The more layers you have, the more complex the extracted features will be. \n",
        "\n",
        "Thus, we can separate the representation with the content (pixel value) and style (texture informations). So to compute the style and the content, we need to look at different intermediate layers of our model.\n",
        "\n"
      ],
      "metadata": {
        "id": "B_yKeGTrmfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, in our implementation, these are not really losses. They are more layers that we put at a certain place in the model, in order to compute Style and Content.\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0QjLH5km4WCtbcv4qJ9OiRC9p4gBYv47F9s2W4vp-r3FiMNBtMLHf1WvhIZh36jURCMA&usqp=CAU\">\n",
        "\n",
        "Well to sum up :     \n",
        "* We will be adding to a Neural Net layers to compute Style and Content\n",
        "* Extracted Features are not the same in the earlier and later layers of the model\n"
      ],
      "metadata": {
        "id": "jXEzXIRBliNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notation\n",
        "\n",
        "\n",
        "For a layer $L$ that processes the input $X$, we define:\n",
        "*  $F_{X;L}$, the feature map of $X$ at $L$.\n",
        "* $G_{X;L}$, the Gram Matrix of $X$ at $L$."
      ],
      "metadata": {
        "id": "QrS60LIvvoYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content Loss\n",
        "\n",
        "Imagine that you have a neural net composed of layers. We need a function that can represent the content of a layer. In other words we want to compute the distance between the oinput image at layer L and the content image at layer L. In other words, how far our Input Image is from the Content Image.\n",
        "\n",
        "i.d : $||F_{X;L}- F_{C;L}||^2$\n",
        "\n",
        "* What it the name of this distance ?\n",
        "\n",
        "Let's define a ContentLoss that inherits from nn.Module\n",
        "\n"
      ],
      "metadata": {
        "id": "754e3xouYzTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO : Compute the distance between the Target and the Input\n",
        "        self.loss = ...\n",
        "\n",
        "        return input"
      ],
      "metadata": {
        "id": "MNDELxbm0r2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Style Loss\n",
        "\n",
        "The style loss is used to compute the style of a layer. Hmmm what does it mean ? We want to use a feature space designed to capture texture information. \n",
        "* How can we compute that ?\n",
        "\n",
        "The style of an image can be computed using the mean and the correlation accross feature maps. In fact, we are going to compute the Gram Matrix of a layer. Style Features tend to be in deeper layers of the network.\n",
        "\n",
        "* Define a Gram Matrix function (1rst year course, Analyse de Fourier 1, chp2 : Espace de Hilbert) that returns a Gram Matrix on an input.\n",
        "\n",
        "* Do we need some normalization ? If yes, why ?\n",
        "\n",
        "<img src =\"https://github.com/enggen/Deep-Learning-Coursera/raw/1407e19c98833d2686a0748db26b594f3102301e/Convolutional%20Neural%20Networks/Week4/Neural%20Style%20Transfer/images/NST_GM.png\">\n",
        "\n",
        "\n",
        "So for some explanation, the Gram Matrix compares how a bunch of vectors ($v_1, v_2...$) are similar by computing their dot product. In our case, it computes how active a filter is. It means that for the different types of features existing in the model, the Gram Matrix computes :\n",
        "* in the diagonal, the different types of features\n",
        "* in the non diagonal, how different the features are."
      ],
      "metadata": {
        "id": "QrM0OIsb2U7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Gram_Mat(input):\n",
        "    # TODO : Compute the Gram Matrix of the input\n",
        "    ..."
      ],
      "metadata": {
        "id": "YC3-DeWA2UYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to calculate the Style Distance aka the Gram Matrix distances at a layer. \n",
        "* Let's define a Style Loss class that inherits from nn.Module.\n",
        "\n",
        "The Style Loss returns the distance between the Gram Matrix of the target and the Input. i.d : $||G_{X;L}- G_{C;L}||^2$\n",
        "\n"
      ],
      "metadata": {
        "id": "JpaGe--3uuJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self,target_feature):\n",
        "        super().__init__() # Remark the difference in writing in the Inheritence init. It basically doing the same\n",
        "        self.target = ...\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO : Compute the distance between the Gram Matrix of the Target and the Input\n",
        "        self.loss = ...\n",
        "\n",
        "        return input"
      ],
      "metadata": {
        "id": "ecepaBjLydED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now that we have defined our loss, we need something to test in on. Let's import a pretrained model.\n",
        "\n",
        "We will follow the paper, and load VGG19 model.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*6U9FJ_se7SIuFKJRyPMHuA.png\">"
      ],
      "metadata": {
        "id": "r_9zmrcx1jpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing a PreTrained Model\n",
        "\n",
        "As a Deep Learning Engineer, you'll often use a model that was already trained for a specific task. Let's import VGG19. TorchVision has a huge library of model that could be loaded with pretrained weights.\n",
        "\n",
        "* Import Pretrained VGG19 model. Have a look at : https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "Don't forget to add .eval() at the end as you do not want to change any weight of this model."
      ],
      "metadata": {
        "id": "XDjTaUrf2bGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ..."
      ],
      "metadata": {
        "id": "zENNPJlO26KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look on what composes a VGG19 Model.\n",
        "* Name every Child Module of the VGG19 model.\n",
        "\n",
        "In fact, we won't need every layer of this model. \n",
        "* Assign only the 'feature' extractor to your model.\n",
        "\n",
        "Don't forget to put your model into evaluation mode. \n",
        "* What is evaluation mode ? What happens to the model's weights ?\n",
        "\n",
        "Print the model and make sure that you kept only the feature extractor."
      ],
      "metadata": {
        "id": "a7sDExiE3AXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "4zMwWYwp6ZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some specification on VGG"
      ],
      "metadata": {
        "id": "IexPF9cMpG6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This VGG was trained on images that were normalized in a specific manner. \n",
        "The mean and the std of each channels is stored in two variables for your knowledge  \n",
        "\n",
        "* Create a Normalization layer that inherits from nn.Module.\n",
        "The normalization should substract the mean and divide with the std"
      ],
      "metadata": {
        "id": "JYtCmW_z6n5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "model_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        # View is used to reshape the Tensors to a specific format.\n",
        "        # Why do we resize it to -1 ,1 ,1 ?\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Normalize IMG\n",
        "        return "
      ],
      "metadata": {
        "id": "fxzgx2ZA61G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get to the most interesting part. The style transfer."
      ],
      "metadata": {
        "id": "x1Tqc7Uz9_PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III - The Main Stuff : Style Transfer\n",
        "\n",
        "Now that we defined all we needed, we need to recreate our model and add the \"losses\" after each Convolutional Layer.\n",
        "The Style Transfer Class will takes as attributes :\n",
        "* a model\n",
        "* a content image\n",
        "* a style image\n",
        "* a number of step for GD\n",
        "\n",
        "We have defined few more useful attributes for this class that you will understand further."
      ],
      "metadata": {
        "id": "fcDqiitG_DZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Method\n",
        "\n"
      ],
      "metadata": {
        "id": "cjzomPI_5ZGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First method is a method to copy the layer of a model.\n",
        "The method : \n",
        "* takes as input an empty model, a layer and an iterative parameters i\n",
        "* returns the model with the layer added, the name of the layer and i\n",
        "\n",
        "You can indeed define it as a @staticmethod."
      ],
      "metadata": {
        "id": "ENJhZ4zRJ4re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Method\n",
        "Second method is a method to add the Style Loss and Content Loss at places that are specified in the content layer and style layers attributes. The method takes as input :\n",
        "* the new model\n",
        "* the name of the layer\n",
        "* content loss and style loss \n",
        "* and the iterative i\n",
        "\n",
        "If the model layer's name is in the specified layer, add to the model the Content of Style layer. \n",
        "Append the Content or Style loss.\n",
        "\n",
        "For the moment, as no input data was forwarded through the model, you should see no values."
      ],
      "metadata": {
        "id": "j2fC5_PGQCpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third Method\n",
        "\n",
        "Third method is used to trim the model. As we just want to keep the layers before the Content of Style Losses, we trim the model so that it doesn't keep what's after the Content and Style layers.\n",
        "\n",
        "We wrote the method for you don't worry.\n",
        "\n",
        "\n",
        "* Can we write it as a @StaticMethod ?\n",
        "\n"
      ],
      "metadata": {
        "id": "EyHvC-XmQvU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourth Method\n",
        "\n",
        "Fourth method builds the new model for us. In fact, the most important method is this one as it sums up all the previous layers.\n",
        "To the new model attribute, we will apply all the previous methods in order to build our style transfer model.\n",
        "* What do you think the steps of this algo will be ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yRQaXUH-RV6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fifth Method\n",
        "\n",
        "Fifth method is our optimizer. As in the original paper, we will use l-BFGS algorithm to run the GD.\n",
        "We wrote the method for you don't worry. Usually, we use Gradient Descent in order to train the model (aka update the weights of the model). Here we will be using it to minimize the content/style losses.\n",
        "\n",
        "* What is an optimizer ?\n",
        "* Can we write it as a @staticmethod"
      ],
      "metadata": {
        "id": "v37rnNphTdKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Bos.. Method\n",
        "\n",
        "HMPF. That was a long ride. Now this is our final method. Aka the Most important method.\n",
        "\n",
        "In fact this method will be used to run the style transfer on an input image. We feed the model an input that is updated and we compute new losses. The input image could be something else than the content image.\n",
        "\n",
        "As you might see, this method contains a closure function. Isn't that weird ? a Function in a Function ?? This is a nested function. In this case it's a Closure. It uses values that were defined in the enclosing function. It reevaluates the module and returns the loss."
      ],
      "metadata": {
        "id": "dYWcxY8lUSom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Style_Transfer():\n",
        "\n",
        "    def __init__(self,model, content_image, style_image,num_steps =500):\n",
        "\n",
        "        # Some Useful Attributes \n",
        "        self.model_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device) \n",
        "        self.model_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        self.content_layers = ['conv_4']\n",
        "        self.style_layers= ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "        # Some useful Constant attributes\n",
        "        self.steps = 0 \n",
        "        self.num_steps =num_steps\n",
        "        self.style_weight=1000000\n",
        "        self.content_weight=1\n",
        "\n",
        "        #TODO : load the model to device \n",
        "        #TODO : create a new Sequential model with the first layer being the Normalization layer \n",
        "        self.model= ...\n",
        "        self.new_model = ...\n",
        "        \n",
        "        # TODO : assign as attributes the content_image and style_image        \n",
        "        self.content_image = ...\n",
        "        self.style_image = ...\n",
        "\n",
        "\n",
        "    def copy_model(self,model,layer,i):\n",
        "        # First Method \n",
        "        # TODO : Check if the layer is a Layer in VGG and keep the layer name with i.\n",
        "        # To Understand : What kind of Layer are present in the VGG19 Model ?\n",
        "        # Every time the layer is a conv layer, the iterative parameters is incremented\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += ...\n",
        "            name = 'conv_{}'.format(i)\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = 'relu_{}'.format(i)\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, ....):\n",
        "            name = .format(i)\n",
        "        elif isinstance(layer,..\n",
        "                        \n",
        "        else:\n",
        "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "        #TODO : add the layer to the new model with corresponding name. have a look at https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "        model......\n",
        "\n",
        "        # Return the model, the name of the layer, and i\n",
        "        return model, name ,i\n",
        "\n",
        "\n",
        "    def add_module(self,model, name,content_losses, style_losses,i):\n",
        "        # Second Method \n",
        "        # TODO : if name is in Content of Style Layer: send the Content of Style image through the new model\n",
        "        #        Compute the content loss\n",
        "        #        Add to the model the needed Layer\n",
        "        # We wrote the example for Content Layer and We append for you the needed loss\n",
        "        if name in self.content_layers:\n",
        "            target = model(self.content_image).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in self.style_layers:\n",
        "            ....\n",
        "\n",
        "\n",
        "            style_losses.append(style_loss)\n",
        "        return model , content_losses, style_losses\n",
        "\n",
        "\n",
        "    def trim(self,model):\n",
        "        # Third Method \n",
        "        # The trimmming of the new model.\n",
        "        for i in range(len(model) - 1, -1, -1):\n",
        "          if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "              break\n",
        "        model = model[:(i + 1)]\n",
        "        return model\n",
        "\n",
        "    \n",
        "    def get_model_and_losses(self):\n",
        "      # Fourth Method \n",
        "      # Defining our much needed content and style losses list that will keep the loss and our iterative i\n",
        "      content_loss, style_loss = [],[]\n",
        "      i=0 \n",
        "      # TODO : iterate through the original model in order to build the new model.\n",
        "      for ... in ...:\n",
        "          # TODO : Copy to the new model the previous model\n",
        "          self.new_model , name ,i= ...\n",
        "          # TODO : Add the Content and Style layers to the new model and \n",
        "          self.new_model, content_losses, style_losses = ...\n",
        "      # TODO : Trim the built new model\n",
        "      self.new_model = ...\n",
        "\n",
        "      return self.new_model, content_loss, style_loss\n",
        "\n",
        "\n",
        "    def optimizer_img(self,input_img):\n",
        "        # Fifth Method \n",
        "        # This is our optimizer. It takes the input_img for optimisation\n",
        "        optimizer = optim.LBFGS([input_img])\n",
        "        return optimizer\n",
        "      \n",
        "\n",
        "    def run(self,input_image):\n",
        "        # Final Boss\n",
        "        # TODO : Build the model, and content and style loss\n",
        "        self.new_model,content_loss,style_loss  = ...\n",
        "\n",
        "        # Now we set require grad to the model and input image, and call our optimizer\n",
        "        input_image.requires_grad_(True)\n",
        "        self.new_model.requires_grad_(False)\n",
        "        optimizer = self.optimizer_img(input_image)\n",
        "\n",
        "        # TODO : Iterate and optimize the Closure function\n",
        "        while ... <= ...:\n",
        "          def closure():\n",
        "                # We clamp the image to 0,1\n",
        "                with torch.no_grad():\n",
        "                    input_image.clamp_(0, 1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # TODO : forward the input image through the new model\n",
        "                ...\n",
        "\n",
        "                # As the input image was forwarded through the model, the Content Loss and Style Loss \n",
        "                # values were updated. Let's compute the scores. We can add the score by iterating through\n",
        "                # the content_loss and style_loss lists.\n",
        "                style_score = 0\n",
        "                content_score = 0\n",
        "\n",
        "                for sl in style_loss:                  \n",
        "                    style_score += sl.loss\n",
        "                for cl in content_loss:\n",
        "                    content_score += cl.loss\n",
        "\n",
        "                # TODO : Mulitply the scores by the weights and define the loss by adding the scores\n",
        "                ....\n",
        "\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # TODO : Iterate through the steps attribute and show the losses\n",
        "                ....+= 1\n",
        "                if ... % 50 == 0:\n",
        "                    print(\"run {}:\".format(...))\n",
        "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                        style_score.item(), content_score.item()))\n",
        "                    print()\n",
        "\n",
        "                return style_score + content_score\n",
        "\n",
        "          # Now we call the optimizer to the closure.\n",
        "          optimizer.step(closure)\n",
        "        \n",
        "        # And we clamp the image again\n",
        "        with torch.no_grad():\n",
        "          input_image.clamp_(0, 1)\n",
        "\n",
        "        return input_image\n"
      ],
      "metadata": {
        "id": "42ZKSmB6KKAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing \n",
        "\n",
        "Alright. Now that everything is put along, we can test the Style Transfer. In order to send the images through the models, we need to apply some transformations.\n",
        "* Define a Compose transformation that resizes the image to a 512x512 size and converts the image to a Tensor. Have a look at : https://pytorch.org/vision/stable/transforms.html\n",
        "* Load and run the Style Transfer\n",
        "\n",
        "Try out different Style Transfer and Show us the results."
      ],
      "metadata": {
        "id": "gDzsfg1UOKCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imsize = ...\n",
        "\n",
        "loader =    # TODO : Resize the Input Image to size 512,512\n",
        "            # TODO : Transform into Tensor"
      ],
      "metadata": {
        "id": "kKqIIyLmaoMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(image_name):\n",
        "    image = Image.fromarray(image_name)\n",
        "    # Fake batch dimension required to fit network's input dimensions\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)"
      ],
      "metadata": {
        "id": "OqBEaxO9aldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL_image = \"https://www.google.com/url?sa=i&url=https%3A%2F%2Fpytorch.org%2Ftutorials%2Fadvanced%2Fneural_style_tutorial.html&psig=AOvVaw2mRWB9GRpXgVTu3MNbg0aI&ust=1643068786074000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJCMkc2KyfUCFQAAAAAdAAAAABAD\"\n",
        "URL_image = 'https://www.google.com/url?sa=i&url=https%3A%2F%2Fjiweibo.github.io%2FNeural-Style-Transfer%2F&psig=AOvVaw2mRWB9GRpXgVTu3MNbg0aI&ust=1643068786074000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJCMkc2KyfUCFQAAAAAdAAAAABAU'"
      ],
      "metadata": {
        "id": "qByizs7nYc6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = image_loader(cv2.cvtColor(skimage.io.imread(URL_content ),cv2.COLOR_BGR2RGB))\n",
        "style_image = image_loader(cv2.cvtColor(skimage.io.imread(URL_style ),cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "oXgyNw510jtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a Device for GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# TODO : Make sure that your model is only the features part of VGG19\n",
        "model = ....\n",
        "\n",
        "# TODO : Define some Random Noise for Style Transfer\n",
        "input_image = ...\n",
        "\n",
        "# TODO : Create a Style Transfer Object and run Style Transfer on a input_image. Try out some White noise\n",
        "output = ..."
      ],
      "metadata": {
        "id": "qgQOIEGt6cOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Show the results"
      ],
      "metadata": {
        "id": "E0TgH596TEzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
        "    image = image.squeeze(0)      # remove the fake batch dimension\n",
        "    image = unloader(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001) \n",
        "imshow(output, title='Output Image')\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 4\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "weDqW2QyTFK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isn't that beautiful ? Now try many styles."
      ],
      "metadata": {
        "id": "ieTHbkk0nk_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More explanation\n",
        "\n",
        "In fact, the goal of this lab is to make you use DL Network and Code.\n",
        "\n",
        "Normally, you might have an intuition of what happens in the model during this style transfer process. In fact, the more you go through the layers of your model, the higher the features will be. What does it mean ? It means that the more you step into your model, the more you'll see object part (eyes, dog booty, cup of tea...). That also means that the model understands the input image by 'seeing' the objects. These features are used for classification. This also means that the first layers will be low level features (edge...), aka feature extraction.\n",
        "\n",
        "By accessing the intermediate layers, we are able to have a compromise between extracted features and image understanding, allowing us to describe the content and the style of the input images.\n",
        "\n",
        "Well Done !\n",
        "\n",
        "<img src=\"https://media0.giphy.com/media/l0ErFafpUCQTQFMSk/giphy.gif\">\n",
        "\n",
        "If you were an engineer, the first thing you would do is research about the topic. You'll quickly stumble upon this paper : https://arxiv.org/pdf/1508.06576.pdf. With the understandings you have now, have a look at the paper.\n",
        "\n",
        "This paper from Gatys et al. is basically was what we were going to be implementing."
      ],
      "metadata": {
        "id": "Gg4EbrhyfyWI"
      }
    }
  ]
}