{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating a Recommendation System.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwPpisnoxQ6+BWC5udoqYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/2A/Option%20IA/TP3/Creating_a_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1b9XN-AD2ph6Akdy4gpWdx8rIlK6HZ-u9\n",
        "!pip install pytorch-lightning\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "from scipy.sparse.csgraph import laplacian \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from scipy.sparse import coo_matrix\n",
        "import scipy.sparse as sparse\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn import preprocessing\n",
        "import torch \n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "LEAsm6M-PtqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before beginning anything :    \n",
        "* Go on the following link : https://drive.google.com/drive/folders/11jRhJ9febEg7eQ8aGx0x0P8iqLpVwCNS?usp=sharing \n",
        "* Read the README file"
      ],
      "metadata": {
        "id": "xNNTMCb-HK7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation Systems\n",
        "\n",
        "A Recommendation System is a system able to recommend an item for a user depending on either :\n",
        "* the content watched by the user\n",
        "* the behavior of a user on items\n",
        "\n",
        "In this lab, we will see two different Recommendation System Algorithm."
      ],
      "metadata": {
        "id": "vE-RoQVUgZWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal of this lab\n",
        "\n",
        "* Understand Recommendation Systems\n",
        "* Build a Graph Recommendation System\n",
        "* Build a Matrix Factorisation Recommendation System\n",
        "* Use Pytorch Lightning\n",
        "* Understand the importance of Embeddings in Deep Learning"
      ],
      "metadata": {
        "id": "Z1vTWehb4On4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Dataset\n",
        "\n",
        "For this lab, we will be using a famous dataset. We will use the Amazon Luxury Dataset. The data are loaded into a Pandas Dataframe. Let's analyze and understand the dataset.\n",
        "\n",
        "\n",
        "\n",
        "* What are the Dataframes keys ?\n",
        "* How many items are there ? How many users are there ?\n",
        "* What's the span of the ratings ?\n",
        "* Plot the number of Ratings that are present. Is it a Gaussian Distribution ? A Skewed Gaussian Distribution ? \n",
        "* What could it mean on the quality of the products ? \n"
      ],
      "metadata": {
        "id": "mM-mBmqnNX8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying it on a real Dataset\n",
        "\n",
        "\n",
        "* Using pivot methods, transform your dataframe to a Rating Matrix."
      ],
      "metadata": {
        "id": "LDYo4IEX4LKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Read the downloaded CSV file \n",
        "df = pd.read_csv(...)\n",
        "# TODO : Answer the previous questions \n",
        "..."
      ],
      "metadata": {
        "id": "S7h4vWCP8D5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Matrix Factorization\n"
      ],
      "metadata": {
        "id": "WYzv3TXP4fgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Theory\n",
        "\n",
        "Matrix Factorization is an easy yet powerful recommendation algorithm. In fact, it is a simple embedding model. To remind you, an embedding is another representation of a vector, in a lower dimension space. The good thing about Matrix Factorization is its usability for either content based recommendation or collaborative filtering based recommendation. This is one of the earliest model used by Netflix. \n",
        "\n",
        "Matrix Factorization finds latent structure in data such as it decomposes a Feedback Matrix $A\\in \\mathbb{R}^{M*N}$ with M the number of users and n the number of items, into :     \n",
        "* $P$ :  a user embedding matrix $\\in \\mathbb{R}^{M*K}$\n",
        "* $Q$ : an item embedding matrix $\\in \\mathbb{R}^{N*K}$\n",
        "\n",
        "The embeddings are learned such that \n",
        "$ \\hat{A} = P*Q^{T} \\approx A$\n",
        "\n",
        "We define the Objective Function as \n",
        "\n",
        "$ \\underset{P \\in \\mathbb{R}^{M*K},  Q\\in \\mathbb{R}^{N*K}} min\\underset{i,j\\in{obs}}\\sum(A_{ij} - \\hat{A{ij}})^{2} $ over $P$ and  $Q$\n",
        "\n",
        "To update the values of the matrixes, we use the typical Gradient Descent. In fact, each element of P and Q are updated using the following equations :\n",
        "\n",
        "$p_{ik} = p_{ik} + 2\\alpha*error* q_{kj}$\n",
        "\n",
        "$q_{kj} = q_{kj} + 2\\alpha*error* p_{ik}$\n",
        "\n"
      ],
      "metadata": {
        "id": "tc1j53ZLoc_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Where do the $p_{ik}$ and $q_{kj}$ update formula come from ?"
      ],
      "metadata": {
        "id": "-mV3Fc3KWX0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to code the Matrix Factorization Class that will perform matrix factorization for us.\n",
        "\n",
        "The class has 4 methods :\n",
        "* the init : for attribute initialization\n",
        "* the error : to compute the error\n",
        "* the update : to update the elements of P and Q \n",
        "* the run : to perform the calcultation for a num_step of times"
      ],
      "metadata": {
        "id": "-znBCm4QWs5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Matrix_Factorization():\n",
        "  \"\"\"\n",
        "  This class performs Matrix Factorization for a given FeedBack Matrix\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,A, latent_dim, steps= 5000, learning_rate = 0.0002):\n",
        "\n",
        "      self.A = A\n",
        "      self.latent_dim = latent_dim\n",
        "      self.steps = steps\n",
        "      self.learning_rate = learning_rate\n",
        "\n",
        "      # TODO : Initialize the correct attributes\n",
        "      # M,N are the shape of the FeedBack Matrix\n",
        "      self.M, self.N  = ...\n",
        "\n",
        "      # TODO : Initialize two random matrix P and Q. Don't forget to Transpose Q\n",
        "      self.P = ...\n",
        "      self.Q = ...\n",
        "\n",
        "  def error(self, a, b):\n",
        "      # TODO : Return the error : the difference between a and b\n",
        "      return ...\n",
        "\n",
        "  def update(self):\n",
        "    # TODO : Perform element update using gradient descent\n",
        "    # TODO : Iterate among the user\n",
        "    for user in range(...):\n",
        "        # TODO : Iterate among the items\n",
        "        for item in range(...):\n",
        "          if self.A[user][item]> 0:\n",
        "            # TODO : Compute the error between the GT Rating and the Predicted Rating of user on item\n",
        "            error = ....\n",
        "            # TODO : Using the gradient descent formula update the values of the elements of P and Q\n",
        "            for latent in range(self.latent_dim):\n",
        "                self.P[user][latent] = ...\n",
        "                self.Q[latent][item] = ...\n",
        "\n",
        "  \n",
        "  def run(self):   \n",
        "    # TODO : Call the update method 'steps' times   \n",
        "    for step in range(self.steps) : \n",
        "        ...\n",
        "    return np.dot(self.P, self.Q)\n"
      ],
      "metadata": {
        "id": "ZJQZrovMq4Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing it on Collaborative Filtering\n",
        "\n",
        "We give you this R matrix. As you can see the R matrix contains some outliers (Nan values)\n",
        "* How can we handle the outliers in this R matrix, what changes should you do ?\n",
        "* Define a function that handles the outliers of your input matrix."
      ],
      "metadata": {
        "id": "nXfGbG-71nVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not TODO : Define a Rating Matrix. An example of Rating Matrix is given\n",
        "R = np.array([[np.NaN,5,1,0,4],\n",
        "              [0,0,1,0,3],\n",
        "              [0,5,0,2,np.NaN],\n",
        "              [1,np.NaN,0,5,1] ])\n",
        "\n",
        "# TODO : Define a function that handles the outliers of the R matrix. Hint nan_to_num \n",
        "def correct_outliers(R):\n",
        "    return ...\n",
        "\n",
        "# TODO : Verify that your function works\n",
        "R = ...\n"
      ],
      "metadata": {
        "id": "bsqeJlhp1lXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Create an instance of the Matrix_Factorization Class\n",
        "latent_dim = ...\n",
        "MF = Matrix_Factorization(A = ...,\n",
        "                          latent_dim=...)"
      ],
      "metadata": {
        "id": "JZE5Inac3Fae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Calculate the approximated Rating Matrix using the run method\n",
        "mat = ..."
      ],
      "metadata": {
        "id": "qP0MA4Dt5Hyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on the Luxury Dataset\n",
        "\n",
        "Now let's test our model on the Luxury Dataset. As you've seen, there's too much data for this Algorithm to handle. The computation time would be too much. We are going to restrict the dataset for the 50 first elements.\n",
        "In order to check the capacity of the algorithm. We masked few ratings to Nan values. The goal is to check if the model would be able to recapture the original rating."
      ],
      "metadata": {
        "id": "TFEsDZN4Ch_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Keep the 50 first elements of the df dataframe\n",
        "mf_df = df.head(...)\n",
        "\n",
        "mf_df_train = mf_df.copy()"
      ],
      "metadata": {
        "id": "Rn84GbtzHKIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we apply some Random Nan values\n",
        "random.seed(10)\n",
        "for i,value in enumerate(mf_df_train['Rating']):\n",
        "  if random.randint(0,5)==5:\n",
        "      mf_df_train['Rating'].loc[i] = np.NaN"
      ],
      "metadata": {
        "id": "mIUTeJhWE7AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mf_df_train dataframe corresponds to the mf_df dataset but with some nan values in the Rating column. \n",
        "* Create a Rating Matrix using 'UserId' as Index and 'ProductId' as Columns.\n",
        "* Initialize well the values of the Rating Matrix and convert it to numpy array.\n",
        "* Run Matrix Factorization on the given Matrix for a feature space of size 2"
      ],
      "metadata": {
        "id": "Fhqty0jDPN7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mf_df_train\n",
        "\n",
        "# TODO : Using pivot method, create a Rating Matrix , with index : UserId and Columns : ProductId\n",
        "R = mf_df_train.pivot(index=..., columns=..., values=...)\n",
        "# TODO : Using a function defined previously, initialize the Nan values to 0\n",
        "R = ...\n",
        "# TODO : Run Matrix Factorization for a latent space of dimension 2 \n",
        "latent_dim = ...\n",
        "MF = Matrix_Factorization(R,latent_dim)\n",
        "R_pred = MF.run()"
      ],
      "metadata": {
        "id": "XfPpiYZiGCH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's answer the following questions : \n",
        "* Will user 6 like item 9 ?\n",
        "* Will user 4 like item 2 ?\n",
        "* What do you think of this model ? (Is it fast ? Is it efficient ? Does it need lot of engineering ?)"
      ],
      "metadata": {
        "id": "YO76Gn7hQjY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it is hard to find a real way to compute the metrics of the model. Here, we just looked if the model is able to find masked items."
      ],
      "metadata": {
        "id": "Z9DWmwOgGkzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III -  Deep Learning on Graphs : Graph Neural Collaborative Filtering\n",
        "\n",
        "What we computed previously is really hard to scale at human size. Indeed, if you take the example of all the students at ENSEA (around 500 at a moment), the MF model will take long time before assessing a result that might not be optimal. Hence, we are going to see much more efficient model that uses graph.\n",
        "We will use Pytorch-Lightning to create and train the model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f37DzWKq5VyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewriting the Dataset\n",
        "\n",
        "We will use the same Dataset as before. But, once again we will transform it to a PyTorch Dataset, to easen the next steps. As the Dataset Letters in the UserId and ProductId columns, we will encode the columns to numbers. It is the same as in the TD :)"
      ],
      "metadata": {
        "id": "Q3zH-bryJU8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Because of the limitation in Google Colab, we will only use 100 000 values.\n",
        "df = df.head(100000)\n",
        "\n",
        "# TODO : Using LabelEncoder transform the columns to numerical values.\n",
        "UserEncoder = ...\n",
        "ItemEncoder = ...\n",
        "\n",
        "df.UserId =  UserEncoder.fit_transform(df.UserId.values)\n",
        "df.ProductId =  ItemEncoder.fit_transform(df.ProductId.values)\n",
        "\n",
        "# TODO : Write the Dataset. The Dataset should return a dictionnary of user, movie, ratings.\n",
        "class AmazonLuxury(Dataset):\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    # TODO : Retrieve a row from the df Dataframe using iloc \n",
        "    row =  ...\n",
        "    # TODO : Return a dictionnary {'user': , 'item':, 'rating':}. Don't forget to convert it to tensors.\n",
        "    return {'user': torch.tensor(row[...], dtype=torch.long),\n",
        "              'item':  torch.tensor(row[...], dtype=torch.long),\n",
        "              'rating':  torch.tensor(row[...], dtype=torch.float)}\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # TODO : Return the length of the Dataset\n",
        "    return ...\n",
        "\n",
        "\n",
        "# TODO : Create an instance of your Dataset and verify if it returns the correct piece of data.\n",
        "dataset = AmazonLuxury(...)"
      ],
      "metadata": {
        "id": "Z85GspivJgmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the DataModule\n",
        "\n",
        "As we created the Dataset, we need some DataLoaders to send the Data to the Model for training. Let's use Pytorch Lightning to create a DataModule.\n",
        "\n",
        "What is a DataModule ? A Lightning DataModule is just a class that gathers all the DataLoaders used for training. It's more some code management class than anything. "
      ],
      "metadata": {
        "id": "8M8yindAxPBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LuxuryDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self,df ):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        # TODO : Define a batch size \n",
        "        self.batch_size_train, self.batch_size_valid, self.batch_size_test = ...\n",
        "        \n",
        "\n",
        "    def setup(self, stage):\n",
        "        train,test = train_test_split(df, test_size=0.2)\n",
        "        train, valid = train_test_split(train, test_size=0.2)\n",
        "        # We need to setup our module. We have a training set that we will be fitting in our model\n",
        "        # and a testing set used to test our models prediction.\n",
        "        # the stage variable corresponds to those two steps : \n",
        "        #         |fit\n",
        "        # stage = <test\n",
        "        #         |None  \n",
        "\n",
        "        # First stage is 'fit' (or None)\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.amazon_train, self.amazon_val =  AmazonLuxury(train),  AmazonLuxury(valid)\n",
        "\n",
        "        # Second stage is 'test' \n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.amazon_test =  AmazonLuxury(test)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # TODO : Create your Training DataLoader\n",
        "        return DataLoader( ..., batch_size=self.batch_size_train,drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # TODO : Create  your Validation DataLoader\n",
        "        return DataLoader( ..., batch_size= self.batch_size_valid,drop_last=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # TODO : Create your Testing DataLoader\n",
        "        return DataLoader( ..., batch_size= self.batch_size_test,drop_last=True)"
      ],
      "metadata": {
        "id": "0vBuCcq4xyG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Definitions\n",
        "\n",
        "\n",
        "An **Adjacency Matrix** is a matrix that indicates the links between nodes. In the case of User-Item Recommendation it is simply the  User-Item Interaction matrix. We will call the Adjancency Matrix $A$\n",
        "\n",
        "A **Diagonal Degree** Matrix is a matrix that represents the degrees for each nodes of a graph. The degree of a node is the number of edge linking a given node. A loop is counted 2 times in the degree. We will call the Diagonal Matrix $D$\n",
        "\n",
        "A **Laplacian Graph** Matrix is a matrix **that represents** the graph and relates to useful properties for a graph. We will call the Laplacian Graph Matrix L. An interesting property is : $L = D - A$\n",
        "\n"
      ],
      "metadata": {
        "id": "-J-b4uBd8us6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Run on NGCF\n",
        "\n",
        "<img src = \"https://miro.medium.com/max/776/1*6cBxhbNs9acjejFvsiqXMw.png\" height=400> \n",
        "\n",
        "\n",
        "During the TD, we've trained a model based on Deep Learning called NCF. The model learns to rate like user U on item I, by comparing and extracting complex features from both embeddings. NGCF adds an interesting part to this model : Graphs. \n",
        "\n",
        "The NGCF model has 3 main components : \n",
        "* Embedding Layer\n",
        "* Multiple Propagation Layers\n",
        "* Prediction Layers\n",
        "\n",
        "In comparison to the NCF model, we add the Multiple Propagation Layers. The goal of this specific layer is to propagate messages between nodes on a User Item Graph. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NGCF works thanks to the l-hop neighbors. A 1-hop neighborhood correspond to the nodes directly connected to the given node. A 2-hop neighborhood correspond to the node that are connected through the 1-hop neighborhood to a given node.\n",
        "<img src =\"https://d3i71xaburhd42.cloudfront.net/c5f5f179d80a3bf9b4f29750283a87eaca42e91b/2-Figure1-1.png\"  height=200>\n",
        "\n",
        "<img src = \"https://miro.medium.com/max/1400/0*rpiKmTCpuLkX7mbG\" height=300>\n",
        "\n",
        "For example, the collaborative signal i4 => u2 => i2 => u1 can be captured using the embedding propagation process. In fact, the message from i4 is encoded in the embedding of u1 from it 3-hop neighbors"
      ],
      "metadata": {
        "id": "pignzrfIlZBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Graph Model\n",
        "\n",
        "A Graph is an object that contains nodes and edges. Each nodes contains informations that is important. Each edges symbolizes the link between nodes. As usual, when working with GNN, we have an $aggregate function$ and a $update function$. In the case of GNCF, the goal of a Node is to aggregate information contained in the neighboring Nodes and update the central Node given the informations. When creating a GNN, we must be careful on the message-passing architecure. Indeed, it should be able to generalize to multiple succesive layers. Propagation is done using the following formulas :    \n",
        "\n",
        "The embedding for user $u$ after memorizing the messages from the $l$-hop neighborhood is :  \n",
        "\n",
        "$$\\mathrm{{e}}_{u}^{(l)}=\\mathrm{{LeakyReLU}}\\Bigl({\\bf m}_{u\\leftarrow u}^{(l)}+\\sum_{i\\in N_{u}}\\mathrm{m}_{u\\leftarrow i}^{(l)}\\Bigr)$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iRi8JQgEJz98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The messages $m$ being propagated are defined as follows\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\left\\{\\begin{array}{l}\n",
        "\\mathbf{m}_{u \\leftarrow i}^{(l)}=p_{u i}\\left(\\mathbf{W}_{1}^{(l)} \\mathbf{e}_{i}^{(l-1)}+\\mathbf{W}_{2}^{(l)}\\left(\\mathbf{e}_{i}^{(l-1)} \\odot \\mathbf{e}_{u}^{(l-1)}\\right)\\right) \\\\\n",
        "\\mathbf{m}_{u \\leftarrow u}^{(l)}=\\mathbf{W}_{1}^{(l)} \\mathbf{e}_{u}^{(l-1)}\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "with \n",
        "\n",
        "$\\mathbf{m}_{u \\leftarrow i}^{(l)}$ : the message (or information) from $i$ to $u$, from its $l-hop$ neighbors \n",
        "\n",
        "$\\mathbf{m}_{u \\leftarrow u}^{(l)}$ : the message from $u$ to $u$  from its $l-hop$ neighbors, also called the self loop.\n",
        "\n",
        "$p_{u i}$ : the first hop norm between user u and item i \n",
        "\n",
        "In fact, we can rewrite this as a matrix multiplication. \n",
        "\n",
        "For \n",
        "$$\n",
        "\\mathbf{E}=[\\underbrace{\\mathbf{e}_{u_{1}}, \\cdots, \\mathbf{e}_{u_{N}}}_{\\text {users embeddings }}, \\underbrace{\\mathbf{e}_{i_{1}}, \\cdots, \\mathbf{e}_{i_{M}}}_{\\text {item embeddings }}] .\n",
        "$$\n",
        "We have :\n",
        "$$\n",
        "\\mathrm{E}^{(l)}=\\operatorname{LeakyReLU}\\left((\\mathcal{L}+\\mathrm{I}) \\mathrm{E}^{(l-1)} \\mathbf{W}_{1}^{(l)}+\\mathcal{L} \\mathrm{E}^{(l-1)} \\odot \\mathrm{E}^{(l-1)} \\mathbf{W}_{2}^{(l)}\\right)$$\n",
        "\n",
        "with \n",
        "\n",
        "$\\mathcal{L}$ : the normalized Laplacian matrix of the User Item Graph\n",
        "\n",
        "$\\mathrm{E}^{(l)}$ : the Representations for user and items after l steps of message propagation\n",
        "\n",
        "$\\mathrm{R}$ : the Rating Matrix aka the User-Item Interaction Matrix\n",
        "$$\n",
        "\\mathcal{L}=\\mathrm{D}^{-\\frac{1}{2}} \\mathrm{AD}^{-\\frac{1}{2}} \\text { and } \\mathrm{A}=\\left[\\begin{array}{cc}\n",
        "0 & \\mathrm{R} \\\\\n",
        "\\mathrm{R}^{\\top} & 0\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "So after propagating for L layers, we have many representation for user u and for item i. By concatenating the embeddings, we enrich the intial embedding and control the range of propagation by adjusting to the number of layers we propagate through.\n",
        "$$\n",
        "\\mathbf{e}_{u}^{*}=\\mathbf{e}_{u}^{(0)}\\|\\cdots\\| \\mathbf{e}_{u}^{(L)}, \\quad \\mathbf{e}_{i}^{*}=\\mathbf{e}_{i}^{(0)}\\|\\cdots\\| \\mathbf{e}_{i}^{(L)}\n",
        "$$"
      ],
      "metadata": {
        "id": "ynUj47Z1UPcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enough With the Theory, Let's Practice\n"
      ],
      "metadata": {
        "id": "dtDnhGlZ_PPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The GNN\n",
        "\n",
        "The GNN Layer will forward the LaplacianMatrix (the messages from the neighbor nodes) and a SelfLoop matrix (the message from the same node). Each $W$ is a learnable weight that is updated through each updates.\n",
        "\n",
        "We are going to create a Graph Layer that reproduces \n",
        "$$\n",
        "\\mathrm{E}^{(l)}=\\operatorname{LeakyReLU}\\left((\\mathcal{L}+\\mathrm{I}) \\mathrm{E}^{(l-1)} \\mathbf{W}_{1}^{(l)}+\\mathcal{L} \\mathrm{E}^{(l-1)} \\odot \\mathrm{E}^{(l-1)} \\mathbf{W}_{2}^{(l)}\\right)$$\n",
        "\n",
        "\n",
        "* What known layer multiplies an input value by a learnable weight ? Linear ? Conv ? MultiHeadAttention ?\n",
        "* Create the GNN Model"
      ],
      "metadata": {
        "id": "9qQT9Xk0DpeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, in_features,  out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.linear = nn.Linear(in_features,out_features)\n",
        "        self.interActTransformation = nn.Linear(in_features,out_features)\n",
        "\n",
        "    def forward(self,features,LaplacianMat, selfLoop):\n",
        "\n",
        "        l1 = selfLoop+ LaplacianMat \n",
        "        l2 = LaplacianMat.cuda()\n",
        "        l1 = l1.cuda()\n",
        "        features = features.cuda() # E\n",
        "        # TODO : Recreate the above function. Be careful between the multiplication and the term by term multiplications. Hint: have a look at matmul and mul\n",
        "        # TODO : Reproduce the matrix multiplication \n",
        "        inter_features = torch.matmul(...,...)  # TODO : L*E\n",
        "        inter_features = torch.mul(...,...) # TODO : LE.*E\n",
        "\n",
        "        # TODO : Send the correct variables through the Linear models to recreate both part of the Sum \n",
        "        left_term = self.linear(torch.mm(...,...)) # TODO : (L+I)E*W\n",
        "        right_term = self.interActTransformation(torch.mm(...,...)) # TODO : (LE.*E)*W\n",
        "        return nn.LeakyReLU()(...)\n",
        "\n"
      ],
      "metadata": {
        "id": "AIXWuNI3MXfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGCF\n",
        "\n",
        "Now let's get our hands dirtier. We previously created our Graph Model. Now the are going to create our NGCF Model. As creating all the class would take a really long time, we gave you a skeleton to complete.\n"
      ],
      "metadata": {
        "id": "f_OSVD6j5zDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphNCF(pl.LightningModule):\n",
        "\n",
        "\n",
        "    def __init__(self,numberUser, numberItem, embeddingSize,df):\n",
        "      super().__init__()\n",
        "      self.numberUser = numberUser\n",
        "      self.numberItem = numberItem\n",
        "      self.df = df\n",
        "      self.layers = [embeddingSize, embeddingSize//2, embeddingSize//4]\n",
        "      self.embeddingSize = embeddingSize\n",
        "\n",
        "      # TODO : Define your Graph by calling one of the existing Methods. Hint : read the definitions given before\n",
        "      self.laplacian = ...\n",
        "      self.selfLoop = self.getSparseEye(self.numberUser + self.numberItem )\n",
        "\n",
        "      # TODO : Create an Embedding Layer for the Users and the Items (1)\n",
        "      self.userEmbed =  nn.Embedding(...,...)\n",
        "      self.itemEmbed =  nn.Embedding(...,...)\n",
        "      # Not TODO : Define your Propagation Layer (2). We define this part for you, it's just a for loop where you add the GNNs into a List. Each list being a L-Hop neighbor\n",
        "      self.GNNLayers = nn.ModuleList()\n",
        "      for From,To in zip(self.layers[:-1],self.layers[1:]):\n",
        "            self.GNNLayers.append(GNNLayer(From,To))\n",
        "      # TODO : Define your Prediction Layers (2). Hint : you might need to forward some data to see what the value should be\n",
        "      self.model = nn.Sequential(nn.Linear(...,...))\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO : Initialize your Optimizer and your Learning Rate (Adam, SGD... ) \n",
        "        optimizer = ...\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self,user, item):        \n",
        "        # TODO : Call the initial FeatureMatrix and clone it. This will be our initial message. To call the feature matrix, a method in this class is present to create it for you.\n",
        "        features = ...\n",
        "        finalEmbed = ... # TODO : clone the features tensor\n",
        "        # TODO : Create the richer embedding by sending your features through your GNNs. Don't forget the argument of your model\n",
        "        for gnn in self.GNNLayers:\n",
        "            features = gnn(...,...,...)\n",
        "            features = nn.ReLU()(features)\n",
        "            finalEmbed = torch.cat([finalEmbed,features.clone()],dim=1)\n",
        "        # Not TODO : Retrieve the correct embedding \n",
        "        userEmbed , itemEmbed = finalEmbed[user], finalEmbed[item]\n",
        "        Embed = torch.cat([userEmbed, itemEmbed], dim =1)\n",
        "        # TODO : Send the final Embedding through your prediction layes\n",
        "        pred = ...\n",
        "        return pred  \n",
        "        \n",
        "\n",
        "    def training_step(self,train_batch, batch_idx):\n",
        "        # TODO : Define your training step\n",
        "        # TODO : Define your Loss (MSE ? BCE ? Hinge Loss ? Focal Loss ?). Hint : Are we performing regression or classification or both ?\n",
        "        criterion = ...\n",
        "        # TODO : Retrieve the Data from the train_batch.\n",
        "        user = train_batch[...]\n",
        "        item = train_batch[...]\n",
        "        rating = ...\n",
        "        # TODO : Send the user and the item tensor to the model.\n",
        "        pred = ...\n",
        "        # TODO : Compute your Loss \n",
        "        loss = ...\n",
        "        return loss  \n",
        "\n",
        "\n",
        "    def validation_step(self,valid_batch, batch_idx):\n",
        "        # TODO : Define your validation step. How does it differs from the training step ?\n",
        "        criterion = ...\n",
        "        user, item, rating = ...\n",
        "        pred = ...\n",
        "        loss = ...\n",
        "\n",
        "    def test_step(self,test_batch, batch_idx):\n",
        "        # TODO : Define your test step. How does it differs from the training step ?\n",
        "        ...\n",
        "        \n",
        "    def AdjacencyMat(self):\n",
        "        user, product, rating = self.df['UserId'].values,self.df['ProductId'].values,self.df['Rating'].values\n",
        "        R = coo_matrix((rating, (user,product))).toarray()\n",
        "        N,M = R.shape\n",
        "        R_t = R.T\n",
        "        H1 = np.vstack([np.zeros((N,N)), R_t])\n",
        "        H2 = np.vstack([R, np.zeros((M,M))])\n",
        "        A = np.hstack([H1,H2])\n",
        "        del H1, H2\n",
        "        sumArr = (A>0).sum(axis=1)\n",
        "        diag = list(np.array(sumArr.flatten()))\n",
        "        diag = np.power(diag,-0.5)\n",
        "        D = np.diag(diag)\n",
        "        return A\n",
        "\n",
        "    def DiagonalMat(self,A):\n",
        "        sumArr = (A>0).sum(axis=1)\n",
        "        diag = list(np.array(sumArr.flatten()))\n",
        "        diag = np.power(diag,-0.5)\n",
        "        D = sparse.diags(diag)\n",
        "        return D\n",
        "\n",
        "    def LaplacianMat(self):\n",
        "        A = self.AdjacencyMat()\n",
        "        D = self.DiagonalMat(A)\n",
        "        L = D*A*D\n",
        "        L = coo_matrix(L)\n",
        "        i = torch.LongTensor([L.row,L.col])\n",
        "        data = torch.FloatTensor(L.data)\n",
        "        SparseL = torch.sparse.FloatTensor(i,data)\n",
        "        return SparseL\n",
        "\n",
        "    def getSparseEye(self,num):\n",
        "        i = torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])\n",
        "        val = torch.FloatTensor([1]*num)\n",
        "        return torch.sparse.FloatTensor(i,val)\n",
        "\n",
        "    def FeatureMat(self):\n",
        "        uidx = torch.LongTensor([i for i in range(self.numberUser)]).to(self.device)\n",
        "        iidx = torch.LongTensor([i for i in range(self.numberItem )]).to(self.device)\n",
        "        userEmbd = self.userEmbed(uidx)\n",
        "        itemEmbd = self.itemEmbed(iidx)\n",
        "        features = torch.cat([userEmbd,itemEmbd],dim=0)\n",
        "        return features"
      ],
      "metadata": {
        "id": "XH68b3c1cTqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training\n",
        "\n",
        "Now let's train the model. We normally have everything we need :     \n",
        "* a DataModule to retrieve Data for Training\n",
        "* a Model with all the training, testing, validation steps, an optimizer, a loss\n",
        "\n",
        "Now we can train. We are going to create a trainer. a Trainer is a class that will handle the training for us. For our convinence, Lightning has a built-in trainer that we can use. More information : https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
        "\n",
        "It makes thing really easy."
      ],
      "metadata": {
        "id": "iIsp-H7h9Nre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Create an instance of your DataModule\n",
        "# TODO : Create an instance of your Model\n",
        "model = ...\n",
        "dm = ...\n",
        "dm.setup('fit')\n",
        "# TODO : Initialize the Trainer. Train the Model for 15 epochs.\n",
        "trainer = pl.Trainer(gpus=-1,max_epochs=...,strategy='dp')\n",
        "\n",
        "# Not TODO : using the .fit method, Fit the data to the model\n",
        "trainer.fit(model, dm)\n"
      ],
      "metadata": {
        "id": "KlJ13UMd-j5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The testing\n",
        "\n",
        "Now that we trained our model, we can test it on unseen data. We are going to see how far our predictions are from the reality\n",
        "* Switch the DataModule to the test stage\n",
        "* Test the model."
      ],
      "metadata": {
        "id": "b4J4_tVY-LOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : setup the DataModule to the 'test' stage\n",
        "dm.setup(...)\n",
        "\n",
        "# TODO : Test \n",
        "trainer.test(datamodule = ...)"
      ],
      "metadata": {
        "id": "WC4zmsGzYo6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Is the model efficient ?\n",
        "* What could we do to have better results ?"
      ],
      "metadata": {
        "id": "HY0Nh8WR-urL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV - Do it again (Optional)\n",
        "\n",
        "In the previous part, we created a model able to regress the rating a user could give to an object. We made some rating predictions. \n",
        "\n",
        "Now we will change the model to know whether a user would click on an object or not.\n",
        "\n",
        "* Change the previous model so that the model infers whether a user would click or not on a given object.\n",
        "\n",
        "Hint :\n",
        "* What task is it ?\n",
        "* What should you change or add ?"
      ],
      "metadata": {
        "id": "qnf0UMT9_ph-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Oat2UIO2_o6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}