{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to DL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFaa7vpd7PRa"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 12PZB5hWpwAyRuSWAgkaFdZdZluGf8_6Z\n",
        "!gdown --id 1g1CEUs8BvSJXSDroHgjUbTa3izGp9WMO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disclaimer \n",
        "\n",
        "Before beginning this lab, please activate a GPU by going into :     \n",
        "- Exécution\n",
        "- Modifier le type d'éxecution\n",
        "- GPU "
      ],
      "metadata": {
        "id": "peN7xMkxh7y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNNs with TensorFlow\n",
        "\n",
        "Welcome to the Deep Learning Lab sessions. As you had a slight introduction to Deep Learning through the Course, you will now apply it through this izi pizi labs.\n",
        "This lab will be done using TensorFlow from Google. \n",
        "You will learn to do the following in TensorFlow: \n",
        "\n",
        "- Initialize variables\n",
        "- Start your own session\n",
        "- Train algorithms \n",
        "- Implement a Neural Network\n",
        "\n",
        "This part of the lab is splitted in two parts :\n",
        "- Using Tensorflow\n",
        "- Creating and Performing Classification using TF.\n",
        "\n",
        "<img src=\"https://www.memesmonkey.com/images/memesmonkey/3f/3fd47d627866ac3f67bc4a38b0b2941c.jpeg\">\n"
      ],
      "metadata": {
        "id": "dcOlbCv-7P0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Time Hun ?\n",
        "\n",
        "So this might be your first time using Deep Learning for a certain task. So in this lab we will perform a certain Task : Classification. We will classify hands. Hun.\n",
        "\n",
        "When training a DL Algorithm you need few things : \n",
        "* A Dataset\n",
        "* A Model\n",
        "* A Learning Algorithm\n",
        "* Some more things innit' ?\n",
        "\n",
        "So we will navigate through each of these points to train a Model that performs Hand Signs Classification.\n",
        "\n",
        "<img src = \"https://i.kym-cdn.com/photos/images/original/000/123/620/Oh-boy-here-we-go.jpg\">\n",
        "\n"
      ],
      "metadata": {
        "id": "B6t5QBmc7RIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Dataset : SIGNS"
      ],
      "metadata": {
        "id": "RVTDKEE9-Lgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We wil use SIGNS Dataset. Let's first have some insight look on our Dataset\n",
        "Load the train and test datasets\n"
      ],
      "metadata": {
        "id": "-jEdisCc94vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO : load the h5 files in the correct variables\n",
        "train_dataset = h5py.File('/content/train_signs.h5', \"r\")\n",
        "test_dataset = ...\n",
        "\n",
        "# TODO : print the keys of each variables. \n",
        "# Questions: Are the keys different ? \n",
        "print(train_dataset.keys())\n",
        "print(...)\n",
        "\n",
        "# TODO : create x_train, y_train, x_test, y_test and convert them to array using numpy\n",
        "# Questions : What is the length of each Dataset.\n",
        "# Questions : What are the labels, what are our images ?\n",
        "\n",
        "print(len(train_dataset))\n",
        "x_train = np.array(train_dataset['train_set_x'][:])/255 # What's the purpose of 255 ?\n",
        "y_train = np.array(train_dataset['train_set_y'][:])\n",
        "x_test = ...\n",
        "y_test = ...\n",
        "\n",
        "y_train = np.array(train_dataset['train_set_y'][:]).reshape((y_train.shape[0]))\n",
        "y_test = np.array(test_dataset['test_set_y'][:]).reshape((y_test.shape[0]))\n",
        "\n",
        "# TODO : load the classes in an array. \n",
        "# Questions : How many labels should you have ? (Count your fingers)\n",
        "classes = ..."
      ],
      "metadata": {
        "id": "4Zv5idtSAyaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : create the datasets using from_tensor_slices class using x and y \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(...)\n",
        "test_dataset = ..."
      ],
      "metadata": {
        "id": "cdzZlgq8L1av"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : define a Batch Size\n",
        "BATCH_SIZE = ...\n",
        "\n",
        "# Create mini batches using .batch method\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = ..."
      ],
      "metadata": {
        "id": "HBPUUn4ZL_bb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few questions on the Dataset: \n",
        "* How many elements do you have in your Train and Test Datasets ? Explain the shape of the Train Dataset\n",
        "* What is the size of one element of the Dataset ?\n",
        "* Are the images RGB images ?\n",
        "\n",
        "Plot some images from the train Dataset with their labels along using Matplotlib"
      ],
      "metadata": {
        "id": "4sViFgkcAgJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = ..\n",
        "plt.title(\"Label is: \" + str(np.squeeze(y_train[ index])))\n",
        "plt.imshow(x_train[index])\n"
      ],
      "metadata": {
        "id": "t8HqOI_TAkCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now you have some understanding of the Dataset. Let's try things"
      ],
      "metadata": {
        "id": "NBXOWo8ZAzE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II- The Model : Your Choice\n",
        "\n",
        "We are going to guide you a lil bit for this. In fact, you can create whatever model you want. However, they might not be as effective as some other networks.\n",
        "In this part, we will try Dense Layers.\n",
        "\n",
        "<img src = \"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/ANN-Graph.gif\" height=300 >\n",
        "\n",
        "\n",
        "* What is the other name of a Dense Layer ?\n",
        "\n",
        "\n",
        "To create this FC model, you have to understand few things :\n",
        "* The input Layer will have a Fixed number of Neurons that must correspond to the image's size.\n",
        "* Between the Layers, you must apply some non-linearity so that the model learns complex features.\n",
        "* You have to compromise between the Shallowness or Depth of your Model. You'll understand why in the next lab.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*E4_pTJctmAofSRpZCZbv-g.jpeg\" height=300>\n",
        "\n",
        "\n",
        "Let's create a simple Sequential Model :    \n",
        "* an Input Layer\n",
        "* a Hidden Layer\n",
        "* an Output Layer\n",
        "\n",
        "(Like the picture above Andrew)"
      ],
      "metadata": {
        "id": "nCynSEY9H7BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : We wrote a model for you but there are some mistakes. Find and Correct the mistakes. \n",
        "# Question : What is the input shape ?\n",
        "# Question : What is the output shape ?\n",
        "# Question : What non linearity must we add ?\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(11)),\n",
        "  tf.keras.layers.Dense(12288, activation='sigmoid'),\n",
        "  tf.keras.layers.Dense(128, activation='softmax'),\n",
        "  tf.keras.layers.Dense(10006, activation='relu')\n",
        "])"
      ],
      "metadata": {
        "id": "l8f_9xapH42z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some questions related to Memes on Activation Functions :\n",
        "\n",
        "<img src=\"https://2.bp.blogspot.com/-IdxHoo3lTrU/XHXNi8HM4_I/AAAAAAAAOhY/xTrp-Z8yYjY6NVBs-PXHw2Gho53vU90DgCLcBGAs/s1600/52386901_10157143758983669_1120348777576660992_o.jpg\" height = 400>\n",
        "\n",
        "*  How do you understand the ReLU function as an activation of a layer ?\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQGVkqoa4YitmdIj-pCt88odvoscPbgocSPDQ&usqp=CAU\" >\n",
        "\n",
        "* Can we use any activation functions ? Take the example of multiclass classification, can we use the same activation function in the last layer andin the earlier layers ?"
      ],
      "metadata": {
        "id": "QsKmzR598Hzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III- The Loss \n",
        "\n",
        "<img src=\"https://i.imgflip.com/63x65w.jpg\" height= 200>\n",
        "\n",
        "Now we need something to tell our model if it predicts well or no. Let's say your model predicts something. We need to compute how far the prediction is compared to the real label. The goal is to have a model that predicts well things.\n",
        "\n"
      ],
      "metadata": {
        "id": "DfsGRDejIMMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, Tensorflow already has coded a bunch of Losses for us. You can have a \n",
        "look at all the existing losses here :\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n"
      ],
      "metadata": {
        "id": "U_W5LW-v0vYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a loss that can measure the difference between two probability distributions. That has something to do with their entropy.\n",
        "* What loss will we be using here ?\n",
        "\n",
        "If you don't know how to answer the previous question, ask yourself these things :\n",
        "\n",
        "* Do we have labels ?\n",
        "* What task are we performing ? Regression or Classification ?\n",
        "* What must be the output of our model ?\n"
      ],
      "metadata": {
        "id": "pJFhbQaBBtPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = ..."
      ],
      "metadata": {
        "id": "mRGTfxuSJ35g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV - The Optimizer\n",
        "\n",
        "So at the end of the Model, we can compute a loss. We need to back-propagate the loss to all the layers so that it sees and understands the end result of a prediction and search for a Local-Global Optimum. You have plenty of optimizers available, the simplest is Gradient Descent. We pratically use Stochastic Gradient Descent in order to find a Global Minimum where the model should be performing 'well'.\n",
        "\n",
        "<img src=\"https://www.memecreator.org/static/images/memes/5296549.jpg\" height = 300>\n",
        "\n",
        "\n",
        "Have a look at the existing optimizers in TensorFlow :    \n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "\n",
        "We will use ADAM. In fact, we can use SGD or any other Optimizer. However, ADAM\n",
        "for Adaptive Moment Estimation is more robust than SGD. (and it has more things such as per parameter learning rate adaptation that improves performance on sparse gradient and other things (To quote Andrew NG : Don't worry about it if you don't understand it) \n",
        "\n",
        "<img src=\"https://i.pinimg.com/474x/7b/58/bb/7b58bb3b853ff61ed8873aeb711cb3b6.jpg\">"
      ],
      "metadata": {
        "id": "cZ0j9m8bISOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Load Adam optimizer and set a learning rate.\n",
        "learning_rate = ....\n",
        "optimizer = ..."
      ],
      "metadata": {
        "id": "rZt9Lv11J4PZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V - The Compilation\n",
        "\n",
        "Now that we have all we need, we have to compile the model with all the preceding stuff. \n",
        "\n",
        "That's where TensorFlow is cool. It has a compile method that compiles everything for you."
      ],
      "metadata": {
        "id": "q7DxtlUdIUfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=...,\n",
        "              loss=...,\n",
        "              metrics=['categorical_accuracy']) # metric are useful for calculation"
      ],
      "metadata": {
        "id": "ajW5ebUEDwuH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI- The Training \n",
        "\n",
        "When you train, you become fit. In Deep Learning, we fit a distribution of Data to the model so that it learns a specific task.\n",
        "\n",
        "Again, TensorFlow has a great method that fits data for you."
      ],
      "metadata": {
        "id": "7tjgUzaTIWU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = ...\n",
        "model.fit(...,\n",
        "          epochs=...)"
      ],
      "metadata": {
        "id": "o6hQFAbXKRzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AW.. It doesn't work ....\n",
        "\n",
        "\n",
        "<img src=\"https://media.npr.org/assets/img/2016/03/29/ap_090911089838_sq-3271237f28995f6530d9634ff27228cae88e3440-s900-c85.webp\" height = 200>\n",
        "\n",
        "In fact, it is normal. Let's have a closer look on our Dataset and our Model. "
      ],
      "metadata": {
        "id": "mk4qWRGsAROH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII - The Investigation \n",
        "\n",
        "<img src=\"https://www.meme-arsenal.com/memes/aab2bd04fcd6bc0642f0ab60718bac0d.jpg\" height=200>"
      ],
      "metadata": {
        "id": "X2I3gC1gReDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzcI2iwZOXrV",
        "outputId": "ba2988f4-88b4-4e72-f3f1-811feb31d536"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 0 2 ... 2 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you might see, the labels are numbers between 0,5. The model will have issues to predict these numbers. That's quite complicated for it. In fact, we put a SoftMax activation layer at the end. The SoftMax makes the model behaves as follows : \n",
        "\n",
        "Let's say that the Last Layer has 6 Neurons. We want to train the model so that \n",
        "when the model sees an image of 0 the 1rst neuron has the highest value. When it sees an image of 1 the 2nd neurons has the highest value.. and so on.\n",
        "\n",
        "That means that we need to change our label to something that helps us to learn to activate the wanted neuron for an image.\n",
        "\n",
        "Let's do some 1-Hot Encoding.\n",
        "\n",
        "<img src= \"https://sandipanweb.files.wordpress.com/2018/01/signs_data_kiank1.png?w=676\">\n",
        "\n",
        "Basically, we are going to map each values to a one-hot-encoded vector, so that the output layers correct neurons ouputs the highest value.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5IdPwIY4O7CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Create a function that 1-Hot encodes a number into a given size vector. Or have a look at this : https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
        "\n",
        "y_train_oh = tf.one_hot(y_train, ...)\n",
        "y_test_oh = ...\n",
        "\n",
        "# TODO : Print the 1-Hot encoded labels and the original labels and show us the difference\n",
        "\n",
        "print(y_train_oh)\n",
        "print(y_train)\n",
        "\n",
        "# TODO : Recreate your train_dataset with one hot encoded vectors\n",
        "train_dataset_oh = ...\n",
        "test_dataset_oh = ...\n",
        "\n",
        "\n",
        "# TODO : Recreate your mini batches\n",
        "BATCH_SIZE = ...\n",
        "\n",
        "train_dataset_oh  = train_dataset_oh.batch(BATCH_SIZE)\n",
        "test_dataset_oh = test_dataset_oh.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "srSx1hy9SjlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIII - The Retraining after Debugging\n",
        "Ok let's try again"
      ],
      "metadata": {
        "id": "pkhXKvVofHfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=...,\n",
        "              loss=...,\n",
        "              metrics=['categorical_accuracy'])\n",
        "\n",
        "num_epochs = ....\n",
        "history = model.fit(train_dataset_oh,epochs=num_epochs)"
      ],
      "metadata": {
        "id": "dSWXqkl7fHvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot some things using the history variable.\n",
        "History logs everything that occurs during training.\n",
        "\n",
        "Let's explore what it registered.\n",
        "* Print the values of history\n",
        "* Using Pyplot, plots the differnets keys in the history variable"
      ],
      "metadata": {
        "id": "HdeNqZXcPTzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO : print the values of the history variables\n",
        "print(history....)\n",
        "\n",
        "# TODO : define accuracy and loss\n",
        "acc = history....\n",
        "loss = history....\n",
        "\n",
        "# TODO : plot the accuracy and the loss\n",
        "# Question : did the model learn anything ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJFhKNrVPkH5",
        "outputId": "8bdf1a12-8ba1-451c-918c-a0db7a595317"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [1.4936935901641846, 1.4361672401428223, 1.3995248079299927, 1.3962522745132446, 1.3949995040893555], 'categorical_accuracy': [0.3861111104488373, 0.42407408356666565, 0.4546296298503876, 0.4407407343387604, 0.4194444417953491]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IX - The Evaluation"
      ],
      "metadata": {
        "id": "fYvRK1IOIdkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we trained the model, let's evaluate it on the test dataset. Again, TensforFlow has this great .evaluate() method that evaluates your model to a dataset.\n",
        "\n",
        "* Evaluate your trained model to the Test Dataset."
      ],
      "metadata": {
        "id": "i2i4gJOFxIUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Evaluate your model on the test dataset.\n",
        "model.evaluate(...)"
      ],
      "metadata": {
        "id": "3c2J-281u96p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hun. Some values that we don't really understand ? Ok let's plot some images to see the mistakes made by the model.\n",
        "\n",
        "<img src=\"https://i.imgflip.com/63vr6e.jpg\" height=200>"
      ],
      "metadata": {
        "id": "8PRpbw0bOl2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "\n",
        "# TODO : send all element to the model to predict\n",
        "\n",
        "for element in train_dataset_oh.as_numpy_iterator():\n",
        "  for image in range(element[0].shape[0]):\n",
        "    \n",
        "    img_array = tf.keras.utils.img_to_array(...)\n",
        "    img_array.resize((64, 64,3))\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    my_image_prediction = ...\n",
        "    # TODO : print and plot the Non Good predictions\n",
        "    if ...:\n",
        "      print(\"Your algorithm predicts: y = \" + ...)\n",
        "      print(\"GT Label is: y = \" + ...)\n",
        "      plt.imshow(np.squeeze(img_array))\n",
        "      plt.show()\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "FUUy0MZA1VA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh it works ! Well ? As you can see, the model still makes some mistakes and it is totally normal. Nothing can be perfect. We want to minimize these mistakes as much as possible. This is were creating specific models is useful. \n",
        "\n",
        "<img src=\"https://static.toiimg.com/thumb/msid-62318428,width-400,resizemode-4/62318428.jpg\">\n",
        "\n"
      ],
      "metadata": {
        "id": "ILKM8zU2OC7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X - To go further (Optional)\n",
        "\n",
        "Let's try it on images taken from the internet."
      ],
      "metadata": {
        "id": "HdMn0T6FREQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "image_filename = r\"\" #URL of image\n",
        "image_numpy = cv2.cvtColor(skimage.io.imread( image_filename ),cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(image_numpy)\n",
        "\n",
        "img_array = tf.keras.utils.img_to_array(image_numpy/255)\n",
        "img_array.resize((64, 64,3))\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "my_image_prediction = ...\n",
        "\n",
        "print(\"Your algorithm predicts: y = \" + str(np.argmax(np.squeeze(my_image_prediction))))"
      ],
      "metadata": {
        "id": "AwWyYlHsQGsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Hmm. Not Working ? Working ? What could be the causes ?\n",
        "\n",
        "<img src=\"https://i.imgflip.com/63vsoc.jpg\" height= 400>\n",
        "\n",
        "* Explain what overfitting is ? Why could overfitting occur in this model ?\n",
        "* How can we verify that the model overfits or not ?\n",
        "* How could we remedy overfitting ?"
      ],
      "metadata": {
        "id": "UMr8YIvSQWJx"
      }
    }
  ]
}