{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTOrt8R9mURa"
      },
      "source": [
        "# Lab 1 : Introduction to Deep Learning and Neural Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzuQhL9ql1Bi"
      },
      "source": [
        "## About this lab\n",
        "\n",
        "Deep Learning models often seems like black boxes that no one understands... In fact, we can actually go inside these model to understand what the model learned. Think of it as if you were looking at some coffee grounds and were trying to construe an object from it.\n",
        "\n",
        "We are going to do some Style Transfer. You know that thing on Snapchat where you can add a Comic Filter on your face... We are basically redoing that kind of style transfer in order to understand what DL is.\n",
        "\n",
        "Goal on this lab :\n",
        "* See how a model works\n",
        "* Import a Pre Trained Model\n",
        "* Code in class using Python\n",
        "\n",
        "\n",
        "Alright, let's get started.\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/16/b2/96/16b296afb78ec57d12c931bc72b42eec.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv6lbpbru8_2"
      },
      "source": [
        "## Which DL Framework to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UXqySPXgX6j"
      },
      "source": [
        "### Popular frameworks\n",
        "\n",
        "Here is an non-exhaustive list of open source Python Framework \n",
        "\n",
        "| Software   | **Creator**                                                         | **Initial release** | **Interface**                                                | **Written in**       |\n",
        "|------------|---------------------------------------------------------------------|---------------------|--------------------------------------------------------------|----------------------|\n",
        "| Theano     | Montréal University                                                 | 2007                | Python (Keras)                                               | Python               |\n",
        "| Caffe      | Berkeley Vision and Learning Center                                 | 2013                | Python, MATLAB, C++                                          | C++                  |\n",
        "| Chainer    | Preferred Networks                                                  | 2015                | Python                                                       | Python               |\n",
        "| TensorFlow | Google Brain                                                        | 2015                | Python (Keras), C/C++, Java, Go, JavaScript, R, Julia, Swift | C++, Python, CUDA    |\n",
        "| Keras      | François Chollet                                                    | 2015                | Python, R                                                    | Python               |\n",
        "| PyTorch    | Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan (Facebook) | 2016                | Python, C++, Julia                                           | Python, C, C++, CUDA |\n",
        "| JAX        | Google Research                                                     | 2018                | Python                                                       | Python, C, C++       |\n",
        "\n",
        "\n",
        "For now, PyTorch is still the \"research\" framework and TensorFlow is still the \"industry\" framework. In fact, many companies are switching into cloud based solutions. Most companies were using Google cloud services. And you can understand quickly why Tensorflow was used a lot (because Tensorflow based Model are easily integrable into an existing Company Architecture based on Google cloud). However it tends to change as more and more companies are switching to Amazon's AWS that can handle many DL frameworks.\n",
        "\n",
        "<img src=\"https://preview.redd.it/p62rqqidzi581.png?width=747&format=png&auto=webp&s=9c3b19ecc9c1386f6706f5b03e905280610ee81e\" style=\"max-height: 700px;\">\n",
        "\n",
        "While more job listings seek users of TensorFlow\n",
        "\n",
        "<img src=\"https://preview.redd.it/lcvzxrwmik581.png?width=747&format=png&auto=webp&s=e669f33897491225e0e793ae452b7ff64da17dee\" style=\"max-height: 700px;\">\n",
        "\n",
        "If you are interested, [here](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/) a more thorough analysis of the relevant differences between Pytorch and Tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ZC6Nlfir-w"
      },
      "source": [
        "### Why using Pytorch in this lab ?\n",
        "\n",
        "As the focus of this lab is on Deep Learning theory and understanding the under-the-hood of Deep Learning models, we will be using [PyTorch](https://pytorch.org/docs/stable/index.html) throughout this lab.\n",
        "\n",
        "Ideally, you should get exposure to Tensorflow framework (lab 1), and dedicate some time to understanding the differences between the frameworks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Introducing Deep Learning with a Simple Task\n",
        "\n"
      ],
      "metadata": {
        "id": "AXPui4ITtZIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Neuron\n",
        "\n",
        "Let's introduce a Simple Task. \n",
        "Given a boolean value (0 or 1), we want the model to invert the input value. \n",
        "Example : if the model's input is 0, we want it to output 1.\n",
        "\n",
        "In order to perform this task, we will need few elements:\n",
        "* a Dataset\n",
        "* a Model\n",
        "* a Training/Testing Loop\n",
        "* Some hyperparameters"
      ],
      "metadata": {
        "id": "KJdIaGITzHRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a - The Simple Dataset and Dataloader\n",
        "\n",
        "As you know, to train a model you will need data. In practice before choosing/creating a model, we usually have a look on the Dataset. These datas come in the form of labeled or unlabeled data. \n",
        "\n",
        "In Pytorch, datasets inherits from the Dataset class. It is a simple class composed of minimum 3 methods :\n",
        "* __init__ : to initialize the class\n",
        "* __getitem__ : to retrieve a sample according to a index number\n",
        "* __len__ : to return the len of the entire Dataset\n",
        "\n",
        "In our case, we will generate a list of 0 and 1. The __getitem__ method should return the opposite value of the element picked at the given index."
      ],
      "metadata": {
        "id": "2nh8xJZ0EWhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDataset(Dataset):\n",
        "\n",
        "  def __init__(self, len_data):\n",
        "\n",
        "    self.len_data = len_data\n",
        "    self.data = [random.randint(0,1) for i in range(len_data)] # This list of length len_data is filled with 0 and 1 \n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    # We select as data the value at index idx of the self.data list\n",
        "    data = self.data[idx]\n",
        "    label = ... if self.data[idx]== 0 else ...\n",
        "    # TODO : Return a dictionnary {'data': .., 'label':}\n",
        "    return {\"data\":torch.as_tensor(), \n",
        "            \"label\": torch.as_tensor()}\n",
        "\n",
        "  def __len__(self):\n",
        "    # Explanation : We know that the total length of the dataset is the length of the self.data attribute\n",
        "    return len(self.data)\n",
        "\n",
        "# TODO : Create dataset_train and dataset_test by initializing the Classes. You can choose a big value for the size of the list\n",
        "len_dataset_train = len_dataset_test= ...\n",
        "dataset_train = SimpleDataset(len_dataset_train)\n",
        "dataset_test = SimpleDataset(len_dataset_test)"
      ],
      "metadata": {
        "id": "QgwLl3CvzxGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataloader is used to fetch batches of data to send them at the same time to the GPU. A DataLoader needs a batch size. Other attributes of the DataLoader class exists but we won't use them."
      ],
      "metadata": {
        "id": "TLToTyilGSu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = DataLoader(dataset_train, batch_size= 2048)\n",
        "dataset_test = DataLoader(dataset_test, batch_size= 2048)"
      ],
      "metadata": {
        "id": "7bx2poGf2Dw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b - The Simple Model\n",
        "\n",
        "First time coding a neural net ? Let's think a little bit.\n",
        "Questions : \n",
        "* **What kind of task is it ?**\n",
        "*  **Is one neuron enough to perform the inversion of a boolean ?**\n",
        "* **Given an input x, a weight b, a bias b and an activation function f, how do we modelize a Single Neuron ?**"
      ],
      "metadata": {
        "id": "hiwHPWo8GdKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Model in PyTorch is pretty simple. We initalize a Class that inherits from nn.Module. \n",
        "\n",
        "It is defined as follows:\n",
        "\n",
        "\n",
        "```\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,...):\n",
        "    \"\"\"\"\n",
        "    Defines the model. You can put the input size as a parameter if needed..\n",
        "    \"\"\"\"\n",
        "    super().__init__() #to init the main class\n",
        "    self.layers = ... # defining the model : could be Conv2d, Linear, RNN, LSTM\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    The input x is forwarded through the neural net. \n",
        "    \"\"\"\n",
        "    output = self.layers(x)\n",
        "    return output\n",
        "```\n",
        "\n",
        "More informations : https://pytorch.org/docs/stable/nn.html\n",
        "\n"
      ],
      "metadata": {
        "id": "44xzhLcRHCru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a nn.Parameter, create a single Neuron model.**"
      ],
      "metadata": {
        "id": "y5M_a6_VdOp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self,):\n",
        "      super().__init__()\n",
        "      # Note : We initialize randomly the values of the weights and the biases as these are learned during training\n",
        "      self.w = nn.Parameter(torch.tensor(random.random()), requires_grad =True) # Weight\n",
        "      # TODO : by looking at the weight parameter (self.w) initialize the bias\n",
        "      self.b =  # Bias\n",
        "      # TODO : add a Sigmoid Activation layer\n",
        "      self.sigmoid = \n",
        "\n",
        "    def forward(self,x):\n",
        "      # TODO : Using the famous formula of a single neuron, compute the value of x1 : the output of the neuron\n",
        "      x1 = \n",
        "      # TODO : Pass x1 through the activation layer\n",
        "      x1 = \n",
        "      return x1"
      ],
      "metadata": {
        "id": "GdburjUE1Ktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Create an instance of the SimpleModel and print the layers it has using print()\n",
        "model = \n",
        "print(model)"
      ],
      "metadata": {
        "id": "LtBBFyV-2ebo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c- The Simple Training\n",
        "\n",
        "As of now, we are supposed to have the model and the dataset. Hence, we need to create the training loop and initialize some useful object for the training."
      ],
      "metadata": {
        "id": "MU-01GdZIZN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The training loop is defined as follows :\n",
        "\n",
        "\n",
        "```\n",
        "for epoch in num_epoch : \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):        \n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i == ??:\n",
        "          running_loss= running_loss/??\n",
        "\n"
      ],
      "metadata": {
        "id": "mw1xLpjqIeDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there are some obscure words that we didn't define yet."
      ],
      "metadata": {
        "id": "YSXJ5iD1I7_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i - Optimizer\n",
        "\n",
        "Optimizers are algorithms or methods used to minimize an error function (loss function) or to maximize the efficiency of production. A learning rate must be defined. The learning rate handles the step of the Gradient Descent update. Here, we will use stochastic gradient descent (SGD)"
      ],
      "metadata": {
        "id": "-bwijU5hJDm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "MGSEliitJap_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii - Criterion \n",
        "\n",
        "This is basically the loss function. It measures the differences between the output and the input. We want to minimize it. \n",
        "\n",
        "\n",
        "**Which Loss function will we be using in this Binary Classification Task ?**"
      ],
      "metadata": {
        "id": "jLMiRb1cJI-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Initialize a criterion. Choose between BCELoss or MSELoss. More Information in https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "criterion = "
      ],
      "metadata": {
        "id": "mDYAyh-sKDr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii - Num Epoch \n",
        "\n",
        "The number of epoch corresponds to the number of time, the model will see the samples from the dataset"
      ],
      "metadata": {
        "id": "64JbigwpJMmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Choose a number of epochs \n",
        "num_epochs = "
      ],
      "metadata": {
        "id": "EUiLjFurKOuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iv - Device \n",
        "\n",
        "Once the model initialized, we prefer accelerating the training. Hence, we send it to the GPU to profit from the parralelisation property of GPUs (Coucou HSP). To send it you have to define a device as follows:\n",
        "\n",
        "```\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "```\n",
        "To send your model or data to device : \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = model.to(device) or data = data.to(device)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qOy1JTJ_JSMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-yz0buH5KejY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v - Gathering everything under a Loop\n",
        "\n",
        "Let's gather everything under a Training loop and a Testing loop.\n",
        "* The training loop will update the weights of the model\n",
        "* The testing loop will only test the model without updating anything.\n",
        "\n",
        "\n",
        "**Fill in the blanks**"
      ],
      "metadata": {
        "id": "XXzsX8KsK8aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Training Loop :\n",
        "# Define Device :\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# TODO : Create an Instance of the Model\n",
        "model = ....to(device)\n",
        "\n",
        "# Define your device : \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# TODO : Define your optimizer. Don't forget your learning rate :\n",
        "lr = ...\n",
        "optimizer = ...\n",
        "\n",
        "# TODO : Define your criterion :\n",
        "criterion = ...\n",
        "\n",
        "# TODO : Define your number of epochs :\n",
        "n_epoch = ...\n",
        "\n",
        "# Initializing some stuff for visualization\n",
        "loss_train, loss_test = [], []\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataset_train, 0):\n",
        "        # TODO : Get the inputs. data is a dictionnary with keys : data and label\n",
        "        inputs, labels = ... .to(device), ... .to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO : Send the input through the model\n",
        "        outputs = model(...)\n",
        "\n",
        "        # TODO : Compute the loss between the ouputs and the targets\n",
        "        loss = criterion(..., labels.float())\n",
        "\n",
        "        # update the weights using backward and optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 19:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss train: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    loss_train.append(running_loss/i)\n",
        "\n",
        "    # Explanation : We don't need gradients here as we are not updating anything. This we call torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(dataset_test, 0):\n",
        "        # TODO : Get the inputs. data is a dictionnary with keys : data and label\n",
        "        inputs, labels =....to(device), ...\n",
        "\n",
        "        # TODO : Send the input through the model\n",
        "        outputs = model(...)\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 19:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss test: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    loss_test.append(running_loss/i)\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "qoRHPfMLK_QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vi - Let's visualize some stuff\n",
        "\n",
        "As you could see we have to lists where we logged loss values each epochs. Let's plot them to see if the model is overfitting/underfitting.\n",
        "* **What can you tell on your model's training ?**\n",
        "* **Can we actually tell if the model is overfitting or not ? Why ?**"
      ],
      "metadata": {
        "id": "XLbnpF_wOc0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.title('Loss')\n",
        "plt.plot(...)\n",
        "plt.plot(...)"
      ],
      "metadata": {
        "id": "UDfHmMkeMkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Neural Style Transfer"
      ],
      "metadata": {
        "id": "x45hba6stdxX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjakeZuAnCLN"
      },
      "source": [
        "## Objective of Style Transfer\n",
        "\n",
        "If you ever wished to paint your house like Van Gogh, puts away your paintbrushes and takes out the gpu because Style Transfer also referred to as Neural Style Transfer (NST) allows you to create such Art using deep learning.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/767/1*B5zSHvNBUP6gaoOtaIy4wg.jpeg\">\n",
        "\n",
        "This technique was first outlined in Leon A. Gatys’ paper : [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)\n",
        "\n",
        "In this lab will learn how to compose images in the style of another image by implementing NST using pytorch. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6ZqfLCp6bz"
      },
      "source": [
        "## Principle\n",
        "\n",
        "<img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-example.jpg\">\n",
        "\n",
        "\n",
        "The principle is simple: \n",
        "\n",
        "The network have this two input:\n",
        "* A content Image (C)\n",
        "* A style Image (SD)\n",
        "\n",
        "and output\n",
        "\n",
        "* A generated Image (G)\n",
        "\n",
        "We define two distances, one for the content ($D_{CD}$) and one for the style ($D_{SD}$).\n",
        "* $D_{CD}$  measures how different the content is between two images\n",
        "* $D_{SD}$ measures how different the style is between two images. \n",
        "\n",
        "Then, we take a third image, the input, and transform it to minimize both its content-distance with the content-image and its style-distance with the style-image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_lpEVvTwDaC"
      },
      "source": [
        "## Theory\n",
        "\n",
        "Let \n",
        "* $\\vec{p}$ be the content image\n",
        "* $\\vec{a}$ be the style image\n",
        "* $\\vec{x}$ the image that is generated \n",
        "* $P^{l}$ be the representation of $\\vec{p}$ in layer $l$\n",
        "* $A^{l}$ be the representation of $\\vec{a}$ in layer $l$\n",
        "* $F^{l}$ be the representation of $\\vec{x}$ in layer $l$\n",
        "\n",
        "A given input image $\\vec{x}$ is encoded in each layer of the $\\mathrm{CNN}$ by the filter responses to that image. A layer with $N_{l}$ distinct filters has $N_{l}$ feature maps each of size $M_{l}$, where $M_{l}$ is the height $H_{l}$ times the width $W_{l}$ of the feature map. So the responses in a layer $l$ can be stored in a matrix $F^{l} \\in \\mathcal{R}^{N_{l} \\times M_{l}}$\n",
        "* $F_{i j}^{l}$ is the activation of the $i^{t h}$ filter at position $j$ in layer $l$.\n",
        "\n",
        "**Content Loss**\n",
        "\n",
        "The content loss is defined as a squared-error loss between the two feature representations\n",
        "$$\n",
        "\\mathcal{L}^{l}_{\\text {content }}(\\vec{p}, \\vec{x})=\\text {MSE}\\left(F^{l}, P^{l} \\right) =\\frac{1}{N_{l} M_{l}} \\sum_{i, j}\\left(F_{i j}^{l}-P_{i j}^{l}\\right)^{2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text {content }}(\\vec{p}, \\vec{x})=\\sum_{l=0}^{L} w^{l}_{\\text {content }} \\mathcal{L}^{l}_{\\text {content }}(\\vec{p}, \\vec{x})\n",
        "$$\n",
        "\n",
        "Gradient descent is performed on a white noise image to find another image that matches the feature responses of the content image.\n",
        "\n",
        "Thus we can change the initially random image $\\vec{x}$ until it generates the same response in a certain layer of the CNN as the content image $\\vec{p}$\n",
        "\n",
        "**Style Loss**\n",
        "\n",
        "A style representation is defined as the correlations between the different filter responses. These feature correlations are given by the\n",
        "Gram matrix \n",
        "\n",
        "$$G^{l} \\in \\mathcal{R}^{N_{l} \\times N_{l}}$$\n",
        "\n",
        "where $G_{i j}^{l}$ is the inner product between the vectorised feature map $i$ and $j$ in layer $l:$\n",
        "$$\n",
        "G_{i j}^{l}=\\frac{1}{M_{l}} \\sum_{k} F_{i k}^{l} F_{j k}^{l}\n",
        "$$\n",
        "\n",
        "To generate a texture that matches the style of a given image, gradient descent is performed from a white noise image to find another image that matches the style representation of the style image. \n",
        "\n",
        "This is done by minimising the mean-squared distance between the entries of the Gram matrix from the style image and the Gram matrix of the image to be generated:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}^{l}_{\\text {style }}(\\vec{a}, \\vec{x})=\\text {MSE}\\left(G^{l}, A^{l} \\right)=\\frac{1}{N_{l}^{2}} \\sum_{i, j}\\left(G_{i j}^{l}-A_{i j}^{l}\\right)^{2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text {style }}(\\vec{a}, \\vec{x})=\\sum_{l=0}^{L} w^{l}_{\\text {style }} \\mathcal{L}^{l}_{\\text {style }}(\\vec{a}, \\vec{x})\n",
        "$$\n",
        "\n",
        "**Loss**\n",
        "\n",
        "The global loss function that is minimised is\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text {total }}(\\vec{p}, \\vec{a}, \\vec{x})=\\alpha \\mathcal{L}_{\\text {content }}(\\vec{p}, \\vec{x})+\\beta \\mathcal{L}_{\\text {style }}(\\vec{a}, \\vec{x})\n",
        "$$\n",
        "\n",
        "where $\\alpha$ and $\\beta$ are the weighting factors for content and style reconstruction respectively\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgflip.com/64t8v0.jpg\" height=400>"
      ],
      "metadata": {
        "id": "hBIW_Vz1MCcu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTuWsJfWxWJS"
      },
      "source": [
        "## Coding the Neural Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8E2QkoXxc6p"
      },
      "source": [
        "### Importing all the lib needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIlKqB6-MaZb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# usual python lib\n",
        "import copy\n",
        "import skimage\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# torchvision for data augmentation and predefined models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "YouTubeVideo(\"KbDFCn29BGE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0nGyj5yyYw2"
      },
      "source": [
        "In Pytorch we need to choose which device to run the network on and import the content and style images. Running the neural transfer algorithm on large images takes longer and will go much faster when running on a GPU. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device` for use throughout the tutorial. Also the `.to(device)` method is used to move tensors or modules to a desired device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOrsQLWIyRzA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputing image\n",
        "\n",
        "<img src=\"https://memegenerator.net/img/instances/73596388/machine-learning-data.jpg\">\n",
        "\n",
        "DeepLearning without data won't do much\n",
        "\n",
        "So let start by feeding data.\n",
        "\n",
        "\n",
        "In order to send the images through the models, we need to apply some transformations. Have a look at : https://pytorch.org/vision/stable/transforms.html\n",
        "* Define a `loader` which is a Compose transformation that resizes the image to 512x512 size and converts the image to a Tensor.\n",
        "* Define `unloader` that convert a tensor to a PIL Image"
      ],
      "metadata": {
        "id": "kmHJqbntaa5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using a small size if no gpu\n",
        "imsize = 512 if torch.cuda.is_available() else 128  \n",
        "\n",
        "# TODO : Resize the Input Image to size 512,512\n",
        "# TODO : Transform into Tensor\n",
        "loader = transforms.Compose([\n",
        "    transforms.?, # TODO\n",
        "    transforms.? # TODO\n",
        "])\n",
        "\n",
        "# TODO : Convert to a PIL Image\n",
        "unloader = transforms.? # TODO"
      ],
      "metadata": {
        "id": "xTfj5hIsbSDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here some image utilies already implemented for you"
      ],
      "metadata": {
        "id": "wfgPpHnibXN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(url):\n",
        "    image_cv2_bgr = skimage.io.imread(url)\n",
        "    # convert it to RGB\n",
        "    image_cv2_rgb = cv2.cvtColor(image_cv2_bgr, cv2.COLOR_BGR2RGB)\n",
        "    # convert it to a PIL Image\n",
        "    image = Image.fromarray(image_cv2_rgb)\n",
        "    # Fake batch dimension required to fit network's input dimensions\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    # return the image to right device and type\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "def image_unloader(tensor):\n",
        "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
        "    image = image.squeeze(0)      # remove the fake batch dimension\n",
        "    image = unloader(image)\n",
        "    return image\n",
        "\n",
        "def tensors_show(tensors, titles=None):\n",
        "    ''' display a list of tensors '''\n",
        "    n = len(tensors)\n",
        "    assert titles is None or len(titles) == n, \"Titles should have the same length as tensors\"\n",
        "    fig, axs = plt.subplots(1, n)\n",
        "    for i, (ax, tensor) in enumerate(zip(axs, tensors)):\n",
        "        image = image_unloader(tensor)\n",
        "        ax.imshow(image)\n",
        "        title = titles[i] if titles is not None else f'Image {i}'\n",
        "        ax.set_title(title)\n",
        "        ax.set_axis_off()"
      ],
      "metadata": {
        "id": "vE7xI4hybSxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose whatever content and style image you like as long as they have the same width and height."
      ],
      "metadata": {
        "id": "zhaFSaipbqw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_url = ?\n",
        "style_url = ?\n",
        "\n",
        "content_image = image_loader(content_url)\n",
        "style_image = image_loader(style_url)\n",
        "\n",
        "tensors_show(\n",
        "    [content_image, style_image], \n",
        "    titles=['Content', 'Style']\n",
        ")"
      ],
      "metadata": {
        "id": "HfdWQ3FgbkW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Losses\n",
        "\n",
        "So we need to define two losses \n",
        "* StyleLoss\n",
        "* ContentLoss\n",
        "\n",
        "***Some explanation***\n",
        "\n",
        "* When you learn to differentiate Pikachu from Jigglypuff, you build a representation where features (forms, texture, colors...) are extracted from the objects.\n",
        "* For a neural net, when classification is performed, images (in other words bunch of pixels) are used to construct that type of representation by extracting features from the input image. From image, the model creates a representation.\n",
        "\n",
        "When training a model using ConvNets, they develop an understanding of the objects features. The more layers you have, the more complex the extracted features will be. \n",
        "\n",
        "Thus, we can separate the representation with the content (pixel value) and style (texture informations). So to compute the style and the content, we need to look at different intermediate layers of our model.\n",
        "\n",
        "\n",
        "Well to sum up :     \n",
        "* Extracted Features are not the same in the earlier and later layers of the model\n"
      ],
      "metadata": {
        "id": "H3FCPCczTJ2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Content Loss\n",
        "\n",
        "Imagine that you have a neural net composed of layers. We need a function that can represent the content of a layer. In other words we want to compute the distance between the oinput image at layer L and the content image at layer L. In other words, how far our Input Image is from the Content Image.\n",
        "\n",
        "* What it the name of this distance ?\n",
        "\n"
      ],
      "metadata": {
        "id": "IXgB-lL1S-Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(a, b):\n",
        "    return F.? # TODO"
      ],
      "metadata": {
        "id": "WlVoz-xKQHM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Style Loss\n",
        "\n",
        "The style loss is used to compute the style of a layer. Hmmm what does it mean ? We want to use a feature space designed to capture texture information. \n",
        "* How can we compute that ?\n",
        "\n",
        "The style of an image can be computed using the mean and the correlation accross feature maps. In fact, we are going to compute the Gram Matrix of a layer. Style Features tend to be in deeper layers of the network.\n",
        "\n",
        "* Define a Gram Matrix function (1rst year course, Analyse de Fourier 1, chp2 : Espace de Hilbert) that returns a Gram Matrix on an input.\n",
        "\n",
        "* Do we need some normalization ? If yes, why ?\n",
        "\n",
        "<img src =\"https://github.com/enggen/Deep-Learning-Coursera/raw/1407e19c98833d2686a0748db26b594f3102301e/Convolutional%20Neural%20Networks/Week4/Neural%20Style%20Transfer/images/NST_GM.png\">\n",
        "\n",
        "\n",
        "So for some explanation, the Gram Matrix compares how a bunch of vectors ($v_1, v_2...$) are similar by computing their dot product. In our case, it computes how active a filter is. It means that for the different types of features existing in the model, the Gram Matrix computes :\n",
        "* in the diagonal, the different types of features\n",
        "* in the non diagonal, how different the features are.\n",
        "\n",
        "Let's define a Style Loss \n",
        "\n",
        "The Style Loss returns the distance between the Gram Matrix of the target and the Input."
      ],
      "metadata": {
        "id": "tHWzd_dSS0pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(input):\n",
        "    ''' compute the normalized gram matrix of input tensor of dim 4'''\n",
        "    # batch_size : batch size (=1)\n",
        "    # n_C : number of feature maps\n",
        "    # (n_W, n_H) : dimensions of a f. map (N=n_W*n_H)\n",
        "    batch_size, n_C, n_W, n_H = input\n",
        "\n",
        "    # reshape input of size (batch_size, n_C, n_W, n_H) \n",
        "    # to features of shape (batch_size * n_C, n_W * n_H)\n",
        "    # features should be a matrix (2 dim)\n",
        "    features = ? # TODO\n",
        "\n",
        "    # compute the gram product\n",
        "    # G should be of shape (batch_size * n_C, batch_size * n_C)\n",
        "    G = ? # TODO\n",
        "\n",
        "    # 'normalize' the values of the gram matrix\n",
        "    # by dividing by the number of element in each feature maps.\n",
        "    G = ? # TODO\n",
        "\n",
        "    return G\n",
        "\n",
        "def style_loss(a, b):\n",
        "    return ? # TODO"
      ],
      "metadata": {
        "id": "cCr9Ytx5S1q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing a PreTrained Model\n",
        "\n",
        "Ok now that we have defined our loss, we need something to test in on. Let's import a pretrained model.\n",
        "\n",
        "We will follow the paper, and load VGG19 model.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*6U9FJ_se7SIuFKJRyPMHuA.png\">\n",
        "\n",
        "As a Deep Learning Engineer, you'll often use a model that was already trained for a specific task. Let's import VGG19. TorchVision has a huge library of model that could be loaded with pretrained weights.\n",
        "\n",
        "* Import Pretrained VGG19 model. Have a look at : https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "Don't forget to add .eval() at the end as you do not want to change any weight of this model.\n",
        "\n",
        "Let's have a look on what composes a VGG19 Model.\n",
        "* Name every Child Module of the VGG19 model.\n",
        "\n",
        "In fact, we won't need every layer of this model. \n",
        "* Assign only the 'feature' extractor to your model.\n",
        "\n",
        "Don't forget to put your model into evaluation mode. \n",
        "* What is evaluation mode ? What happens to the model's weights ?\n",
        "\n",
        "Print the model and make sure that you kept only the feature extractor."
      ],
      "metadata": {
        "id": "wroREcA0Tb7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained vgg19 model\n",
        "cnn = models.? # TODO\n",
        "\n",
        "# keep only its 'features' part\n",
        "cnn = ? # TODO\n",
        "\n",
        "# send the model to the device\n",
        "cnn = ? # TODO \n",
        "\n",
        "# enable evaluation mode\n",
        "cnn = ? # TODO \n",
        "\n",
        "# display each layers\n",
        "for i, layer in enumerate(cnn.children()):\n",
        "    print(i, layer)\n",
        "\n",
        "# verification, the assertion should not be raised !\n",
        "assert isinstance(layer, nn.MaxPool2d)"
      ],
      "metadata": {
        "id": "14m2OHZcY5Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From which layer (index and type) comes the following feature ?\n",
        "* `print` and analyze its shape\n"
      ],
      "metadata": {
        "id": "g_ql_2DKcH6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature = cnn(content_image)\n",
        "\n",
        "# TODO\n",
        "print( ? )"
      ],
      "metadata": {
        "id": "HqRvRixwcE7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the features extractor\n",
        "\n",
        "When using a pretrained model, one should normalized inputs using the same weight and std. Using the mean `(0.485, 0.456, 0.406)` and std `(0.229, 0.224, 0.225)`of Imagenet is a common practice. They are calculated based on millions of images. If you want to train from scratch on your own dataset, you can calculate the new mean and std. Otherwise, using the Imagenet pretrained model with its own mean and std is recommended.\n",
        "\n",
        "* Why the mean and std comes with 3 values ?\n",
        "\n",
        "We want to be able to extract different feature with `FeaturesModel` class.\n",
        "The `forward` shound returns a dict with each feature from layers defined by `registered_layers`\n",
        "\n",
        "For instance, for `registered_layers=['conv_2', 'conv_5']`, the forward should return a dictionary with theses two keys associated to their respective feature: the 2nd Conv2D layer and the 5th Conv2D layer.\n",
        "\n",
        "they should respectively corresponds to the 5th and 12th layer of vgg19  \n",
        "```py\n",
        "...\n",
        "5 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "...\n",
        "12 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "...\n",
        "```\n",
        "\n",
        "* Complete IMAGENET_MEAN and IMAGENET_STD. they both should be `3-tuple`\n",
        "* Question: Why is there 3 values ?\n",
        "* Question: How they have been obtained ?\n",
        "* Question: Why do we normalize our image ?\n",
        "* Complete the normalization in the `forward` method\n"
      ],
      "metadata": {
        "id": "pXTpqil1Z70j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = ? # TODO\n",
        "IMAGENET_STD  = ? # TODO\n",
        "\n",
        "class FeaturesModel(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_model, \n",
        "                 registered_layers=[], \n",
        "                 mean = IMAGENET_MEAN, \n",
        "                 std  = IMAGENET_STD,\n",
        "                 debug=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.registered_layers = registered_layers\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
        "        self.model = self.convert_model(input_model, registered_layers, debug=debug)\n",
        "\n",
        "    @staticmethod   \n",
        "    def convert_model(input_model, registered_layers, trim=True, debug=False):\n",
        "        if debug: \n",
        "            print('FeaturesModel')\n",
        "        model = nn.Sequential()\n",
        "\n",
        "        layers_count = {}\n",
        "        last_index = None\n",
        "        for i, layer in enumerate(input_model.children()):\n",
        "            # create the name\n",
        "            layer_type = FeaturesModel.get_layer_name(layer)\n",
        "            index = layers_count.setdefault(layer_type, 0)\n",
        "            layers_count[layer_type] += 1\n",
        "            name = f\"{layer_type}_{index}\"\n",
        "            if debug:\n",
        "                print('\\t', name)\n",
        "            \n",
        "            model.add_module(name, layer) # add the layer\n",
        "            if name in registered_layers:\n",
        "                last_index = i\n",
        "\n",
        "        # optimization : trim the model\n",
        "        if trim and last_index is not None:\n",
        "            model = model[:last_index+1]\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def get_layer_name(layer, raise_on_unknown=False) -> str:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            return 'conv'\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            return f'relu'\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            # TODO : return the correct string\n",
        "            return ... \n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            # TODO : return the correct string\n",
        "            return ...\n",
        "        msg = f'Unrecognized layer: {layer.__class__.name}'\n",
        "        if raise_on_unknown:\n",
        "            raise RuntimeError(msg)\n",
        "        print(msg)\n",
        "        return 'unkwown'\n",
        "\n",
        "    def forward(self, input, detach=False):\n",
        "        # out is the normalized input using self.mean and self.std \n",
        "        # TODO : Normalize\n",
        "        out = ... \n",
        "\n",
        "        features = {}\n",
        "        for name, layer in self.model.named_children():\n",
        "            out = layer(out)\n",
        "            if name in self.registered_layers:\n",
        "                feature = out.clone()\n",
        "                if detach:\n",
        "                    feature = feature.detach()\n",
        "                features[name] = feature\n",
        "        return features"
      ],
      "metadata": {
        "id": "gLQFl07jZ8OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciate a `FeaturesModel` class with the previous pretrained vgg19 model.\n",
        "Use the following registered layers:\n",
        "- conv_2\n",
        "- relu_4\n",
        "- pool_3\n",
        "\n",
        "If `debug=True` you should see every layer that can be used.\n",
        "\n",
        "* Compare `features` with the previous `feature` outputed by vgg19.\n",
        "\n",
        "* Which layer should you put in `registered_layers` in order to have the same `feature` as before ?"
      ],
      "metadata": {
        "id": "xDncFCQA977t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_model = FeaturesModel(\n",
        "    cnn, \n",
        "    registered_layers= ?, # TODO\n",
        "    debug=True # to print all the new layer names\n",
        ")\n",
        "\n",
        "# display features (dict)\n",
        "features = features_model(content_image)\n",
        "for layer, feature in features.items():\n",
        "    print(layer, feature.shape)"
      ],
      "metadata": {
        "id": "VtMYzY68lSrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing StyleTransferLoss class\n",
        "\n",
        "Now that we defined all we needed, we need to create a model that return the content and style loss in the forward method.\n",
        "\n",
        "The `StyleTransferLoss` Class will takes as attributes :\n",
        "* content_features : precomputed  content's features\n",
        "* style_features : precomputed  style's features\n",
        "* content_layers : layers where content's features will be extracted\n",
        "* style_layers : layers where style's features will be extracted\n",
        "\n",
        "Complete the forward pass to compute the Loss"
      ],
      "metadata": {
        "id": "vD0lflXCYtGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleTransferLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 content_features, \n",
        "                 style_features,\n",
        "                 content_layers,\n",
        "                 style_layers\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.content_features = content_features\n",
        "        self.style_features = style_features\n",
        "\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "\n",
        "    def forward(self, features):\n",
        "        cl = 0\n",
        "        for name in self.content_layers:\n",
        "            # retrieve the feature and content's feature of the layer named name\n",
        "            # hint: python dictionnnary access with a key\n",
        "            feature = ?[name]                 # TODO\n",
        "            feature_content = ?[name]         # TODO\n",
        "            # call the right function to compute the loss\n",
        "            cl += ?(feature, feature_content) # TODO\n",
        "            \n",
        "        sl = 0\n",
        "        for name in self.style_layers:\n",
        "            feature = ?[name]                 # TODO\n",
        "            feature_style = ?[name]           # TODO\n",
        "            sl += ?(features, feature_style)  # TODO\n",
        "\n",
        "        return cl, sl"
      ],
      "metadata": {
        "id": "Tie3aT2xMjgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training function\n",
        "\n",
        "* Define generate_from_input function that returns either a copy or random noise from an input image."
      ],
      "metadata": {
        "id": "0N4gfOYneQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_input(image, generated_image_init):\n",
        "    # TODO : if generated_image_init is copy : clone input image\n",
        "    if generated_image_init == 'copy':\n",
        "        generated_image =  # TODO\n",
        "    # TODO : if generated_image_init is white-noise : return white noise of same size as input image size\n",
        "\n",
        "    elif generated_image_init == 'white-noise':\n",
        "        generated_image = \n",
        "    else:\n",
        "        raise Exception(f'Unkwown generated_image_init = {generated_image_init}')\n",
        "\n",
        "    # TODO : Do we need grad on generated image ? if yes, set the grad in generated image\n",
        "    generated_image.requires_grad_(.....\n",
        "    return ..."
      ],
      "metadata": {
        "id": "_OHs1mhsiUHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* complete the loss computation (line 67)\n",
        "\n",
        "You might see in the code some SummaryWriter. You'll see why later. It basically logs stuff, here it's images and some useful values"
      ],
      "metadata": {
        "id": "4YC8uIhwjSGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(cnn, \n",
        "        content_image, \n",
        "        style_image,\n",
        "        num_steps=300,\n",
        "        content_layers=['conv_4'], \n",
        "        style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5'],\n",
        "        style_weight=1000000, \n",
        "        content_weight=1,\n",
        "        generated_image_init='copy'\n",
        "    ):\n",
        "    \"\"\"Run the style transfer.\"\"\"\n",
        "\n",
        "    # Input image\n",
        "    # TODO : fill in the blanks\n",
        "    generated_image = generate_from_input(image = ....,generated_image_init=...)\n",
        "\n",
        "    # Model\n",
        "    registered_layers = set(content_layers + style_layers)\n",
        "    model = FeaturesModel(cnn, registered_layers=registered_layers)\n",
        "    model.requires_grad_(False)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = optim.LBFGS([generated_image])\n",
        "\n",
        "    # precompute content_features and style_features\n",
        "    content_features = model(content_image, detach=True)\n",
        "    style_features = model(style_image, detach=True)\n",
        "                \n",
        "    # Loss \n",
        "    loss_fn = StyleTransferLoss(\n",
        "        content_features, \n",
        "        style_features,\n",
        "        content_layers,\n",
        "        style_layers\n",
        "    )\n",
        "\n",
        "    # logger / tensorboard\n",
        "    history = []\n",
        "    writer = SummaryWriter('runs/nst')\n",
        "    writer.add_image('image_content', content_image.squeeze(0))\n",
        "    writer.add_image('image_style', style_image.squeeze(0))\n",
        "\n",
        "    print('Optimizing...')\n",
        "    step = 0\n",
        "    while step < num_steps:\n",
        "\n",
        "        def single_step(): # CLOSURE\n",
        "            nonlocal step\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated_image.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            features = model(generated_image)\n",
        "            content_loss, style_loss = loss_fn(features)\n",
        "\n",
        "            # compute the final loss wrt. \n",
        "            #    style_weight\n",
        "            #    content_weight \n",
        "            #    content_loss\n",
        "            #    style_loss\n",
        "            loss = ? # TODO\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # log\n",
        "            if True or step % 50 == 0:\n",
        "                print(f\"run {step+1}/{num_steps} : loss={loss.item()}\"\n",
        "                        f\" content_loss={content_loss.item()} \"\n",
        "                        f\" style_loss={style_loss.item()} \")\n",
        "                \n",
        "                writer.add_image('image_generated', generated_image.squeeze(0), step)\n",
        "                writer.add_scalar('loss_content', content_loss, step)\n",
        "                writer.add_scalar('loss_style', style_loss, step)\n",
        "                writer.add_scalar('loss_global', loss, step)\n",
        "            \n",
        "            # prepare the next iter\n",
        "            history.append((content_loss, style_loss))\n",
        "            step += 1\n",
        "            return loss\n",
        "        \n",
        "        optimizer.step(single_step)\n",
        "    \n",
        "    return generated_image, history"
      ],
      "metadata": {
        "id": "Lxin1I6vMtQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "RAxLqBIjM33m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = image_loader( ? # TODO)\n",
        "style_image = image_loader( ? # TODO)\n",
        "\n",
        "generated_image, history = run(\n",
        "    ..., \n",
        "    ..., \n",
        "    ...,\n",
        "    num_steps=300,\n",
        "    content_layers=['conv_4'], \n",
        "    style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'],\n",
        "    style_weight=1000000, \n",
        "    content_weight=1,\n",
        "    generated_image_init='copy'\n",
        ")\n",
        "\n",
        "tensors_show(\n",
        "    [content_image, style_image, generated_image], \n",
        "    titles=['Content', 'Style', 'Generated']\n",
        ")"
      ],
      "metadata": {
        "id": "Jp868ex5Nag4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot the generated image using a function that we defined before"
      ],
      "metadata": {
        "id": "-LY5W-QJkfTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_nst = generated_image..."
      ],
      "metadata": {
        "id": "Sd28TWfPuCir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will display the history and content loss\n",
        "\n",
        "* Analyze what you see"
      ],
      "metadata": {
        "id": "CNm2jfn4fY2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_content = [cl for (cl, sl) in history]\n",
        "history_style   = [sl for (cl, sl) in history]\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
        "# plot loss\n",
        "ax0.plot(....., '.--', label='content loss')\n",
        "ax0.set_xlabel('Step')\n",
        "ax0.set_ylabel('loss')\n",
        "ax0.legend(loc='lower right')\n",
        "ax0.grid(True)\n",
        "\n",
        "# plot accuracy\n",
        "ax1.plot(....., '.--', label='style loss')\n",
        "ax1.set_xlabel('Step')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True)"
      ],
      "metadata": {
        "id": "64_7E_m0NaXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for your personnal understanding, try to change the style weight and content weight value. \n",
        "* What do you observe ?"
      ],
      "metadata": {
        "id": "EJ_qr-60OlYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0f4vFSyh7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard"
      ],
      "metadata": {
        "id": "oNBtK7OL4f-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "BzsG6wI01bGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/nst"
      ],
      "metadata": {
        "id": "AT379NBF4Oh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg4EbrhyfyWI"
      },
      "source": [
        "# III - Conclusion\n",
        "\n",
        "In fact, the goal of this lab is to make you use DL Network and Code.\n",
        "\n",
        "Normally, you might have an intuition of what happens in the model during this style transfer process. In fact, the more you go through the layers of your model, the higher the features will be. What does it mean ? It means that the more you step into your model, the more you'll see object part (eyes, dog booty, cup of tea...). That also means that the model understands the input image by 'seeing' the objects. These features are used for classification. This also means that the first layers will be low level features (edge...), aka feature extraction.\n",
        "\n",
        "By accessing the intermediate layers, we are able to have a compromise between extracted features and image understanding, allowing us to describe the content and the style of the input images.\n",
        "\n",
        "Well Done !\n",
        "\n",
        "<img src=\"https://media0.giphy.com/media/l0ErFafpUCQTQFMSk/giphy.gif\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copie de Style_Transfer_using_Python_and_Pytorch_TODO.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}