{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/2A/Majeure%20Signal/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUZCOMw1Gem5"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "Time : 8h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ah0rPJ6n08"
      },
      "source": [
        "# Preparatory Questions \n",
        "\n",
        "## General Questions\n",
        "- First watch this video : https://www.youtube.com/watch?v=Rdpbnd0pCiI\n",
        "- What is the difference between supervised and unsupervised learning ?\n",
        "- What does it mean to train a model for a certain task ?\n",
        "- What is a Loss function ?\n",
        "\n",
        "## AutoEncoder questions\n",
        "- Is an AutoEncoder a Supervised or Unsupervised algorithm in the case of Image Compression\n",
        "- What loss function should be used in an AutoEncoder ?\n",
        "- Go on the following link : https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html. What's the purpose of this website ? \n",
        "- Play a lil' bit with that website. Just above the \"Network Visualization\" Title, you have a drawing of digits (0,1,2.....9). Explain what that visualisation is ? \n",
        "- Do you see clusters ? Is it good or no ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc8OA-MQ6n09"
      },
      "source": [
        "# Disclaimer\n",
        "\n",
        "We will refer to :\n",
        "- AutoEncoder as AE\n",
        "- Multi Layer Perceptron as MLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fup2OihC7Kkv"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "So basically during the part 1 of this lab, we have seen JPEG Compression. JPEG compression is a general algorithm that can compress any image. Let's first see your understanding of the JPEG algorithm :\n",
        "* What are the component in the Encoding part of the algorithm ?\n",
        "* What are the component in the Decoding part of the algorithm ?\n",
        "* Is the down-sampling phase of JPEG linear ? (i.e : in a y = ax+b form)\n",
        "* Is it a lossless compression algorithm ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmPz4EM86n09"
      },
      "source": [
        "# AI for the Win\n",
        "\n",
        "Now let's do some Deep Learning for Image compression. This might be your first time using Deep Learning, so don't be afraid we will guide you.\n",
        "\n",
        "Goal of this lab :\n",
        "* Get to know Deep Learning\n",
        "* Learn how to compress images using AutoEncoders\n",
        "* Understand the differences between JPEG compression and Deep Learning for compression\n",
        "\n",
        "Alright, let's get started.\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/16/b2/96/16b296afb78ec57d12c931bc72b42eec.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "315B4XDj9ln-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1iy5iLS6n0_"
      },
      "source": [
        "# What is Deep Learning ?\n",
        "\n",
        "Deep Learning is a branch of AI where you **teach a Model** a certain **task** using a **Dataset**. The Model is composed of **Layers of Neurons** that are updated using a **Loss**. The Model infers a prediction from an **input**. In fact, a Deep Neural Network can be seen as a complex function ${f}$ that maps the input data to a learned space from the Dataset. Note the bold words. These are the important things you need to understand about Deep Learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awLz7ozJ6n1A"
      },
      "source": [
        "# Generalities on AutoEncoder\n",
        "\n",
        "An autoencoder is a model able to reconstruct an input from a Latent Space.\n",
        "An autoencoder (AE) is composed of 2 models :\n",
        "* an encoder model that encodes input data into a latent space\n",
        "* a decoder model that recreates the input image from the latent space\n",
        "\n",
        "\n",
        "\n",
        "Think of it as the Chinese Whispers (Téléphone Arabe en francais) game :\n",
        "- First player says a sentence\n",
        "- Second player repeats the sentence of First player to Third player\n",
        "- Third player repeats the sentence of Second player to Fourth player\n",
        "- ...\n",
        "- The final player repeats the sentence of Before Last Player, which by recurrence should be what First Player said.\n",
        "\n",
        "This basically is an AutoEncoder :\n",
        "- Player 1 sends a Voice signal that is compressed by all the Players\n",
        "- Last Player must reconstruct perfectly the Voice Signal of Player 1 from the compressed signal of all the previous players.\n",
        "\n",
        "\n",
        "<img src = \"https://journals.openedition.org/marges/docannexe/image/364/img-2.jpg\" width=\"400\" height=\"500\">\n",
        "\n",
        "You will code two types of AE models :\n",
        "* MLP style\n",
        "* Conv style\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DLNHchj6n1B"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqws4UbZ6n1B"
      },
      "source": [
        "## Dataset\n",
        "When training  a DL model, we use a Dataset. The model uses the Dataset to learn something for a task. We usually divide the Dataset into Train, Valid, Test Dataset.\n",
        "In this lab, we will use MNIST Dataset. First, let's see the Dataset. We do not have to code the Dataset as Folks from Torchvision wrote it for us.\n",
        "\n",
        "- Load the Dataset from TorchVision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOdefqiw6n1B"
      },
      "outputs": [],
      "source": [
        "# TODO: Load MNIST Train Dataset from TorchVision\n",
        "\n",
        "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# TODO: What's the size of the Dataset ?\n",
        "# TODO: Retrieve one element of the Dataset ? What is the shape of one piece of Data ? \n",
        "\n",
        "size_of_dataset =  len(dataset)\n",
        "data = dataset[0]\n",
        "print(data.shape)\n",
        "\n",
        "# TODO: Plot the retrieved Data\n",
        "\n",
        "plt.imshow(data.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsqbBFiz6n1C"
      },
      "source": [
        "As you can see, there's a train attribute to the MNIST Class. When it's set to True, you're loading the train Dataset. Hence, change it to false to load the test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVnRxxaa6n1C"
      },
      "outputs": [],
      "source": [
        "mnist_test =  MNIST('', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSbPGhqP6n1D"
      },
      "source": [
        "We will create a Validation Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4kz-IGe6n1D"
      },
      "outputs": [],
      "source": [
        "mnist_train, _ = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiGz4sBr6n1D"
      },
      "outputs": [],
      "source": [
        "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test =  MNIST('', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_train, _ = \n",
        "\n",
        "train_loader = \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blh2vwZu6n1D"
      },
      "source": [
        "## Dataloader\n",
        "\n",
        "So the Dataset returns one element at a time. In DL, we like sending many items at the same time to the model. We form BATCH of Data and send it using a DataLoader. Dataloader are an iterable over the dataset. It means that the Dataloader will form BATCH of Data for you and fetch them when training.\n",
        "- Create a DataLoader for your Training, Valid and Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EM2xVJQ6n1D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utu6siUB6n1D"
      },
      "source": [
        "# Creating the Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUHGbOi6n1E"
      },
      "source": [
        "## MLP Style\n",
        "\n",
        "<img src='https://www.researchgate.net/publication/344394387/figure/fig1/AS:974657746399232@1609387923440/Figure-Computational-Schematics-of-the-MLP-and-the-autoencoder.png'>\n",
        "\n",
        "The first type of AutoEncoder, you will code a MLP style AE. In this part, you will :\n",
        "- Create Layers that inherits from nn.Module\n",
        "\n",
        "The model is composed of an input layer, a bottleneck, and an output layer.\n",
        "- Where do you think the compression occurs ?\n",
        "\n",
        "The bottleneck forces a compressed representation of the original input. \n",
        "\n",
        "You will create few nets :\n",
        "- MLPDown that compresses the input Image\n",
        "- Encoder that stacks multiples MLPDown to create the Latent Space\n",
        "- MLPUp that uses the Latent Space to decode and recreate the Input Image\n",
        "- Decoder that stack multiple MLPUp to recreate the Compressed Input Image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3pt1m726n1E"
      },
      "source": [
        "Quick Tips : A PyTorch module is usually composed of two methods : \n",
        "\n",
        "- Init to initialize the class\n",
        "- forward to forward the data through your model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SubModules"
      ],
      "metadata": {
        "id": "lcJOTYUX6xPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Down"
      ],
      "metadata": {
        "id": "sk3VchXmKCDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krFXAqCh6n1E"
      },
      "outputs": [],
      "source": [
        "class MLPDown(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.model = nn.Sequential(nn.Linear(self.input_size, self.output_size),\n",
        "                                    nn.ReLU())\n",
        "                                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Up"
      ],
      "metadata": {
        "id": "aaLBbyPiKE9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxuAZqyc6n1E"
      },
      "outputs": [],
      "source": [
        "class MLPUp(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.model = nn.Sequential(nn.Linear(self.input_size, self.output_size),\n",
        "                                    nn.ReLU())\n",
        "                                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modules"
      ],
      "metadata": {
        "id": "cVYR3dFB62IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "Hw3Hlcw_J-ol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeMpTyPs6n1F"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size, latent_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.model = nn.Sequential( MLPDown(self.input_size, self.latent_size))\n",
        "                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "5vbn7RoIKAeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zKSWTHf6n1F"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_size,input_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.model = nn.Sequential( MLPUp( self.latent_size,self.input_size))\n",
        "                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model : The MLP AutoEncoder\n",
        "\n",
        "Now that we have the Encoder and Decoder, we just have to stack them in order to form the AutoEncoder."
      ],
      "metadata": {
        "id": "2VdNvXbB636Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdlCRucN6n1F"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,latent_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.model = nn.Sequential(Encoder(self.input_size,self.latent_size),\n",
        "                                   Decoder(self.latent_size, self.input_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N8KQNJy6n1F"
      },
      "source": [
        "# Training\n",
        "\n",
        "At this moment of the lab, you have :\n",
        "- a Model\n",
        "- a Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_4-zwYK6n1F"
      },
      "source": [
        "\n",
        "\n",
        "Now that we created our AE, we need to train it on the MNIST Dataset. Let's form the training Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Loss \n",
        "\n",
        "We need a Loss Function. Let's reason. We are recreating an Image that is compressed. It means that the recreated image must be a similar as the original image. \n",
        "- How can you calculate the similarity between two vectors ?\n",
        "- What type of loss do you know that calculates the distance between two inputs ?\n"
      ],
      "metadata": {
        "id": "5MqMXbQw_1Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Am2fuauUBoUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An Optimizer\n",
        "\n",
        "<img src=\"https://i.imgflip.com/640sfs.jpg\" height= 400>\n",
        "\n",
        "We need something to update the weights of the model. In fact, we need to perform Gradient Descent to recalculate the weights of each layers regarding the model's predictions."
      ],
      "metadata": {
        "id": "vdJa2rgV___i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7lykHsQ-HrMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Hyper Parameters\n",
        "\n",
        "Now we have to define some hyperparameters for the training."
      ],
      "metadata": {
        "id": "ssV_ZjB6Hrpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Now that we have everything that is needed for training, we have to create the training loop. The loop consists of :\n",
        "* Sending Data through the model to obtain Predictions\n",
        "* Computing the Loss \n",
        "* Backwarding the Loss using Gradients \n",
        "* Logging the losses and accuracies (if exists)"
      ],
      "metadata": {
        "id": "9sVerRhLIdpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in num_epoch : \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):        \n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i == ??:\n",
        "          running_loss= running_loss/??"
      ],
      "metadata": {
        "id": "v_9nrxJj7Bj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jak6Xd8h6n1F"
      },
      "source": [
        "Ok now to see the effect of the compression, change the latent_size to different values. For example try : 512, 128, 16, 1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layer Style\n",
        "\n",
        "<img src=\"https://onlinelibrary.wiley.com/cms/asset/452a6491-bc35-488b-ac9f-27906d62d589/cpe6282-fig-0001-m.jpg\">\n",
        "\n",
        "In the previous episode of AutoEncoder, we have seen Fully Connected AE. Let's see another one : Convolutional AE.\n",
        "\n",
        "Convolutional Layers are filters that \"scans\" the input image in order to extract features. Take 5 minutes and play with the following link :   \n",
        "\n",
        "* https://ezyang.github.io/convolution-visualizer/\n",
        "\n",
        "Questions :    \n",
        "* What is the stride parameter ?\n",
        "* What is the padding parameter ?\n",
        "* What does it change on the output to increase the Kernel Size ?"
      ],
      "metadata": {
        "id": "i1kpRMLuU9_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Definition\n",
        "\n",
        "Receptive Field : The receptive field are the pixels seen by the kernel layer"
      ],
      "metadata": {
        "id": "CC7RbnN1dKiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n",
        "We are going to create the model in the same manner as the MLP model. Instead of having Linear layers, we are going to have Conv2d layers. However there some things that we must be aware of."
      ],
      "metadata": {
        "id": "6v_cYtuufVur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SubModules"
      ],
      "metadata": {
        "id": "FiDIW6xzfURZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conv Down\n",
        "\n",
        "ConvDown is used to compress the input image. It applies a convolution between the input image and the kernel. In fact, it is used to extract features. Our ConvDown Model will be composed of two layers :     \n",
        "* Conv2d layer\n",
        "* Non Linearity (ReLU)\n",
        "\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2017/07/no_padding_no_strides.gif\">"
      ],
      "metadata": {
        "id": "6aR_6-5AfURZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30AE171_fURZ"
      },
      "outputs": [],
      "source": [
        "class ConvDown(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, kernel_size = 3):\n",
        "        super().__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.model = nn.Sequential(nn.Conv2d(self.input_channel, self. output_channel, kernel_size =self.kernel_size ),\n",
        "                                    nn.ReLU())\n",
        "                                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conv Up\n",
        "\n",
        "ConvUp is used to decompress the input image. In fact, it uses extracted features to propose a reconstructed output feature map.\n",
        "\n",
        "* From what you've seen on the website, is it possible to increase output size map using Conv2d layers ?\n",
        "\n",
        "We introduce ConvTranpose2D layers, that applies Transpose Convolution over an input image. It also means that these layers upsamples the input image. In fact the ConvTranspose layers lear to upsample the images.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*HnxnJDq-IgsSS0q3Lut4xA.gif\" height=300>"
      ],
      "metadata": {
        "id": "RTj9S6BdfURZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iup_9vP8fURZ"
      },
      "outputs": [],
      "source": [
        "class ConvUp(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, kernel_size = 3):\n",
        "        super().__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.model = nn.Sequential(nn.ConvTranspose2d(self.input_channel, self. output_channel, kernel_size =self.kernel_size ),\n",
        "                                    nn.ReLU())\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modules"
      ],
      "metadata": {
        "id": "xKSienu_fURZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "add5ul-BfURZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJg8gNNPfURZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,input_channel, output_channel, kernel_size = 3):\n",
        "        super().__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.model = nn.Sequential( ConvDown(self.input_size, self.latent_size,self.kernel_size))\n",
        "                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "-Ty_eE8ufURa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOIjpP5JfURa"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,input_channel, output_channel, kernel_size = 3):\n",
        "        super().__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.model = nn.Sequential( ConvUp(self.input_size, self.latent_size,self.kernel_size))\n",
        "                    \n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model : The Conv AutoEncoder\n",
        "\n",
        "Now that we have the Encoder and Decoder, we just have to stack them in order to form the AutoEncoder. The stacking is different here as we refer to the input and output channels of each layers.\n",
        "\n",
        "We advise you to take "
      ],
      "metadata": {
        "id": "IuUrSK7QfURa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGslcjwafURa"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,latent_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.model = nn.Sequential(Encoder(self.input_size,self.latent_size),\n",
        "                                   Decoder(self.latent_size, self.input_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxIt_IxgfURa"
      },
      "source": [
        "# Training\n",
        "\n",
        "At this moment of the lab, you have :\n",
        "- a Model\n",
        "- a Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFdBGGd3fURa"
      },
      "source": [
        "\n",
        "\n",
        "Now that we created our AE, we need to train it on the MNIST Dataset. Let's form the training Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Loss \n",
        "\n",
        "We need a Loss Function. Let's reason. We are recreating an Image that is compressed. It means that the recreated image must be a similar as the original image. \n",
        "- How can you calculate the similarity between two vectors ?\n",
        "- What type of loss do you know that calculates the distance between two inputs ?\n"
      ],
      "metadata": {
        "id": "KiBLqkS6fURa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "sbc21j0pezi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hnZIgga3VIsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DuDAwAM2cXUY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ankQnkaU6n1G"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The encoder encodes the input image(s). Indeed, it takes the image and extracts features thanks to its different layers. The deeper the model, the complexer the extracted features will be. At the end of the Decoded Network you will have a latent representation of the image.\n",
        "* However is it necessary to have very Deep Networks ? \n",
        "* Create a **Fully Connected** Encoder Model. \n",
        "\n",
        "Your model must be instanciated using two parameters : \n",
        "* Input shape\n",
        "* Latent size\n",
        "\n",
        "What are those parameters ?\n",
        "\n",
        "Feel free to design your own model, by using whatever Activation function and Layer size you want (but can you ?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd5KFdqN_m4M"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "You will use the MNIST dataset in order to train and test you model. You won't have to code the Dataset as Pytorch has already coded it for you. Please refer to the introduction to Lightning if needed.\n",
        "\n",
        "Load the MNIST dataset, and setup what is needed for the training. Refer to the 1rst lab if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX7tuNthA1zB"
      },
      "outputs": [],
      "source": [
        "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test =  MNIST('', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_train, _ = \n",
        "\n",
        "train_loader = \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K32VD7Jm7oo4"
      },
      "source": [
        "# Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv7uFzMz7TEG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX29RrTh8Wx2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igiyUUPS888C"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "The encoder encodes the input image(s). Indeed, it takes the image and extracts features thanks to its different layers. The deeper the model, the complexer the extracted features will be. At the end of the Decoded Network you will have a latent representation of the image.\n",
        "* However is it necessary to have very Deep Networks ? \n",
        "* Create a **Fully Connected** Encoder Model. \n",
        "\n",
        "Your model must be instanciated using two parameters : \n",
        "* Input shape\n",
        "* Latent size\n",
        "\n",
        "What are those parameters ?\n",
        "\n",
        "Feel free to design your own model, by using whatever Activation function and Layer size you want (but can you ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4copY8_Q87fK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_shape,latent_size):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjkOPYoY91E1"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "\n",
        "The Decoder takes as input the latent representation of the input vector and recreates the input image.\n",
        "* Create a Decoder model.\n",
        "\n",
        "Your model must be instanciated using two parameters : \n",
        "* Output shape\n",
        "* Latent size\n",
        "\n",
        "What are those parameters ? Are they different from the Encoder ?\n",
        "\n",
        "\n",
        "Feel free to design your own model, by using whatever Activation function and Layer size you want (but can you ?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-XPV5Kc8rLC"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_size, output_shape):\n",
        "        super().__init__()\n",
        "        \n",
        "  def forward(self,x):\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQzk7G4g_LXi"
      },
      "source": [
        "# Autoencoder\n",
        "\n",
        "\n",
        "Let's create the AutoEncoder. Using Lightning Framework,  :     \n",
        "* Stack the encoder and the decoder in order to create the AE. \n",
        "\n",
        "Now let's think a little bit before beginning :\n",
        "* What task is it ? (Classification ? Regression ?)\n",
        "* What optimizer will you be using ?\n",
        "* What learning rate ?\n",
        "* Will your model learn ? (overfit_batches will help)\n",
        "* What is the impact of the latent_space size ?\n",
        "* What loss shold we be using ?\n",
        "\n",
        "Don't forget to log the needed values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6fr5hrs_Ksz"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(pl.LightningModule):\n",
        "    def __init__(self, latent_size, input_shape, output_shape):\n",
        "        super(AutoEncoder,self).__init__()\n",
        "\n",
        "        self.encoder = \n",
        "        self.decoder = \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self,x):\n",
        "        reconstructed_image = \n",
        "        return reconstructed_image\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x,y = train_batch\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x,y = val_batch\n",
        "        self.log('val_loss', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfMirANECMZm"
      },
      "source": [
        "Set up Tensorboard for observation. The given directory is the default directory and might not the created at the moment you run the code. In fact, the trainer will create the folder where the checkpoints are located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PXeMOVsCR0z"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/tb_logs/my_model/version_0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8BmZ3vXBApf"
      },
      "source": [
        "Now let's create your Trainer. \n",
        "* Load your model\n",
        "* Create your lightning Trainer\n",
        "* Fit the data to your model.\n",
        "\n",
        "Once your trainer set up, train for 10 epochs and watch your training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKhaiie2BILx"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder(_, _)\n",
        "trainer = pl.Trainer(gpus=-1,max_epochs=_)\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtseLWkCUFQ"
      },
      "source": [
        "# Test\n",
        "\n",
        "\n",
        "Now that you have trained your model, let's test it. We provide you a function that will help you to load your model and use it. \n",
        "You might need to do something on your testing data before sending it to the model. What is it ?\n",
        "\n",
        "Try few images reconstructions.\n",
        "* Where should you pick your testing data from ? \n",
        "* Can we test on training set ? Why ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtWdzhJaa3Ch"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "def load_model_from_checkpoint(model, checkpoint_path):\n",
        "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
        "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
        "    model = model.load_from_checkpoint(ckp)\n",
        "    model.freeze()\n",
        "    return model\n",
        "\n",
        "def forward_and_print_image(model,image):\n",
        "  image = model(test_image.view(1,-1))\n",
        "  image_reco = image.view(28,28)\n",
        "  plt.imshow(image_reco)\n",
        "\n",
        "\n",
        "model = load_model_from_checkpoint(model,_)\n",
        "test_image = mnist_val.__getitem__(1)[0].unsqueeze(0)\n",
        "forward_and_print_image(model,test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqxGu3xXjVNo"
      },
      "source": [
        "# Convolutional Type\n",
        "\n",
        "In the previous episode of AutoEncoder, we have seen Fully Connected AE. Let's see another one : Convolutional AE.\n",
        "\n",
        "We will make you code in a different way, using Module Lists. But at the end the results will be the same : we are going to make the same reconstruction using Convolution Layers.\n",
        "\n",
        "By looking at the following links, **explain each parameters of the nn.Conv2d and nn.ConvTranspose2d methods** :\n",
        "* https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "* https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
        "* https://ezyang.github.io/convolution-visualizer/\n",
        "\n",
        "\n",
        "We will ask you few things :\n",
        "* Calculate the size of the images after each pass through a layer of your model. Write the formula, you're using for ths calculation.\n",
        "* Are you going to use MaxPooling ? If yes, explain with a photo what Maxpooling does. \n",
        "* Can we revert MaxPooling effect without information loss ?\n",
        "\n",
        "You will still use the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMyrG9K10Sui"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xshb4Xc7z0Eq"
      },
      "source": [
        "# Encoder Conv\n",
        "\n",
        "First let's write one Bottleneck of your Convolutional Encoder.\n",
        "\n",
        "Your Model should be instanciated using :    \n",
        "* in_channels : number of input channels\n",
        "* out_channels : number of output channels\n",
        "\n",
        "Why ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vA27jptz-NM"
      },
      "outputs": [],
      "source": [
        "class ConvDown(nn.Module):\n",
        "      def __init__(self, in_channels, out_channels):\n",
        "            super(ConvDown,self).__init__()\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(...\n",
        "\n",
        "      def forward(self,x):\n",
        "\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP2OhrY8z_eM"
      },
      "source": [
        "# Decoder Conv\n",
        "\n",
        "blablabla.\n",
        "Write your Decoding Bottleneck.\n",
        "Be careful if you used MaxPooling for the shape of your inputs. Don't hesitate to print each output shape. \n",
        "\n",
        "\n",
        "Your Model should be instanciated using :    \n",
        "* in_channels : number of input channels\n",
        "* out_channels : number of output channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhDLCCeZjXod"
      },
      "outputs": [],
      "source": [
        "class ConvUp(nn.Module):\n",
        "      def __init__(self, in_channels, out_channels):\n",
        "            super(ConvUp,self).__init__()\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.ConvTranspose2d(....\n",
        "            )\n",
        "\n",
        "      def forward(self,x):\n",
        "          return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DnOnYU70Xrd"
      },
      "source": [
        "# Create your Conv AE.\n",
        "\n",
        "\n",
        "Let's make things a Lil' bit spicy. We are going to stack the Bottlenecks in order to create our Encoder and Decoder. Then, we are going to stack the encoder and the Decoder in order to create the AutoEncoder. Easy ?\n",
        "\n",
        "Your model must be instanciated using :\n",
        "* in channels \n",
        "* out_channels\n",
        "* feature channel shape : list of feature channel shape you want.\n",
        "\n",
        "\n",
        "The given feature_shape can be indeed changed.\n",
        "Using nn.ModuleList() create your encoder and your decoder. \n",
        "Have a look at : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
        "\n",
        "\n",
        "We have written the forward for you. Take some inspiration from it to define your training step. Don't forget to log some values.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxH33SFx0V8B"
      },
      "outputs": [],
      "source": [
        "class AutoEncoderConv(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels,feature_shape = [3,16,32]):\n",
        "        super(AutoEncoderConv,self).__init__()\n",
        "        self.save_hyperparameters() #mandatory\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.feature_shape = feature_shape\n",
        "        self.reverse_list = list(reversed([self.out_channels]+self.feature_shape))\n",
        "        for feature in self.feature_shape:\n",
        "            self.encoder.append(_) #UP or DOWN ?\n",
        "            in_channels = feature\n",
        "        for i,_ in enumerate(self.reverse_list):\n",
        "            if i < len(self.reverse_list) -1:\n",
        "                self.decoder.append(_) #UP or DOWN ?\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        for i,down in enumerate(self.encoder):\n",
        "          x = down(x)\n",
        "        x_reconstructed=x\n",
        "        for i,up in enumerate(self.decoder):\n",
        "          x_reconstructed = up(x_reconstructed)\n",
        "        return x_reconstructed\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x,y = train_batch\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x,y = val_batch\n",
        "        self.log('val_loss', loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQWWLagA1uAe"
      },
      "source": [
        "# Load your Logger\n",
        "\n",
        "When training a model you want to get a log of every useful values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piQxjvIK1tdJ"
      },
      "outputs": [],
      "source": [
        "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JotPUWo2OrS"
      },
      "source": [
        "# Train\n",
        "\n",
        "Fit that model. Don't forget to load tensorboard before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZHr1ujc2Wcv"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoderConv(_, _)\n",
        "trainer = pl.Trainer(gpus=-1,logger = logger,max_epochs = _)\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlmqjXHveXna"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/tb_logs/my_model/version_0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azB_n4wT2ZNv"
      },
      "source": [
        "# Test\n",
        "\n",
        "Now by loading the last checkpoint of your trained model, show some reconstructed images from the Test set.\n",
        "\n",
        "* How is the reconstruction ?\n",
        "\n",
        "We provide two functions to help you in this mission :\n",
        "* load_model_from_chekcpoint that return the model with the weights loaded\n",
        "* forward_and_print_image that forwards the image through the model and prints the output\n",
        "\n",
        "Don't forget these squeezes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkOVtUgQDEys"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pickle\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_model_from_chekcpoint(model, checkpoint_path):\n",
        "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
        "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
        "    checkpoint = torch.load(ckp)  \n",
        "    model =  model.load_from_checkpoint(ckp,hparams_file = checkpoint_path+'/hparams.yaml')\n",
        "    model.freeze()\n",
        "    return model\n",
        "\n",
        "def forward_and_print_image(model,image):\n",
        "  image = model(test_image)\n",
        "  image_reco = image.view(28,28)\n",
        "  plt.imshow(image_reco)\n",
        "\n",
        "\n",
        "model = load_model_from_chekcpoint(_,_)\n",
        "test_image = \n",
        "plt.imshow()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yopEX26n3GxG"
      },
      "outputs": [],
      "source": [
        "forward_and_print_image(_,_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtlZ-Rba2OFX"
      },
      "source": [
        "AutoEncoders got few advantages :\n",
        "* It compresses images \n",
        "* It is used for denoising Images\n",
        "* It can be used for Anomaly Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11HbQ67X7_bZ"
      },
      "source": [
        "# Adding Classification\n",
        "\n",
        "Let's get back to FC AE.\n",
        "Let's classify something. Now we are going to classify the reconstructed image.\n",
        "* What task are we adding to the model ? How can we add it ?\n",
        "* Where should we add the corresponding layer ? Draw a Schema of your model. What's the name of these type of models ?\n",
        "* What loss should we use ?\n",
        "* Can we calculate an accuracy ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pzfGw0q8F2y"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder_for_Accuracy(pl.LightningModule):\n",
        "    def __init__(self, latent_size, input_shape,num_class):\n",
        "        super(AutoEncoder_for_Accuracy,self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.input_shape = input_shape\n",
        "        self.num_class = \n",
        "        self.encoder = \n",
        "        self.decoder = \n",
        "        self.fc = \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        return reconstructed_image,label_hat\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x,y = train_batch\n",
        "       \n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', accuracy)\n",
        "\n",
        "        return loss+loss_cls\n",
        "\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x,y = val_batch\n",
        "        \n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        x,y = test_batch\n",
        "        \n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCCzZBCDzw9h"
      },
      "source": [
        "Train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5ByagozvZz"
      },
      "outputs": [],
      "source": [
        "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
        "model = AutoEncoder_for_Accuracy(_, _,_)\n",
        "trainer = pl.Trainer(gpus=-1,logger = logger,max_epochs=10)\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LFbLK4DOJz7"
      },
      "source": [
        "# Load and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv94XVoIOM1f"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pickle\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_model_from_chekcpoint(model, checkpoint_path):\n",
        "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
        "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
        "    checkpoint = torch.load(ckp)  \n",
        "    model =  model.load_from_checkpoint(ckp,hparams_file = checkpoint_path+'/hparams.yaml')\n",
        "    model.freeze()\n",
        "    return model\n",
        "\n",
        "def forward_and_print_image(model,image):\n",
        "    image,label = model(test_image)\n",
        "    label = torch.argmax(label, dim =1)\n",
        "    print('label is: ', label.detach().cpu().numpy()[0])\n",
        "    image_reco = image.view(28,28)\n",
        "    plt.imshow(image_reco)\n",
        "\n",
        "\n",
        "model = load_model_from_chekcpoint(model,\"/content/tb_logs/my_model/version_1/\")\n",
        "test_image = mnist_test.__getitem__(1)[0].unsqueeze(0)\n",
        "plt.imshow(test_image.squeeze(0).squeeze(0))\n",
        "forward_and_print_image(model,test_image.view(28,28)) ####reconstruit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlLKlbq0GAGp"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/tb_logs/my_model/version_1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2uu6YHx3rmO"
      },
      "source": [
        "# Denoising Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVGa7IBH32Hg"
      },
      "source": [
        "\n",
        "AutoEncoder has that good ability to denoise your input image. Let's try it !\n",
        "* Add various noises to your input images and test what your AutoEncoder outputs.\n",
        "* Where should you pick your test images from ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Upx-rrEC31db"
      },
      "outputs": [],
      "source": [
        "def add_noise(inputs):\n",
        "     noise = _\n",
        "     return inputs + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DakEBPhfeEtH"
      },
      "outputs": [],
      "source": [
        "model = load_model_from_chekcpoint(_,_)\n",
        "test_image = \n",
        "test_image_with_noise = \n",
        "plt.imshow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnsaa7Pg6hx4"
      },
      "outputs": [],
      "source": [
        "forward_and_print_image(_,_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwsvWzvg2kTe"
      },
      "source": [
        "Now let's use the AutoEncoder for harder tasks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AutoEncoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}