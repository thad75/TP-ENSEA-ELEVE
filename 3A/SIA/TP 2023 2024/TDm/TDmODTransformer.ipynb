{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thad75/TP-ENSEA-ELEVE/blob/main/3A/SIA/TP%25202023%25202024/TDmODTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFhsEWCEzp3b"
   },
   "source": [
    "# I - Object Detection and Segmentation\n",
    "\n",
    "Let's apply what we have seen in class for the moment. Let's train a segmentation model : Mask R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
    "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n",
    "\n",
    "As we only have 2 hours, let's finetune the model instead of training it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdb8F8uc0W0W"
   },
   "outputs": [],
   "source": [
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
    "!unzip PennFudanPed.zip\n",
    "\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2ndvWz8zjMT"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3giAJixe0cm8"
   },
   "source": [
    "## a - Dataset PennFunnPed\n",
    "\n",
    "So we have a dataset, and we provide a dataset class. Let's observe, few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJgKw3ch0eTR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0rGU4p9cLia"
   },
   "source": [
    "TODO : Observe a sample of the dataset. What is inside ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-55ZnmT08EU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K15kYmq_cSKc"
   },
   "source": [
    "## TODO : Plot a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUesu5o-09MS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Provided sample item\n",
    "image_tensor, target = ...\n",
    "\n",
    "# Convert tensor to numpy array and transpose the dimensions for visualization\n",
    "image_np = ...\n",
    "\n",
    "# Create a figure and plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')\n",
    "\n",
    "# Get bounding boxes and masks from the target dictionary\n",
    "boxes = ...\n",
    "masks = ...\n",
    "labels = ...\n",
    "\n",
    "# Plot bounding boxes on the image\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], linewidth=2, label=f'Label: {labels[i]}')\n",
    "\n",
    "# Plot masks on the image\n",
    "# Plot masks on the image\n",
    "for i in range(masks.shape[0]):\n",
    "    mask = masks[i]\n",
    "    plt.imshow(mask, alpha=0.3, cmap='viridis', interpolation='none')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN34jGul3-o8"
   },
   "source": [
    "## Model\n",
    "\n",
    "We will start with the pretrained on COCO version for a head start. Obviously we won't code everything. But we will modify the bakcbone of the model. We will modify a well chosen model to create our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ_rYlyS4Pcb"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    # TODO : Have a look at the following page and pick the right model : https://pytorch.org/vision/stable/models.html\n",
    "    model = ...\n",
    "\n",
    "    # Modify the model's box predictor for classification\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Modify the model's mask predictor for segmentation\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mat6law2gdra"
   },
   "source": [
    "## Let's go training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa4jJZPAd_C6"
   },
   "source": [
    "Let's put everything together and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffJjJL1UeDm_"
   },
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzIi8CpqeFev"
   },
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_indices = indices[:-50]\n",
    "test_indices = indices[-50:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "# TODO : Define your dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ...,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    ...,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgSqjGJfeIoY"
   },
   "source": [
    "Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqW6jmAzeKaC"
   },
   "outputs": [],
   "source": [
    "num_classes = ... # What is the number of classes\n",
    "model = ... # TODO : Create your Model\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params,\n",
    "                            lr=0.005,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnSAbyfteL4A"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztEId6G5eNRI"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate  # Assuming functions for training and evaluation are in a separate file\n",
    "\n",
    "num_epochs = # TODO : Define a num epoch\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, test_loader, device=device)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRPjvY2d7AP9"
   },
   "source": [
    "## Testing the model\n",
    "\n",
    "Let's test the model on a random image taken from the internet. Do not hesitate to pick another image for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU9SmhTZ68pQ"
   },
   "outputs": [],
   "source": [
    "!wget https://rspcb.safety.fhwa.dot.gov/RSF/images/unit2_crosswalk.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsoTv-e36rZc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "image = read_image(\"/content/unit2_crosswalk.png\")\n",
    "eval_transform = ... # TOOD : Do we add something ?\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = ... # TODO : Get the predictions\n",
    "\n",
    "\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "masks = (pred[\"masks\"] > ...).squeeze(1) # TODO : Select a good value for mask visualization\n",
    "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWKnjfhYW94s"
   },
   "source": [
    "# II - Attention Visualization\n",
    "\n",
    "Go on this website and try to understand what each tokens see : https://epfml.github.io/attention-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhUuLf-lhldG"
   },
   "source": [
    "# III - Transformers using HuggingFace\n",
    "\n",
    "We will classify sentences. In this case we leverage from BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvQmppwiJrV5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyT3Xda7awPg"
   },
   "source": [
    "The goal of this lab is to fine-tune a transformer model that can accurately determine whether two given sentences are paraphrases (semantically equivalent) of each other using the Microsoft Research Paraphrase Corpus (MRPC) dataset from the  [GLUE Benchmark](https://gluebenchmark.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8ZCelgNdO1M"
   },
   "source": [
    " The Hugging Face Transformers library is a popular open-source library that provides pre-trained models and various utilities for working with transformer-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOJXtOR-Zzoj"
   },
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6vn3Rp1J1Jo",
    "outputId": "bbfd233d-87bf-4a83-9fc7-7cc540446444"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c57_aiAtNMhr"
   },
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Let's load the GLUE Dataset and do some data exploration. Beware the format is not the same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh_VmcoIKy1M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KAZ1IblK7sQ"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", 'mrpc')\n",
    "metric = load_metric('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyAvBQFw4wwV"
   },
   "outputs": [],
   "source": [
    "# TODO : Explore your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZsYP7vULrzr"
   },
   "source": [
    "TODO : Analyse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isdDDVrxNP4y"
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Can we put the input sentences as is ? I don't think so, we need to perform tokenization. HuggingFace has already implemented some Tokenizers for us. Let's use them.\n",
    "\n",
    "Documentation :\n",
    "https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "https://huggingface.co/transformers/v3.0.2/model_doc/auto.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyCfqH9YNTd4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\" # TODO : We choose distillbert has it is a smaller distilled model. But feel free to choose something else.\n",
    "tokenizer = AutoTokenizer.from_pretrained(...,\n",
    "                                          use_fast=True) # Load the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoECEi0H5uNt"
   },
   "source": [
    "Now let's test the tokenization.\n",
    "\n",
    "TODO : Test different type of sentences:\n",
    "- Sentences from different languages\n",
    "- Sentences that has no linked meaning\n",
    "- Sentences with wrong words.\n",
    "\n",
    "Question : What is the attention mask ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ-dgxr0NdCK"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it to Disneyland.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOK59qV4dbQi"
   },
   "source": [
    "Now apply the tokenizer to sentence of our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bd9RFd3POWlm"
   },
   "outputs": [],
   "source": [
    "# TODO : Print one element of the dataset. How do we access them ?\n",
    "print(f\"Sentence 1: {dataset['train'][0][...]}\")\n",
    "print(f\"Sentence 2: {dataset['train'][0][...]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2l6hhdMOe1b"
   },
   "outputs": [],
   "source": [
    "# TODO : Define a function that applies tokenization to the input. Please keep the format of the input sentences.\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  return tokenizer(examples[...], examples[...], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx2eX8rK6rps"
   },
   "source": [
    "Finally we are going to reencode the whole dataset using our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2309bd657e014edeb42212f386c5f58c",
      "48a5644a90334eac8228aabecf062e54",
      "235c283d90f24aefb1d9fb874cb71272",
      "5dc8e085338e4002a9a088128c3d7cc2",
      "412c05c6fcf64d3aa5ba226c902de87b",
      "658e8487db6444799624024b5d91f466",
      "c21db446e9be41708b44923dbd1b3e47",
      "3edaeb1a19b449448f482916d0f4cbe2",
      "69a5ff137dd14ab892e56af847426efe",
      "dbf433ae581f4413a85a038f8864ae6f",
      "507a911bb7db470883ef921b68df30ce"
     ]
    },
    "id": "p4ZozY6EPMi0",
    "outputId": "8c514dde-23d8-4a98-8085-b5757a09e9e3"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCxFgUnqQYAX"
   },
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "We are going to leverage from the AutoModels from HF, that encompasses lots of thing (model, processings, trainings,...) within one class. This is similar to the creation of a Trainer, model, module when using PytorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVLQBugzPgjN",
    "outputId": "9ae3b6ce-a0ce-4a0b-ca0f-d8c58b24b7d0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = ... # TODO : Define your number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(..., # TODO : Initialize your Pipeline with the model. Which one ?\n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeNI1WnY7XX2"
   },
   "source": [
    "Now let's use the HF Trainer. Similar to the trainer in Lightning, there are lots of arguments we can leverage from. Let's keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AE69jQcKQD6C"
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "metric_name = ... # TODO : What Metric would you define here ?\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-mrpc\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4ncImoARMN7"
   },
   "outputs": [],
   "source": [
    "# TODO : Define the metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = ... # TODO : What goes here ?\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVBlu7867wEA"
   },
   "source": [
    "Now let's init the Trainer and train and evaluate the model. It is as easy as in Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5vfUcAvS8PG"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset= ..., # TODO : What do we put here ?\n",
    "    eval_dataset= ... , # TODO : What do we put here ?\n",
    "    tokenizer= ..., # TODO : What do we put here ?\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JzA2z6US-oA"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSNaMAlCUAwU"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaiuyubNd1K7"
   },
   "source": [
    "How can we improve the results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XVfpmUBWhKa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1J8PBTInJss5L9AaSvW2e",
   "collapsed_sections": [
    "cFhsEWCEzp3b",
    "Mat6law2gdra",
    "VRPjvY2d7AP9",
    "jhUuLf-lhldG"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
