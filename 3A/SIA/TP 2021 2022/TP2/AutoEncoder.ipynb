{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/SIA/TP2/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcv0xtfj7HsN"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUZCOMw1Gem5"
   },
   "source": [
    "# Autoencoders\n",
    "\n",
    "Time : 1h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "315B4XDj9ln-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fup2OihC7Kkv"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab we'll be using Generative Model. We are going to recreate an Input that's been deconstructed. As a helper for your training we will use Pytorch Lightning along with Pytorch. \n",
    "\n",
    "Goal of this lab :\n",
    "* Understand AutoEncoders\n",
    "* Reconstruct Images\n",
    "* Denoise Images\n",
    "* Code in Class and use PyTorch Lightning\n",
    "* Show that lightning can be used along PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd5KFdqN_m4M"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "You will use the MNIST dataset in order to train and test you model. You won't have to code the Dataset as Pytorch has already coded it for you. Please refer to the introduction to Lightning if needed.\n",
    "\n",
    "Load the MNIST dataset, and setup what is needed for the training. Refer to the 1rst lab if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX7tuNthA1zB"
   },
   "outputs": [],
   "source": [
    "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test =  MNIST('', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "mnist_train, _ = \n",
    "\n",
    "train_loader = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K32VD7Jm7oo4"
   },
   "source": [
    "# Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv7uFzMz7TEG"
   },
   "source": [
    "An autoencoder is a model able to reconstruct an input from a Latent Space.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/0*b5eT77a_idC3v3BP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX29RrTh8Wx2"
   },
   "source": [
    "An autoencoder (AE) is composed of 2 models :\n",
    "* an encoder model that encodes input data into a latent space\n",
    "* a decoder model that recreates the input image from the latent space\n",
    "\n",
    "<img src = \"https://static.wikia.nocookie.net/dragonball/images/f/f8/Goten-Trunks-Fusion-dragon-ball-all-fusion-33379377-855-482.png/revision/latest?cb=20181202000509&path-prefix=fr\">\n",
    "\n",
    "You will code two types of AE models :\n",
    "* MLP style\n",
    "* Conv style\n",
    "\n",
    "**Before beginning anything take a piece of paper and draw a scheme of what your model will look like**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igiyUUPS888C"
   },
   "source": [
    "# Encoder\n",
    "\n",
    "The encoder encodes the input image(s). Indeed, it takes the image and extracts features thanks to its different layers. The deeper the model, the complexer the extracted features will be. At the end of the Decoded Network you will have a latent representation of the image.\n",
    "* However is it necessary to have very Deep Networks ? \n",
    "* Create a **Fully Connected** Encoder Model. \n",
    "\n",
    "Your model must be instanciated using two parameters : \n",
    "* Input shape\n",
    "* Latent size\n",
    "\n",
    "What are those parameters ?\n",
    "\n",
    "Feel free to design your own model, by using whatever Activation function and Layer size you want (but can you ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4copY8_Q87fK"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_shape,latent_size):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjkOPYoY91E1"
   },
   "source": [
    "# Decoder\n",
    "\n",
    "\n",
    "The Decoder takes as input the latent representation of the input vector and recreates the input image.\n",
    "* Create a Decoder model.\n",
    "\n",
    "Your model must be instanciated using two parameters : \n",
    "* Output shape\n",
    "* Latent size\n",
    "\n",
    "What are those parameters ? Are they different from the Encoder ?\n",
    "\n",
    "\n",
    "Feel free to design your own model, by using whatever Activation function and Layer size you want (but can you ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-XPV5Kc8rLC"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_size, output_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "  def forward(self,x):\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQzk7G4g_LXi"
   },
   "source": [
    "# Autoencoder\n",
    "\n",
    "\n",
    "Let's create the AutoEncoder. Using Lightning Framework,  :     \n",
    "* Stack the encoder and the decoder in order to create the AE. \n",
    "\n",
    "Now let's think a little bit before beginning :\n",
    "* What task is it ? (Classification ? Regression ?)\n",
    "* What optimizer will you be using ?\n",
    "* What learning rate ?\n",
    "* Will your model learn ? (overfit_batches will help)\n",
    "* What is the impact of the latent_space size ?\n",
    "* What loss shold we be using ?\n",
    "\n",
    "Don't forget to log the needed values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6fr5hrs_Ksz"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, latent_size, input_shape, output_shape):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "\n",
    "        self.encoder = \n",
    "        self.decoder = \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        reconstructed_image = \n",
    "        return reconstructed_image\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x,y = train_batch\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x,y = val_batch\n",
    "        self.log('val_loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfMirANECMZm"
   },
   "source": [
    "Set up Tensorboard for observation. The given directory is the default directory and might not the created at the moment you run the code. In fact, the trainer will create the folder where the checkpoints are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PXeMOVsCR0z"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"/content/tb_logs/my_model/version_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8BmZ3vXBApf"
   },
   "source": [
    "Now let's create your Trainer. \n",
    "* Load your model\n",
    "* Create your lightning Trainer\n",
    "* Fit the data to your model.\n",
    "\n",
    "Once your trainer set up, train for 10 epochs and watch your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKhaiie2BILx"
   },
   "outputs": [],
   "source": [
    "model = AutoEncoder(_, _)\n",
    "trainer = pl.Trainer(gpus=-1,max_epochs=_)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYtseLWkCUFQ"
   },
   "source": [
    "# Test\n",
    "\n",
    "\n",
    "Now that you have trained your model, let's test it. We provide you a function that will help you to load your model and use it. \n",
    "You might need to do something on your testing data before sending it to the model. What is it ?\n",
    "\n",
    "Try few images reconstructions.\n",
    "* Where should you pick your testing data from ? \n",
    "* Can we test on training set ? Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtWdzhJaa3Ch"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "def load_model_from_checkpoint(model, checkpoint_path):\n",
    "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
    "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
    "    model = model.load_from_checkpoint(ckp)\n",
    "    model.freeze()\n",
    "    return model\n",
    "\n",
    "def forward_and_print_image(model,image):\n",
    "  image = model(test_image.view(1,-1))\n",
    "  image_reco = image.view(28,28)\n",
    "  plt.imshow(image_reco)\n",
    "\n",
    "\n",
    "model = load_model_from_checkpoint(model,_)\n",
    "test_image = mnist_val.__getitem__(1)[0].unsqueeze(0)\n",
    "forward_and_print_image(model,test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqxGu3xXjVNo"
   },
   "source": [
    "# Convolutional Type\n",
    "\n",
    "In the previous episode of AutoEncoder, we have seen Fully Connected AE. Let's see another one : Convolutional AE.\n",
    "\n",
    "We will make you code in a different way, using Module Lists. But at the end the results will be the same : we are going to make the same reconstruction using Convolution Layers.\n",
    "\n",
    "By looking at the following links, **explain each parameters of the nn.Conv2d and nn.ConvTranspose2d methods** :\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "* https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
    "* https://ezyang.github.io/convolution-visualizer/\n",
    "\n",
    "\n",
    "We will ask you few things :\n",
    "* Calculate the size of the images after each pass through a layer of your model. Write the formula, you're using for ths calculation.\n",
    "* Are you going to use MaxPooling ? If yes, explain with a photo what Maxpooling does. \n",
    "* Can we revert MaxPooling effect without information loss ?\n",
    "\n",
    "You will still use the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMyrG9K10Sui"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xshb4Xc7z0Eq"
   },
   "source": [
    "# Encoder Conv\n",
    "\n",
    "First let's write one Bottleneck of your Convolutional Encoder.\n",
    "\n",
    "Your Model should be instanciated using :    \n",
    "* in_channels : number of input channels\n",
    "* out_channels : number of output channels\n",
    "\n",
    "Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vA27jptz-NM"
   },
   "outputs": [],
   "source": [
    "class ConvDown(nn.Module):\n",
    "      def __init__(self, in_channels, out_channels):\n",
    "            super(ConvDown,self).__init__()\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(...\n",
    "\n",
    "      def forward(self,x):\n",
    "\n",
    "          return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP2OhrY8z_eM"
   },
   "source": [
    "# Decoder Conv\n",
    "\n",
    "blablabla.\n",
    "Write your Decoding Bottleneck.\n",
    "Be careful if you used MaxPooling for the shape of your inputs. Don't hesitate to print each output shape. \n",
    "\n",
    "\n",
    "Your Model should be instanciated using :    \n",
    "* in_channels : number of input channels\n",
    "* out_channels : number of output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhDLCCeZjXod"
   },
   "outputs": [],
   "source": [
    "class ConvUp(nn.Module):\n",
    "      def __init__(self, in_channels, out_channels):\n",
    "            super(ConvUp,self).__init__()\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(....\n",
    "            )\n",
    "\n",
    "      def forward(self,x):\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DnOnYU70Xrd"
   },
   "source": [
    "# Create your Conv AE.\n",
    "\n",
    "\n",
    "Let's make things a Lil' bit spicy. We are going to stack the Bottlenecks in order to create our Encoder and Decoder. Then, we are going to stack the encoder and the Decoder in order to create the AutoEncoder. Easy ?\n",
    "\n",
    "Your model must be instanciated using :\n",
    "* in channels \n",
    "* out_channels\n",
    "* feature channel shape : list of feature channel shape you want.\n",
    "\n",
    "\n",
    "The given feature_shape can be indeed changed.\n",
    "Using nn.ModuleList() create your encoder and your decoder. \n",
    "Have a look at : https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "\n",
    "\n",
    "We have written the forward for you. Take some inspiration from it to define your training step. Don't forget to log some values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxH33SFx0V8B"
   },
   "outputs": [],
   "source": [
    "class AutoEncoderConv(pl.LightningModule):\n",
    "    def __init__(self, in_channels, out_channels,feature_shape = [3,16,32]):\n",
    "        super(AutoEncoderConv,self).__init__()\n",
    "        self.save_hyperparameters() #mandatory\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.feature_shape = feature_shape\n",
    "        self.reverse_list = list(reversed([self.out_channels]+self.feature_shape))\n",
    "        for feature in self.feature_shape:\n",
    "            self.encoder.append(_) #UP or DOWN ?\n",
    "            in_channels = feature\n",
    "        for i,_ in enumerate(self.reverse_list):\n",
    "            if i < len(self.reverse_list) -1:\n",
    "                self.decoder.append(_) #UP or DOWN ?\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i,down in enumerate(self.encoder):\n",
    "          x = down(x)\n",
    "        x_reconstructed=x\n",
    "        for i,up in enumerate(self.decoder):\n",
    "          x_reconstructed = up(x_reconstructed)\n",
    "        return x_reconstructed\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x,y = train_batch\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x,y = val_batch\n",
    "        self.log('val_loss', loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQWWLagA1uAe"
   },
   "source": [
    "# Load your Logger\n",
    "\n",
    "When training a model you want to get a log of every useful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piQxjvIK1tdJ"
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JotPUWo2OrS"
   },
   "source": [
    "# Train\n",
    "\n",
    "Fit that model. Don't forget to load tensorboard before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZHr1ujc2Wcv"
   },
   "outputs": [],
   "source": [
    "model = AutoEncoderConv(_, _)\n",
    "trainer = pl.Trainer(gpus=-1,logger = logger,max_epochs = _)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlmqjXHveXna"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"/content/tb_logs/my_model/version_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azB_n4wT2ZNv"
   },
   "source": [
    "# Test\n",
    "\n",
    "Now by loading the last checkpoint of your trained model, show some reconstructed images from the Test set.\n",
    "\n",
    "* How is the reconstruction ?\n",
    "\n",
    "We provide two functions to help you in this mission :\n",
    "* load_model_from_chekcpoint that return the model with the weights loaded\n",
    "* forward_and_print_image that forwards the image through the model and prints the output\n",
    "\n",
    "Don't forget these squeezes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkOVtUgQDEys"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_model_from_chekcpoint(model, checkpoint_path):\n",
    "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
    "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
    "    checkpoint = torch.load(ckp)  \n",
    "    model =  model.load_from_checkpoint(ckp,hparams_file = checkpoint_path+'/hparams.yaml')\n",
    "    model.freeze()\n",
    "    return model\n",
    "\n",
    "def forward_and_print_image(model,image):\n",
    "  image = model(test_image)\n",
    "  image_reco = image.view(28,28)\n",
    "  plt.imshow(image_reco)\n",
    "\n",
    "\n",
    "model = load_model_from_chekcpoint(_,_)\n",
    "test_image = \n",
    "plt.imshow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yopEX26n3GxG"
   },
   "outputs": [],
   "source": [
    "forward_and_print_image(_,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtlZ-Rba2OFX"
   },
   "source": [
    "AutoEncoders got few advantages :\n",
    "* It compresses images \n",
    "* It is used for denoising Images\n",
    "* It can be used for Anomaly Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11HbQ67X7_bZ"
   },
   "source": [
    "# Adding Classification\n",
    "\n",
    "Let's get back to FC AE.\n",
    "Let's classify something. Now we are going to classify the reconstructed image.\n",
    "* What task are we adding to the model ? How can we add it ?\n",
    "* Where should we add the corresponding layer ? Draw a Schema of your model. What's the name of these type of models ?\n",
    "* What loss should we use ?\n",
    "* Can we calculate an accuracy ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pzfGw0q8F2y"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder_for_Accuracy(pl.LightningModule):\n",
    "    def __init__(self, latent_size, input_shape,num_class):\n",
    "        super(AutoEncoder_for_Accuracy,self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.input_shape = input_shape\n",
    "        self.num_class = \n",
    "        self.encoder = \n",
    "        self.decoder = \n",
    "        self.fc = \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        return reconstructed_image,label_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x,y = train_batch\n",
    "       \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', accuracy)\n",
    "\n",
    "        return loss+loss_cls\n",
    "\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x,y = val_batch\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x,y = test_batch\n",
    "        \n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCCzZBCDzw9h"
   },
   "source": [
    "Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV5ByagozvZz"
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "model = AutoEncoder_for_Accuracy(_, _,_)\n",
    "trainer = pl.Trainer(gpus=-1,logger = logger,max_epochs=10)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LFbLK4DOJz7"
   },
   "source": [
    "# Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv94XVoIOM1f"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_model_from_chekcpoint(model, checkpoint_path):\n",
    "    ckp = os.listdir(checkpoint_path+'/checkpoints/')\n",
    "    ckp = checkpoint_path + '/checkpoints/' + ckp[0]\n",
    "    checkpoint = torch.load(ckp)  \n",
    "    model =  model.load_from_checkpoint(ckp,hparams_file = checkpoint_path+'/hparams.yaml')\n",
    "    model.freeze()\n",
    "    return model\n",
    "\n",
    "def forward_and_print_image(model,image):\n",
    "    image,label = model(test_image)\n",
    "    label = torch.argmax(label, dim =1)\n",
    "    print('label is: ', label.detach().cpu().numpy()[0])\n",
    "    image_reco = image.view(28,28)\n",
    "    plt.imshow(image_reco)\n",
    "\n",
    "\n",
    "model = load_model_from_chekcpoint(model,\"/content/tb_logs/my_model/version_1/\")\n",
    "test_image = mnist_test.__getitem__(1)[0].unsqueeze(0)\n",
    "plt.imshow(test_image.squeeze(0).squeeze(0))\n",
    "forward_and_print_image(model,test_image.view(28,28)) ####reconstruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlLKlbq0GAGp"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"/content/tb_logs/my_model/version_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2uu6YHx3rmO"
   },
   "source": [
    "# Denoising Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVGa7IBH32Hg"
   },
   "source": [
    "\n",
    "AutoEncoder has that good ability to denoise your input image. Let's try it !\n",
    "* Add various noises to your input images and test what your AutoEncoder outputs.\n",
    "* Where should you pick your test images from ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Upx-rrEC31db"
   },
   "outputs": [],
   "source": [
    "def add_noise(inputs):\n",
    "     noise = _\n",
    "     return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DakEBPhfeEtH"
   },
   "outputs": [],
   "source": [
    "model = load_model_from_chekcpoint(_,_)\n",
    "test_image = \n",
    "test_image_with_noise = \n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnsaa7Pg6hx4"
   },
   "outputs": [],
   "source": [
    "forward_and_print_image(_,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwsvWzvg2kTe"
   },
   "source": [
    "Now let's use the AutoEncoder for harder tasks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AutoEncoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
