{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sTNlEw24MYri",
        "L9-P2_wdO6Cn"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwQOQhigIVgMEWMjhb9Yxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/3A/SIA/TP2/Generative_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkXD8o6gLm8z"
      },
      "outputs": [],
      "source": [
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\n",
        "!tar xvzf /content/maps.tar.gz\n",
        "!pip install pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer"
      ],
      "metadata": {
        "id": "VI1m9nl5QpRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Images\n",
        "\n",
        "In recent studies, Groundbreaking research are done using diffusion models to generate images. However, previously other models were used to perform that task. Image Generation has multiple application in Industries (NFT Creation, Virtual Try Outs, Deep Fake Generation..)\n",
        "\n",
        "Goal of this lab : \n",
        "* Hands on Generative Models\n",
        "* Solidify your knowledge in Deep Learning\n",
        "* Use someone else's code\n",
        "* Reuse Pytorch Lightning\n",
        "* Have an Insight of industrial application of AI"
      ],
      "metadata": {
        "id": "1QDjipXtMVQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to the Basics : AutoEncoders\n",
        "\n",
        "As you might remember in your 2nd year labs, we used an AutoEncoder to recreate some input images. In this part, we will reconstruct data from the MNIST Dataset.\n"
      ],
      "metadata": {
        "id": "sTNlEw24MYri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning DataModule\n",
        "\n",
        "We will reuse the datamodule used in the first lab. So just run the following cell."
      ],
      "metadata": {
        "id": "G7b2qc6hp0_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        self.data_dir = ''\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # This method is used to download beforehand the dataset if needed.\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "        \n",
        "    def setup(self, stage):\n",
        "        # First stage is 'fit' (or None)\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # We create a validation split to watch the training.\n",
        "            mnist_train_dataset = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.train_size = int(0.8 * len(mnist_train_dataset))\n",
        "            self.valid_size = len(mnist_train_dataset) - self.train_size\n",
        "            self.mnist_train, self.mnist_valid =  torch.utils.data.random_split(mnist_train_dataset, [self.train_size, self.valid_size])          \n",
        "        # Second stage is 'test' \n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_valid, self.batch_size, shuffle=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test,self.batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Wl8-pFeGqdRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning Module"
      ],
      "metadata": {
        "id": "2BKlscXEp7N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AutoEncoder : Creating and Using a Latent Space\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*nqzWupxC60iAH2dYrFT78Q.png\">\n",
        "\n",
        "Reminder : An AutoEncoder is composed of an Encoder and a Decoder. The Encoder creates a representation of the input datas, called latent space. The Decoder uses the latent space representation of the Input to reconstruct it.\n",
        "\n",
        "We provide you with a code of an AutoEncoder class. \n",
        "* **Using the given code, create an AutoEncoder composed of 3 stacks.**"
      ],
      "metadata": {
        "id": "l__z-nI1rZsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn \n",
        "import torch\n",
        "class ConvDown(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "      super(ConvDown, self).__init__()\n",
        "      self.in_channels = in_channels\n",
        "      self.out_channels = out_channels\n",
        "      self.model = nn.Sequential(nn.Conv2d(in_channels = self.in_channels,\n",
        "                                            out_channels = self.out_channels,\n",
        "                                            kernel_size = 3,\n",
        "                                            stride = 1,\n",
        "                                            padding = 0,\n",
        "                                            dilation = 1),\n",
        "                                  nn.BatchNorm2d(self.out_channels),\n",
        "                                  nn.Dropout2d(0.5),\n",
        "                                  nn.LeakyReLU(0.2))\n",
        "  def forward(self,x):\n",
        "      return self.model(x)\n",
        "\n",
        "class ConvUp(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "      super(ConvUp, self).__init__()\n",
        "      self.in_channels = in_channels\n",
        "      self.out_channels = out_channels\n",
        "      self.model = nn.Sequential(nn.ConvTranspose2d(in_channels = self.in_channels,\n",
        "                                      out_channels = self.out_channels,\n",
        "                                      kernel_size = 3,\n",
        "                                      stride = 1,\n",
        "                                      padding = 0,\n",
        "                                      dilation = 1),\n",
        "                    nn.LeakyReLU(0.2))\n",
        "  def forward(self,x):\n",
        "      return self.model(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, number_of_stack):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.number_of_stack = number_of_stack\n",
        "    channels = [in_channels]+ [2**i for i in range(3,10)]\n",
        "    self.encoder = nn.ModuleList([ConvDown(channels[i], channels[i+1]) for i in range(number_of_stack)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i, layer in enumerate(self.encoder):\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, out_channels,number_of_stack ):\n",
        "    super(Decoder, self).__init__()\n",
        "    channels = [out_channels]+ [2**i for i in range(3,10)]\n",
        "    self.decoder = nn.ModuleList([ConvUp(channels[i+1], channels[i]) for i in range(number_of_stack)])[::-1]\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i, layer in enumerate(self.decoder):\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "############################################## TODO ##############################################\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, in_channels, number_of_stack):\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.number_of_stack = number_of_stack\n",
        "    self.encoder = Encoder(self.in_channels, number_of_stack)\n",
        "    self.decoder = Decoder(self.in_channels, number_of_stack)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO : Define your forward\n",
        "    latent_representation = self.encoder(...)\n",
        "    reconstructed_image = self.decoder(...)\n",
        "    return reconstructed_image\n",
        "\n",
        "# TODO : Create a model composed of 3 stacks\n",
        "model = ..."
      ],
      "metadata": {
        "id": "9LDS2vTrrWjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Create your Lightning Module\n",
        "\n"
      ],
      "metadata": {
        "id": "2Dv4HUvJ-U1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoderPL(pl.LightningModule):\n",
        "    def __init__(self, in_channels, number_of_stack):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # TODO : Define your model here.\n",
        "        self.model = ...\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO : Send the input through your model\n",
        "        return ...\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Choose your optimizer \n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        # Define your Training Step\n",
        "        # This method is pretty much similar to what your did in the first lab to train your model.\n",
        "        x,y = train_batch\n",
        "        x_reconstructed = ...\n",
        "        loss = ...\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        # Define your Validation Step\n",
        "        # What is the difference between the Training and the Validation Step ?\n",
        "        x,y = val_batch\n",
        "        x_reconstructed = ...\n",
        "        loss = ...\n",
        "        self.log('val_loss', loss)\n",
        "    \n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        # Define your Test Step\n",
        "        # What is the difference between the Training, Validation and Test Step ?\n",
        "        x,y = test_batch\n",
        "        x_reconstructed = ...\n",
        "        loss = ...\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('test_loss', loss)\n",
        "\n",
        "    # TODO : Do we have to add some functions (test_epoch_start, test_epoch_end..) to compute a reconstruction metric ?"
      ],
      "metadata": {
        "id": "lkVr4Eca-T5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "4cWbcW3nFIJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Setting TensorBoard up"
      ],
      "metadata": {
        "id": "o-VFE5czDzK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/tb_logs/my_model/version_0\" # You might have to change the name of the folder, just look what folder were created on your Colab environment"
      ],
      "metadata": {
        "id": "awoGXQzxD1lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Launch the Training"
      ],
      "metadata": {
        "id": "YAoh46OIEG2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : \n",
        "model = AutoEncoderPL(...,...)\n",
        "dm = MNISTDataModule(...)\n",
        "trainer = pl.Trainer(gpus=-1,max_epochs=...)\n",
        "trainer.fit(model, dm)"
      ],
      "metadata": {
        "id": "zcfeXB7zCZKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing\n",
        "\n",
        "Test some images from the testing dataset to verify that your model works."
      ],
      "metadata": {
        "id": "OL2fgGwDFcCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Load your test dataloader from the datamodule\n",
        "\n",
        "# TODO : Compute your average reconstruction on the test set.\n"
      ],
      "metadata": {
        "id": "ycJetDY1GFSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot some reconstructed images. Are they correct ?\n"
      ],
      "metadata": {
        "id": "v0b3xbvLc38Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Going Further on AutoEncoders (Optional)\n",
        "\n",
        "If you have some time, create other AutoEncoder with differnet latent sizes and train it.\n",
        "* **What is the impact of the latent space toward the images reconstructions ?**"
      ],
      "metadata": {
        "id": "NEvmHfVzcAmz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QYO2G9oHdunE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Industrial Application of GAN : Creating Google Maps Calque from Google Map Satellite view \n",
        "\n",
        "Working on basic datasets is cool but as you are engineers to be, let's take it to industrial level. Given an image of a Google Map Satellite view, we want the model to predict a Calque view from it. This task is an Image translation task.\n",
        "\n",
        "For example, using the image on the left, we want the model to predict the image on the right.\n",
        "<img src=\"https://eu42.github.io/images/gcgan/aerial_image_map_example.png\" height = 200>\n",
        "\n"
      ],
      "metadata": {
        "id": "buqa0Hb1Mek3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning DataModule \n",
        "\n"
      ],
      "metadata": {
        "id": "9lF1mAs_Nbh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "First, we need to write the Dataset. As usual, a custom Dataset class must implement three functions: \n",
        "* __init__\n",
        "* __len__\n",
        "* __getitem__\n",
        "\n",
        "Each image's shape is 1200x600x3. The satellite and the calque has the same size and are exactly half of the image\n",
        "\n",
        "As the dataset doesn't have a proper test dataset, we will use the validation dataset as the test set. We will further create a validation dataset by taking a portion of the train dataset"
      ],
      "metadata": {
        "id": "L9-P2_wdO6Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleDataset(Dataset):\n",
        "\n",
        "  def __init__(self, path, transform = None):\n",
        "    self.path = path\n",
        "    self.images = [path + i for i in os.listdir(path)]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    image = self.images[idx]\n",
        "    full_image = cv2.imread(image)\n",
        "    h,w,c = full_image.shape\n",
        "    # TODO : Retrieve the correct parts of the image.\n",
        "    satellite = ...\n",
        "    calque = ...\n",
        "    if self.transform : \n",
        "      satellite = self.transform(satellite)\n",
        "      calque = self.transform(calque)\n",
        "    return {'satellite':satellite,\n",
        "            'calque':calque}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n"
      ],
      "metadata": {
        "id": "YV7WH-H4P9Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lightning DataModule\n",
        "\n",
        "Now, let's prepare our DataModule."
      ],
      "metadata": {
        "id": "Ah60SBreZk6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleMapDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        # TODO : In your Transformation, transform to Tensor and resize images to 128x128\n",
        "        self.transform =  transforms.Compose([...,\n",
        "                                              ...])\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # we are hardcoding the path are they won't change\n",
        "        self.train_path = '/content/maps/train/' \n",
        "        self.test_path = '/content/maps/val/'\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # TODO : load the train and test dataset\n",
        "        GoogleDataset(..., self.transform)\n",
        "        GoogleDataset(..., self.transform)\n",
        "\n",
        "\n",
        "    def setup(self, stage):\n",
        "\n",
        "        #First stage is 'fit' (or None)\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # We create a validation split to watch the training.\n",
        "            google_train = GoogleDataset(self.train_path, self.transform)\n",
        "            train_size = int(0.7 * len(google_train ))\n",
        "            test_size = len(google_train ) - train_size\n",
        "            self.google_train, self.google_valid =  torch.utils.data.random_split(google_train , [train_size, test_size])\n",
        "        #Second stage is 'test' \n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.google_test = GoogleDataset(self.test_path, self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # TODO : Now create your Training DataLoader\n",
        "        return ...\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # TODO : Now create your Validation DataLoader\n",
        "        return ...\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # TODO : Now create your Testing DataLoader\n",
        "        return ..."
      ],
      "metadata": {
        "id": "quUkvgCPZ5yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning Module\n",
        "\n",
        "* **Can we use an AutoEncoder to perform this task ?**\n"
      ],
      "metadata": {
        "id": "K9NS4X0MOZM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a much complex model. Generative Adversarial Networks."
      ],
      "metadata": {
        "id": "8mzmeYzHd-KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pix2Pix : cGAN \n",
        "\n",
        "Instead of using a simple GAN, we will use a  conditional GAN (cGAN).\n",
        "\n",
        "<img src='https://www.researchgate.net/profile/Gerasimos-Spanakis/publication/330474693/figure/fig1/AS:956606955139072@1605084279074/GAN-conditional-GAN-CGAN-and-auxiliary-classifier-GAN-ACGAN-architectures-where-x_Q320.jpg'>\n",
        "\n",
        " On the contrary of a normal GAN, cGAN has a condition that will help us CONTROL how the GAN should generate images. The generator will take some \"inspiration\" from the Condition. For example, if you want to generate digits from the MNIST Dataset, you can add a condition to force the GAN to create a specific Digit. In the MNIST case, the condition would be the class label.\n",
        "\n",
        "Further details are in the following paper : https://arxiv.org/pdf/1411.1784.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "7Zq_IBN6iHOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Generator : Generating Images\n",
        "\n",
        "In general, the Generator is here to generate Data from Noise. For example if we train a GAN on MNIST dataset, the generator will create digits using the noise we give it as input. In this lab, we are dealing with cGANs, so we need a Condition to condition our GAN. \n",
        "* **What could be the condition in this task ?** \n",
        "\n",
        "To answer that :\n",
        "* **What are we trying to do ?**\n",
        "* **What should the Generator do ? Should it recreate something specific ?**\n",
        "\n"
      ],
      "metadata": {
        "id": "p2KmN1xri0dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use an AutoEncoder to perform the image translation. \n",
        "* **Using previous classes, initialize your Generator using a 3 stack AutoEncoder.**"
      ],
      "metadata": {
        "id": "uE9V2RNcHC_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    # TODO : Create your Generator using the previous AutoEncoder Class\n",
        "    def __init__(self,...,...,...):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,x):"
      ],
      "metadata": {
        "id": "aiX0iil2m3Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### The Discriminator : Forcing the Generator to predict better\n",
        "\n",
        "The generator is able to create Images from Inputs, but we want the best quality possible. We need a model that would indicate to the Generator whether the generated Image are correct or not : the Discriminator.\n",
        "\n",
        "The Discriminator is here to force the Generator to create better and better images. While the Generator creates bad quality generated image, the Discriminator will give a feedback.\n",
        "\n",
        "<img src=\"https://i.imgflip.com/6y0kqe.jpg\">\n",
        "\n",
        "* **What could be the worst case scenario with the Generator and the Discriminator ?**\n",
        "\n",
        "We provide you the code of a Discriminator that \"works well\"."
      ],
      "metadata": {
        "id": "s3myz15ki-Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriConv(nn.Module):    \n",
        "  def __init__(self, in_channels, out_channels, kernel_size):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(nn.Conv2d(in_channels = in_channels,\n",
        "                                  out_channels = out_channels,\n",
        "                                  kernel_size = kernel_size,\n",
        "                                  stride = 2,\n",
        "                                  padding = 1),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.LeakyReLU(0.2, inplace=True))\n",
        "       \n",
        "  def forward(self,x):\n",
        "      return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.main = nn.Sequential(DiscriConv(in_channels, 32,3),\n",
        "                              DiscriConv(32, 64,4),\n",
        "                              DiscriConv(64, 128,4),\n",
        "                              # 128x4x4\n",
        "                              nn.Conv2d(in_channels = 128,\n",
        "                                        out_channels = out_channels,\n",
        "                                        kernel_size = 3,\n",
        "                                        bias=False))\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.main(input)"
      ],
      "metadata": {
        "id": "ISB7WF6Om3tY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Loss : Adversarial Battle\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*d96q6bCKbmZT9Ls7f3X6xg.jpeg\">\n",
        "\n",
        "This formula is the basic loss of the cGAN model. There are two terms, each optimizing specific.\n",
        "Beautiful formula's no ? Let's understand them like humans.\n",
        "* What is x, y, z in our Case ? \n",
        "\n",
        "\n",
        "If we look at the formulas, there are two losses :\n",
        "* What kind of losses are they ? Cross Entropies, Distance Losses ?\n",
        "* Which term is the Discriminator loss ? Generator loss ?\n",
        "\n",
        "The entire model will converge when the Generator provides the best fakes possible that the Discriminator won't be able to differentiate from the real images."
      ],
      "metadata": {
        "id": "gW3jm9STkfX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GANLoss(nn.Module):\n",
        "      def __init__(self, real_label=1, fake_label=0):\n",
        "        super().__init__()\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        # Initialization a binary label, if real label 1, if fake label 0\n",
        "        self.real_label = real_label\n",
        "        self.fake_label = fake_label\n",
        "\n",
        "      def get_labels(self, predictions, real_or_not):\n",
        "          # TODO : If we have a real data as input, we want its target label to be self.real_label. If we have fake data as input, we want its target lable to be self.fake_label\n",
        "          # Fill in the ...\n",
        "          labels = ... if real_or_not else ...\n",
        "          return torch.tensor(labels).expand_as(predictions).to(predictions.device)\n",
        "      \n",
        "      def forward(self, predictions, real_or_not):\n",
        "          # TODO : Return the Loss between the models prediction and the labels\n",
        "          return "
      ],
      "metadata": {
        "id": "jmNbS0vPl4AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Final Module\n",
        "\n",
        "Let's encompass everything under our Lightning Module. However, there are many steps to follow. In order to understand the code, read the commented parts."
      ],
      "metadata": {
        "id": "o1Eg96OZkpj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(pl.LightningModule):\n",
        "  def __init__(self ):\n",
        "    super().__init__()\n",
        "    # TODO : Initalize your Generator and Discriminator\n",
        "    self.generator = Generator(in_channels = ...,\n",
        "                               latent_size = ...,\n",
        "                               out_channels = ...)\n",
        "    self.discriminator = Discriminator(in_channels = 2*3,\n",
        "                                       out_channels = ...)\n",
        "\n",
        "  def forward(self,satellite):\n",
        "    img_fake = self.generator(satellite)\n",
        "    return img_fake\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx, optimizer_idx):\n",
        "    # TODO:  Follow the steps\n",
        "    satellite, calque = train_batch['satellite'], train_batch['calque']\n",
        "    # Disclaimer : You might have to put some self. ...\n",
        "    criterion = GANLoss()\n",
        "    criterionL1 = nn.L1Loss() # We add the L1 Loss for better correspondance between the colors\n",
        "    if optimizer_idx == 1:\n",
        "        # Discriminator Training Part\n",
        "        # Goal : Train the Discriminator to differentiate fake and real data\n",
        "        # Part 0 : Train on Fake data\n",
        "        # TODO : Step 1 : Send the Satellite image through the generator to create a a Fake Calque\n",
        "        fake_calque = self.generator(...)\n",
        "        # Concatenating the Input Image and the Fake Generated Calque before sending to the Discriminator\n",
        "        fake_data = torch.cat([satellite, fake_calque], dim = 1)\n",
        "        # TODO : Step 2 : Send the fake data to the Discriminator\n",
        "        fake_prediction = self.discriminator(...)\n",
        "        # Compute Loss\n",
        "        loss_fake = criterion(real_prediction, real_or_not = False) # In this case, we know that the Image are fake.\n",
        "        # Part 1 : Train on Real data\n",
        "        # TODO : Step 1 : Concatenate the Real Calque to the Satellite\n",
        "        real_data = torch.cat([...,...], dim = 1)\n",
        "        # TODO : Step 2 : Send the Data through the Discriminator\n",
        "        real_prediction = self.discriminator(...)\n",
        "        # TODO : Compute the loss.\n",
        "        loss_real = criterion(...,real_or_not = ...)\n",
        "        loss =(loss_real + loss_fake)/2\n",
        "        self.log('discriminator loss', loss)\n",
        "        return loss\n",
        "\n",
        "    if optimizer_idx == 0 :\n",
        "        # Generator Training Part\n",
        "        # Goal : Train the Generator to create best Images\n",
        "        # TODO : Step 1 : Send the Satellite image through the generator to create a a Fake Calque\n",
        "        fake_calque = self.generator(...)\n",
        "        fake_data = torch.cat([satellite, fake_calque], dim = 1)\n",
        "        # TODO : Step 2 : Send the fake data to the Discriminator\n",
        "        fake_prediction = self.discriminator(...)\n",
        "        # TODO : Step 3 : Compute the loss, don't forget that we want to create the best images possible, so what must be the value of real_or_not ?\n",
        "        loss_true = criterion(...,real_or_not = ...)\n",
        "        loss_l1 = criterionL1(fake_calque, calque)* 100 # Adding the L1 Loss for color matching\n",
        "        self.log('generator loss', loss_true)\n",
        "\n",
        "        return loss_true + loss_l1\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "\n",
        "    satellite, calque = val_batch['satellite'], val_batch['calque']\n",
        "    criterion = GANLoss()\n",
        "    criterionL1 = nn.L1Loss()\n",
        "    # TODO : Send the Satellite Image through the Generator\n",
        "    fake_calque = self.generator(...)\n",
        "    # TODO : Compute the Loss between the fake calque and the real calque\n",
        "\n",
        "    # TODO : Send the fake calque, the satellite to the Discriminator, and compute the loss\n",
        "\n",
        "    # TODO : Send the real calque, the satellite to the Discriminator,and compute the loss\n",
        "\n",
        "    # TODO : Don't forget to log the losses\n",
        "\n",
        "  def test_step(self, test_batch, batch_idx):\n",
        "\n",
        "    satellite, calque = test_batch['satellite'], test_batch['calque']\n",
        "    criterion = GANLoss()\n",
        "    criterionL1 = nn.L1Loss()\n",
        "    # TODO : Send the Satellite Image through the Generator\n",
        "    fake_calque = self.generator(...)\n",
        "    # TODO : Compute the Loss between the fake calque and the real calque\n",
        "\n",
        "    # TODO : Send the fake calque, the satellite to the Discriminator, and compute the loss\n",
        "\n",
        "    # TODO : Send the real calque, the satellite to the Discriminator,and compute the loss\n",
        "\n",
        "    # TODO : Don't forget to log the losses\n",
        "\n",
        "    \n",
        "  def configure_optimizers(self):\n",
        "    # As we are optimizing to model, we will use 2 optimizer as the generator and discriminator \n",
        "    # don't have the same architecture. \n",
        "    optimizer_generator = torch.optim.Adam(self.generator.parameters(), \n",
        "                                           lr=2e-4, \n",
        "                                           betas= (0.005, 0.999))\n",
        "    optimizer_discriminator = torch.optim.Adam(self.discriminator.parameters(),  \n",
        "                                           lr=2e-4, \n",
        "                                           betas= (0.005, 0.999))\n",
        "    # Lightning will take this list of generator along with the index of each optimizer\n",
        "    return [optimizer_generator, optimizer_discriminator], []"
      ],
      "metadata": {
        "id": "Ekmi9LH5mxgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Train your model. We suggest you to train the model at least 10 epochs to see 'results'\n"
      ],
      "metadata": {
        "id": "sr0XTJXgOZpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule = ...\n",
        "model = ...\n",
        "trainer = ..."
      ],
      "metadata": {
        "id": "s9xaV1fhCi-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "Test your model on few images of the test dataset. You can further try the model by cropping Satellite views from Google Maps"
      ],
      "metadata": {
        "id": "sYMyEHzgC_vo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXvnQkMqDBxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}