{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFfsaZ48xBILmUjeyXfKqU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/TP_ENSEA_ELEVE/blob/main/3A/SIA/TP4/Transformer_in_Real_Life.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-iNSH47r3uu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer : A Model Used Everywhere"
      ],
      "metadata": {
        "id": "RAV2huZur75y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal of this lab :     \n",
        "\n",
        "\n",
        "*   Understand basic Transformer \n",
        "*   Introduction to NLP, Finance, Image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aGZZ0bSCsCp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen through multiple labs industrial applications of Deep Learning models in Computer Vision. Now let's look at other tasks."
      ],
      "metadata": {
        "id": "1RqzZHrcNQx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer : Definitions\n"
      ],
      "metadata": {
        "id": "g7VWYvDMNpNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "\n",
        "The transformer is a  network architecture based on attention mechanisms. It consists of an encoder-decoder architecture: \n",
        "* the encoder maps an input sequence of symbol representations $(x_1,…,x_n)$ to a sequence $z=(z_1,…,z_n)$ \n",
        "* the decoder, given $z$ , generates an output sequence $(y_1,…,y_m)$. \n",
        "\n",
        "At each time step the model consumes the previously generated symbols as additional input when generating the next (it is auto-regressive) \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" width= 400>"
      ],
      "metadata": {
        "id": "t2nagjfnOO9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "Transformers work on a sequence of Data. Order does matter. \n",
        "Example :  \n",
        "\n",
        "\n",
        "> **Vivre pour manger =! Manger pour vivre.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For recurrent networks (RNN, LSTM, GRU), the word positions are implicitly embedded inside the model since they are processed sequentially. The Transformer doesn't process sequentially the input. How can we specify the position of a Token ?\n",
        "The solution of the authors was to add a vector representing the position of the words to the embedding vectors: \n",
        "\n",
        "\n",
        "> **THE POSITIONAL ENCODING**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HJarJb4OuV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention : The reason of this model\n",
        "\n",
        "Attention , self-attention and multi-head attention are the main component of this model."
      ],
      "metadata": {
        "id": "lQKt23MiP10x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wL4ncywNWJJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finance : Is attention all you need ?"
      ],
      "metadata": {
        "id": "zyBltd6vNIiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6"
      ],
      "metadata": {
        "id": "nZfLz7trNMT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "RhVCKt7SMVpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The usage of Self Supervised Learning\n",
        "\n",
        "In the previous lab, we saw that self supervised learning is used as pretraining of the model. But what are the real applications of SSL in companies ?"
      ],
      "metadata": {
        "id": "VPzQZ4CIMZAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT : Generative Pre-trained Transformer"
      ],
      "metadata": {
        "id": "vac5CyDJMq3H"
      }
    }
  ]
}